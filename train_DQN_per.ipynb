{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PrioritizedReplayBuffer' from 'stable_baselines3.common.buffers' (C:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\buffers.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstable_baselines3\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PPO,DDPG,DQN\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msb3_contrib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QRDQN\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstable_baselines3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbuffers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PrioritizedReplayBuffer\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstable_baselines3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvec_env\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdummy_vec_env\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DummyVecEnv\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstable_baselines3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvec_env\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DummyVecEnv, VecNormalize, SubprocVecEnv\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'PrioritizedReplayBuffer' from 'stable_baselines3.common.buffers' (C:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\buffers.py)"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO,DDPG,DQN\n",
    "from sb3_contrib import QRDQN\n",
    "from stable_baselines3.common.buffers import PrioritizedReplayBuffer\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env\n",
    "env = env.ElectricVehicleEnv()\n",
    "#vec_env = make_vec_env(env, n_envs=4)\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-01T08:20:50.996610900Z",
     "start_time": "2024-09-01T08:20:44.126672100Z"
    }
   },
   "id": "376f27caa974f1e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-7460.05 +/- 2466.43\n",
      "Episode length: 2.60 +/- 1.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-7755.41 +/- 1767.47\n",
      "Episode length: 2.10 +/- 0.44\n",
      "Eval num_timesteps=3000, episode_reward=-7780.50 +/- 1808.84\n",
      "Episode length: 1.95 +/- 0.22\n",
      "Eval num_timesteps=4000, episode_reward=-7355.77 +/- 2486.45\n",
      "Episode length: 1.90 +/- 0.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-7330.89 +/- 2529.30\n",
      "Episode length: 2.15 +/- 0.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-7761.44 +/- 1746.15\n",
      "Episode length: 2.05 +/- 0.50\n",
      "Eval num_timesteps=7000, episode_reward=-8193.78 +/- 118.78\n",
      "Episode length: 2.40 +/- 0.80\n",
      "Eval num_timesteps=8000, episode_reward=-8337.03 +/- 140.94\n",
      "Episode length: 3.20 +/- 0.93\n",
      "Eval num_timesteps=9000, episode_reward=-8233.52 +/- 148.48\n",
      "Episode length: 2.60 +/- 0.97\n",
      "Eval num_timesteps=10000, episode_reward=-7791.94 +/- 1774.39\n",
      "Episode length: 1.95 +/- 0.22\n",
      "Eval num_timesteps=11000, episode_reward=-8212.49 +/- 49.65\n",
      "Episode length: 3.00 +/- 1.38\n",
      "Eval num_timesteps=12000, episode_reward=-8194.69 +/- 70.40\n",
      "Episode length: 2.20 +/- 0.40\n",
      "Eval num_timesteps=13000, episode_reward=-8269.68 +/- 123.00\n",
      "Episode length: 2.75 +/- 0.77\n",
      "Eval num_timesteps=14000, episode_reward=-7820.41 +/- 1779.11\n",
      "Episode length: 2.35 +/- 0.65\n",
      "Eval num_timesteps=15000, episode_reward=-6994.09 +/- 2913.53\n",
      "Episode length: 1.85 +/- 0.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-8285.45 +/- 211.82\n",
      "Episode length: 2.75 +/- 2.09\n",
      "Eval num_timesteps=17000, episode_reward=-7810.74 +/- 1779.13\n",
      "Episode length: 2.00 +/- 0.32\n",
      "Eval num_timesteps=18000, episode_reward=-6967.39 +/- 2885.08\n",
      "Episode length: 1.90 +/- 0.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=-7411.44 +/- 2450.56\n",
      "Episode length: 2.15 +/- 0.57\n",
      "Eval num_timesteps=20000, episode_reward=-7729.43 +/- 1788.49\n",
      "Episode length: 1.95 +/- 0.22\n",
      "Eval num_timesteps=21000, episode_reward=-8193.54 +/- 40.32\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-8169.70 +/- 77.61\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-7773.21 +/- 1769.71\n",
      "Episode length: 2.15 +/- 0.57\n",
      "Eval num_timesteps=24000, episode_reward=-8203.92 +/- 124.23\n",
      "Episode length: 2.35 +/- 0.73\n",
      "Eval num_timesteps=25000, episode_reward=-7836.31 +/- 1761.18\n",
      "Episode length: 2.30 +/- 0.46\n",
      "Eval num_timesteps=26000, episode_reward=-8173.54 +/- 104.93\n",
      "Episode length: 2.50 +/- 0.74\n",
      "Eval num_timesteps=27000, episode_reward=-8349.15 +/- 125.28\n",
      "Episode length: 3.00 +/- 0.95\n",
      "Eval num_timesteps=28000, episode_reward=-8153.88 +/- 1818.65\n",
      "Episode length: 5.45 +/- 1.88\n",
      "Eval num_timesteps=29000, episode_reward=-8216.81 +/- 22.32\n",
      "Episode length: 2.10 +/- 0.44\n",
      "Eval num_timesteps=30000, episode_reward=-8187.78 +/- 65.15\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-8178.87 +/- 82.12\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-7438.90 +/- 2459.85\n",
      "Episode length: 2.30 +/- 0.84\n",
      "Eval num_timesteps=33000, episode_reward=-8477.37 +/- 280.89\n",
      "Episode length: 4.70 +/- 2.37\n",
      "Eval num_timesteps=34000, episode_reward=-7366.22 +/- 2435.16\n",
      "Episode length: 1.90 +/- 0.30\n",
      "Eval num_timesteps=35000, episode_reward=-7852.87 +/- 1789.23\n",
      "Episode length: 4.40 +/- 1.85\n",
      "Eval num_timesteps=36000, episode_reward=-8409.58 +/- 134.69\n",
      "Episode length: 3.90 +/- 1.09\n",
      "Eval num_timesteps=37000, episode_reward=-8050.77 +/- 1871.26\n",
      "Episode length: 4.35 +/- 3.53\n",
      "Eval num_timesteps=38000, episode_reward=-8512.76 +/- 158.57\n",
      "Episode length: 4.40 +/- 1.28\n",
      "Eval num_timesteps=39000, episode_reward=-7804.93 +/- 1777.89\n",
      "Episode length: 2.10 +/- 0.44\n",
      "Eval num_timesteps=40000, episode_reward=-8251.78 +/- 82.48\n",
      "Episode length: 2.35 +/- 0.48\n",
      "Eval num_timesteps=41000, episode_reward=-8249.30 +/- 80.12\n",
      "Episode length: 2.25 +/- 0.43\n",
      "Eval num_timesteps=42000, episode_reward=-7045.31 +/- 2931.29\n",
      "Episode length: 2.35 +/- 0.79\n",
      "Eval num_timesteps=43000, episode_reward=-8244.99 +/- 84.94\n",
      "Episode length: 2.30 +/- 0.64\n",
      "Eval num_timesteps=44000, episode_reward=-8215.53 +/- 131.13\n",
      "Episode length: 2.30 +/- 0.64\n",
      "Eval num_timesteps=45000, episode_reward=-8226.39 +/- 116.38\n",
      "Episode length: 2.40 +/- 0.66\n",
      "Eval num_timesteps=46000, episode_reward=-7854.25 +/- 1788.89\n",
      "Episode length: 2.40 +/- 0.58\n",
      "Eval num_timesteps=47000, episode_reward=-7531.27 +/- 2470.39\n",
      "Episode length: 3.45 +/- 1.50\n",
      "Eval num_timesteps=48000, episode_reward=-7894.37 +/- 1873.00\n",
      "Episode length: 2.50 +/- 0.87\n",
      "Eval num_timesteps=49000, episode_reward=-8209.80 +/- 55.86\n",
      "Episode length: 2.40 +/- 0.66\n",
      "Eval num_timesteps=50000, episode_reward=-8300.33 +/- 68.38\n",
      "Episode length: 2.80 +/- 0.51\n",
      "Eval num_timesteps=51000, episode_reward=-8389.49 +/- 1924.88\n",
      "Episode length: 7.50 +/- 2.85\n",
      "Eval num_timesteps=52000, episode_reward=-7429.32 +/- 2460.76\n",
      "Episode length: 2.50 +/- 0.97\n",
      "Eval num_timesteps=53000, episode_reward=-8288.72 +/- 127.59\n",
      "Episode length: 2.75 +/- 0.70\n",
      "Eval num_timesteps=54000, episode_reward=-8276.36 +/- 92.18\n",
      "Episode length: 2.70 +/- 0.64\n",
      "Eval num_timesteps=55000, episode_reward=-8320.30 +/- 171.27\n",
      "Episode length: 2.80 +/- 1.25\n",
      "Eval num_timesteps=56000, episode_reward=-8236.07 +/- 62.48\n",
      "Episode length: 2.20 +/- 0.40\n",
      "Eval num_timesteps=57000, episode_reward=-8377.44 +/- 87.12\n",
      "Episode length: 3.70 +/- 0.84\n",
      "Eval num_timesteps=58000, episode_reward=-7357.06 +/- 2511.83\n",
      "Episode length: 1.90 +/- 0.30\n",
      "Eval num_timesteps=59000, episode_reward=-6630.71 +/- 3270.50\n",
      "Episode length: 2.35 +/- 0.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-8535.44 +/- 246.00\n",
      "Episode length: 4.75 +/- 2.07\n",
      "Eval num_timesteps=61000, episode_reward=-7798.53 +/- 1812.80\n",
      "Episode length: 1.95 +/- 0.22\n",
      "Eval num_timesteps=62000, episode_reward=-8376.09 +/- 183.81\n",
      "Episode length: 3.55 +/- 1.43\n",
      "Eval num_timesteps=63000, episode_reward=-7897.04 +/- 1774.80\n",
      "Episode length: 2.80 +/- 0.93\n",
      "Eval num_timesteps=64000, episode_reward=-6974.63 +/- 2904.55\n",
      "Episode length: 1.95 +/- 0.50\n",
      "Eval num_timesteps=65000, episode_reward=-8297.92 +/- 58.65\n",
      "Episode length: 2.60 +/- 0.49\n",
      "Eval num_timesteps=66000, episode_reward=-8198.51 +/- 71.94\n",
      "Episode length: 2.20 +/- 0.40\n",
      "Eval num_timesteps=67000, episode_reward=-8435.34 +/- 204.05\n",
      "Episode length: 4.10 +/- 1.97\n",
      "Eval num_timesteps=68000, episode_reward=-7282.40 +/- 3007.28\n",
      "Episode length: 4.10 +/- 1.22\n",
      "Eval num_timesteps=69000, episode_reward=-8696.27 +/- 322.61\n",
      "Episode length: 6.35 +/- 2.87\n",
      "Eval num_timesteps=70000, episode_reward=-8272.69 +/- 124.68\n",
      "Episode length: 2.35 +/- 0.96\n",
      "Eval num_timesteps=71000, episode_reward=-8165.50 +/- 62.00\n",
      "Episode length: 1.90 +/- 0.30\n",
      "Eval num_timesteps=72000, episode_reward=-7853.14 +/- 1788.96\n",
      "Episode length: 2.30 +/- 0.71\n",
      "Eval num_timesteps=73000, episode_reward=-7443.37 +/- 2460.90\n",
      "Episode length: 2.50 +/- 0.74\n",
      "Eval num_timesteps=74000, episode_reward=-8284.47 +/- 122.10\n",
      "Episode length: 2.50 +/- 0.97\n",
      "Eval num_timesteps=75000, episode_reward=-7842.08 +/- 1786.45\n",
      "Episode length: 2.20 +/- 0.60\n",
      "Eval num_timesteps=76000, episode_reward=-7077.73 +/- 2945.20\n",
      "Episode length: 2.85 +/- 1.39\n",
      "Eval num_timesteps=77000, episode_reward=-6236.18 +/- 3512.71\n",
      "Episode length: 2.50 +/- 1.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78000, episode_reward=-7879.17 +/- 1797.67\n",
      "Episode length: 2.60 +/- 1.16\n",
      "Eval num_timesteps=79000, episode_reward=-7555.29 +/- 2476.95\n",
      "Episode length: 3.35 +/- 1.06\n",
      "Eval num_timesteps=80000, episode_reward=-7144.04 +/- 2973.83\n",
      "Episode length: 3.70 +/- 1.79\n",
      "Eval num_timesteps=81000, episode_reward=-8051.85 +/- 1846.71\n",
      "Episode length: 4.20 +/- 2.16\n",
      "Eval num_timesteps=82000, episode_reward=-7662.08 +/- 2962.23\n",
      "Episode length: 8.40 +/- 2.03\n",
      "Eval num_timesteps=83000, episode_reward=-7493.26 +/- 2480.03\n",
      "Episode length: 2.75 +/- 0.89\n",
      "Eval num_timesteps=84000, episode_reward=-8209.91 +/- 88.52\n",
      "Episode length: 2.10 +/- 0.30\n",
      "Eval num_timesteps=85000, episode_reward=-8062.17 +/- 1866.71\n",
      "Episode length: 4.40 +/- 3.10\n",
      "Eval num_timesteps=86000, episode_reward=-7826.87 +/- 1820.62\n",
      "Episode length: 2.75 +/- 0.94\n",
      "Eval num_timesteps=87000, episode_reward=-8301.79 +/- 133.42\n",
      "Episode length: 2.90 +/- 0.94\n",
      "Eval num_timesteps=88000, episode_reward=-7452.13 +/- 2469.06\n",
      "Episode length: 4.05 +/- 1.99\n",
      "Eval num_timesteps=89000, episode_reward=-8221.86 +/- 116.13\n",
      "Episode length: 3.00 +/- 0.95\n",
      "Eval num_timesteps=90000, episode_reward=-8316.07 +/- 151.82\n",
      "Episode length: 4.65 +/- 2.43\n",
      "Eval num_timesteps=91000, episode_reward=-7687.38 +/- 2506.97\n",
      "Episode length: 4.70 +/- 1.68\n",
      "Eval num_timesteps=92000, episode_reward=-8209.03 +/- 111.94\n",
      "Episode length: 2.10 +/- 0.30\n",
      "Eval num_timesteps=93000, episode_reward=-8223.83 +/- 27.37\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-8147.00 +/- 94.21\n",
      "Episode length: 2.05 +/- 0.22\n",
      "Eval num_timesteps=95000, episode_reward=-8246.26 +/- 42.62\n",
      "Episode length: 2.25 +/- 0.43\n",
      "Eval num_timesteps=96000, episode_reward=-7863.00 +/- 1732.13\n",
      "Episode length: 2.50 +/- 0.92\n",
      "Eval num_timesteps=97000, episode_reward=-8522.18 +/- 122.24\n",
      "Episode length: 4.70 +/- 1.05\n",
      "Eval num_timesteps=98000, episode_reward=-5794.37 +/- 4116.10\n",
      "Episode length: 5.40 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=-5909.33 +/- 3809.91\n",
      "Episode length: 3.20 +/- 1.33\n",
      "Eval num_timesteps=100000, episode_reward=-7741.04 +/- 1766.63\n",
      "Episode length: 2.05 +/- 0.50\n",
      "Eval num_timesteps=101000, episode_reward=-8603.87 +/- 356.65\n",
      "Episode length: 5.95 +/- 3.14\n",
      "Eval num_timesteps=102000, episode_reward=-8222.50 +/- 131.40\n",
      "Episode length: 2.30 +/- 0.71\n",
      "Eval num_timesteps=103000, episode_reward=-7867.06 +/- 1768.47\n",
      "Episode length: 2.65 +/- 0.73\n",
      "Eval num_timesteps=104000, episode_reward=-9110.90 +/- 79.50\n",
      "Episode length: 10.80 +/- 0.40\n",
      "Eval num_timesteps=105000, episode_reward=-6725.07 +/- 3288.49\n",
      "Episode length: 2.85 +/- 1.11\n",
      "Eval num_timesteps=106000, episode_reward=-7572.02 +/- 2473.58\n",
      "Episode length: 3.75 +/- 1.67\n",
      "Eval num_timesteps=107000, episode_reward=-7851.70 +/- 1803.44\n",
      "Episode length: 3.15 +/- 1.11\n",
      "Eval num_timesteps=108000, episode_reward=-7569.06 +/- 2514.15\n",
      "Episode length: 3.35 +/- 1.39\n",
      "Eval num_timesteps=109000, episode_reward=-8532.59 +/- 179.14\n",
      "Episode length: 4.75 +/- 1.09\n",
      "Eval num_timesteps=110000, episode_reward=-7440.31 +/- 2466.28\n",
      "Episode length: 2.45 +/- 1.40\n",
      "Eval num_timesteps=111000, episode_reward=-7814.99 +/- 1779.95\n",
      "Episode length: 2.05 +/- 0.38\n",
      "Eval num_timesteps=112000, episode_reward=-6852.40 +/- 3443.39\n",
      "Episode length: 4.45 +/- 3.50\n",
      "Eval num_timesteps=113000, episode_reward=-8099.87 +/- 1765.32\n",
      "Episode length: 4.90 +/- 1.89\n",
      "Eval num_timesteps=114000, episode_reward=-7068.73 +/- 2905.28\n",
      "Episode length: 2.55 +/- 0.67\n",
      "Eval num_timesteps=115000, episode_reward=-8181.21 +/- 1897.06\n",
      "Episode length: 5.30 +/- 3.48\n",
      "Eval num_timesteps=116000, episode_reward=-6643.84 +/- 3285.54\n",
      "Episode length: 2.40 +/- 1.69\n",
      "Eval num_timesteps=117000, episode_reward=-7971.82 +/- 1804.17\n",
      "Episode length: 3.75 +/- 1.48\n",
      "Eval num_timesteps=118000, episode_reward=-7122.56 +/- 2971.37\n",
      "Episode length: 3.15 +/- 1.71\n",
      "Eval num_timesteps=119000, episode_reward=-6775.65 +/- 3348.76\n",
      "Episode length: 3.85 +/- 3.61\n",
      "Eval num_timesteps=120000, episode_reward=-8190.89 +/- 87.04\n",
      "Episode length: 2.10 +/- 0.30\n",
      "Eval num_timesteps=121000, episode_reward=-8074.17 +/- 1848.37\n",
      "Episode length: 4.40 +/- 1.93\n",
      "Eval num_timesteps=122000, episode_reward=-7766.70 +/- 3192.33\n",
      "Episode length: 9.65 +/- 3.21\n",
      "Eval num_timesteps=123000, episode_reward=-7064.86 +/- 2939.70\n",
      "Episode length: 2.45 +/- 1.12\n",
      "Eval num_timesteps=124000, episode_reward=-8399.59 +/- 60.91\n",
      "Episode length: 3.35 +/- 0.57\n",
      "Eval num_timesteps=125000, episode_reward=-6935.25 +/- 3311.99\n",
      "Episode length: 4.70 +/- 0.95\n",
      "Eval num_timesteps=126000, episode_reward=-8435.66 +/- 156.14\n",
      "Episode length: 3.90 +/- 1.22\n",
      "Eval num_timesteps=127000, episode_reward=-7472.50 +/- 2555.77\n",
      "Episode length: 2.75 +/- 1.48\n",
      "Eval num_timesteps=128000, episode_reward=-8198.35 +/- 1793.42\n",
      "Episode length: 6.10 +/- 1.30\n",
      "Eval num_timesteps=129000, episode_reward=-8380.07 +/- 157.12\n",
      "Episode length: 3.50 +/- 1.07\n",
      "Eval num_timesteps=130000, episode_reward=-7563.99 +/- 2524.17\n",
      "Episode length: 4.15 +/- 1.68\n",
      "Eval num_timesteps=131000, episode_reward=-7885.64 +/- 2646.03\n",
      "Episode length: 6.60 +/- 4.48\n",
      "Eval num_timesteps=132000, episode_reward=-7824.74 +/- 1785.73\n",
      "Episode length: 2.85 +/- 1.42\n",
      "Eval num_timesteps=133000, episode_reward=-8342.10 +/- 114.23\n",
      "Episode length: 3.10 +/- 1.18\n",
      "Eval num_timesteps=134000, episode_reward=-8169.49 +/- 1768.85\n",
      "Episode length: 5.45 +/- 1.60\n",
      "Eval num_timesteps=135000, episode_reward=-7264.80 +/- 3039.78\n",
      "Episode length: 4.35 +/- 3.17\n",
      "Eval num_timesteps=136000, episode_reward=-7834.10 +/- 1783.36\n",
      "Episode length: 2.60 +/- 1.07\n",
      "Eval num_timesteps=137000, episode_reward=-8618.55 +/- 324.06\n",
      "Episode length: 5.95 +/- 3.31\n",
      "Eval num_timesteps=138000, episode_reward=-8226.85 +/- 83.39\n",
      "Episode length: 2.45 +/- 0.59\n",
      "Eval num_timesteps=139000, episode_reward=-7430.21 +/- 2456.91\n",
      "Episode length: 2.10 +/- 0.54\n",
      "Eval num_timesteps=140000, episode_reward=-7452.10 +/- 2466.02\n",
      "Episode length: 2.75 +/- 1.13\n",
      "Eval num_timesteps=141000, episode_reward=-7517.61 +/- 2307.56\n",
      "Episode length: 3.10 +/- 2.39\n",
      "Eval num_timesteps=142000, episode_reward=-8031.86 +/- 1807.86\n",
      "Episode length: 3.80 +/- 0.98\n",
      "Eval num_timesteps=143000, episode_reward=-7545.63 +/- 2498.77\n",
      "Episode length: 4.60 +/- 2.01\n",
      "Eval num_timesteps=144000, episode_reward=-7827.84 +/- 2565.19\n",
      "Episode length: 6.25 +/- 3.59\n",
      "Eval num_timesteps=145000, episode_reward=-7793.32 +/- 1830.07\n",
      "Episode length: 2.20 +/- 0.51\n",
      "Eval num_timesteps=146000, episode_reward=-8344.92 +/- 205.30\n",
      "Episode length: 3.20 +/- 1.72\n",
      "Eval num_timesteps=147000, episode_reward=-7655.91 +/- 2517.99\n",
      "Episode length: 4.00 +/- 1.87\n",
      "Eval num_timesteps=148000, episode_reward=-7190.14 +/- 3003.24\n",
      "Episode length: 3.85 +/- 2.31\n",
      "Eval num_timesteps=149000, episode_reward=-8531.35 +/- 282.96\n",
      "Episode length: 5.55 +/- 3.07\n",
      "Eval num_timesteps=150000, episode_reward=-7206.71 +/- 2990.11\n",
      "Episode length: 3.60 +/- 1.53\n",
      "Eval num_timesteps=151000, episode_reward=-8323.57 +/- 1829.45\n",
      "Episode length: 7.10 +/- 2.02\n",
      "Eval num_timesteps=152000, episode_reward=-8320.40 +/- 125.37\n",
      "Episode length: 3.00 +/- 1.00\n",
      "Eval num_timesteps=153000, episode_reward=-7906.27 +/- 1754.95\n",
      "Episode length: 2.85 +/- 0.96\n",
      "Eval num_timesteps=154000, episode_reward=-7833.67 +/- 2565.28\n",
      "Episode length: 6.90 +/- 3.28\n",
      "Eval num_timesteps=155000, episode_reward=-7733.44 +/- 2500.97\n",
      "Episode length: 4.70 +/- 2.41\n",
      "Eval num_timesteps=156000, episode_reward=-8221.02 +/- 111.79\n",
      "Episode length: 2.20 +/- 0.68\n",
      "Eval num_timesteps=157000, episode_reward=-7100.91 +/- 2995.38\n",
      "Episode length: 3.90 +/- 1.73\n",
      "Eval num_timesteps=158000, episode_reward=-8519.91 +/- 386.03\n",
      "Episode length: 5.40 +/- 4.21\n",
      "Eval num_timesteps=159000, episode_reward=-7007.97 +/- 3349.27\n",
      "Episode length: 5.90 +/- 1.87\n",
      "Eval num_timesteps=160000, episode_reward=-7505.45 +/- 2481.74\n",
      "Episode length: 2.95 +/- 1.07\n",
      "Eval num_timesteps=161000, episode_reward=-6860.60 +/- 3405.84\n",
      "Episode length: 4.35 +/- 2.55\n",
      "Eval num_timesteps=162000, episode_reward=-6375.00 +/- 3631.72\n",
      "Episode length: 3.85 +/- 1.28\n",
      "Eval num_timesteps=163000, episode_reward=-8428.98 +/- 1877.56\n",
      "Episode length: 7.75 +/- 2.66\n",
      "Eval num_timesteps=164000, episode_reward=-7533.02 +/- 2421.43\n",
      "Episode length: 3.20 +/- 2.25\n",
      "Eval num_timesteps=165000, episode_reward=-8421.06 +/- 193.87\n",
      "Episode length: 4.20 +/- 1.57\n",
      "Eval num_timesteps=166000, episode_reward=-8432.45 +/- 122.48\n",
      "Episode length: 4.45 +/- 1.50\n",
      "Eval num_timesteps=167000, episode_reward=-8662.96 +/- 1987.38\n",
      "Episode length: 10.15 +/- 2.15\n",
      "Eval num_timesteps=168000, episode_reward=-7907.44 +/- 1805.20\n",
      "Episode length: 3.25 +/- 0.99\n",
      "Eval num_timesteps=169000, episode_reward=-8419.39 +/- 69.89\n",
      "Episode length: 3.55 +/- 0.59\n",
      "Eval num_timesteps=170000, episode_reward=-8463.96 +/- 94.28\n",
      "Episode length: 4.10 +/- 0.62\n",
      "Eval num_timesteps=171000, episode_reward=-7203.65 +/- 2990.03\n",
      "Episode length: 5.55 +/- 1.99\n",
      "Eval num_timesteps=172000, episode_reward=-7632.04 +/- 2507.96\n",
      "Episode length: 4.10 +/- 1.18\n",
      "Eval num_timesteps=173000, episode_reward=-8032.00 +/- 1820.38\n",
      "Episode length: 4.05 +/- 2.42\n",
      "Eval num_timesteps=174000, episode_reward=-7829.22 +/- 1736.40\n",
      "Episode length: 2.35 +/- 1.31\n",
      "Eval num_timesteps=175000, episode_reward=-7566.83 +/- 2493.06\n",
      "Episode length: 3.50 +/- 2.16\n",
      "Eval num_timesteps=176000, episode_reward=-8129.13 +/- 1854.21\n",
      "Episode length: 4.60 +/- 1.02\n",
      "Eval num_timesteps=177000, episode_reward=-8276.70 +/- 84.69\n",
      "Episode length: 2.95 +/- 0.92\n",
      "Eval num_timesteps=178000, episode_reward=-8415.40 +/- 122.55\n",
      "Episode length: 3.90 +/- 1.09\n",
      "Eval num_timesteps=179000, episode_reward=-8340.02 +/- 156.05\n",
      "Episode length: 3.25 +/- 1.44\n",
      "Eval num_timesteps=180000, episode_reward=-8512.12 +/- 174.51\n",
      "Episode length: 4.85 +/- 0.91\n",
      "Eval num_timesteps=181000, episode_reward=-8431.78 +/- 204.08\n",
      "Episode length: 4.30 +/- 1.82\n",
      "Eval num_timesteps=182000, episode_reward=-7408.38 +/- 3240.13\n",
      "Episode length: 6.00 +/- 4.07\n",
      "Eval num_timesteps=183000, episode_reward=-7166.00 +/- 3367.73\n",
      "Episode length: 6.95 +/- 2.11\n",
      "Eval num_timesteps=184000, episode_reward=-8084.42 +/- 1821.63\n",
      "Episode length: 4.40 +/- 2.18\n",
      "Eval num_timesteps=185000, episode_reward=-8086.55 +/- 1867.37\n",
      "Episode length: 5.00 +/- 2.72\n",
      "Eval num_timesteps=186000, episode_reward=-8394.07 +/- 142.45\n",
      "Episode length: 4.15 +/- 1.80\n",
      "Eval num_timesteps=187000, episode_reward=-5858.57 +/- 3832.66\n",
      "Episode length: 3.90 +/- 1.41\n",
      "Eval num_timesteps=188000, episode_reward=-7835.03 +/- 2592.80\n",
      "Episode length: 7.65 +/- 3.28\n",
      "Eval num_timesteps=189000, episode_reward=-7125.69 +/- 2941.13\n",
      "Episode length: 3.25 +/- 1.44\n",
      "Eval num_timesteps=190000, episode_reward=-8205.89 +/- 153.92\n",
      "Episode length: 2.35 +/- 0.85\n",
      "Eval num_timesteps=191000, episode_reward=-6457.61 +/- 3638.83\n",
      "Episode length: 4.75 +/- 1.92\n",
      "Eval num_timesteps=192000, episode_reward=-8364.45 +/- 130.73\n",
      "Episode length: 3.20 +/- 0.98\n",
      "Eval num_timesteps=193000, episode_reward=-7819.91 +/- 1782.21\n",
      "Episode length: 2.35 +/- 0.73\n",
      "Eval num_timesteps=194000, episode_reward=-7433.76 +/- 2443.54\n",
      "Episode length: 2.60 +/- 1.07\n",
      "Eval num_timesteps=195000, episode_reward=-7556.36 +/- 2376.72\n",
      "Episode length: 3.95 +/- 1.12\n",
      "Eval num_timesteps=196000, episode_reward=-8628.08 +/- 448.92\n",
      "Episode length: 6.15 +/- 4.27\n",
      "Eval num_timesteps=197000, episode_reward=-7746.48 +/- 2981.42\n",
      "Episode length: 9.50 +/- 3.31\n",
      "Eval num_timesteps=198000, episode_reward=-7831.41 +/- 2613.14\n",
      "Episode length: 6.65 +/- 3.51\n",
      "Eval num_timesteps=199000, episode_reward=-8066.57 +/- 1820.88\n",
      "Episode length: 4.55 +/- 1.32\n",
      "Eval num_timesteps=200000, episode_reward=-7927.87 +/- 1786.68\n",
      "Episode length: 2.90 +/- 0.99\n",
      "Eval num_timesteps=201000, episode_reward=-7786.02 +/- 1773.63\n",
      "Episode length: 2.25 +/- 0.70\n",
      "Eval num_timesteps=202000, episode_reward=-8028.86 +/- 1816.27\n",
      "Episode length: 4.60 +/- 1.88\n",
      "Eval num_timesteps=203000, episode_reward=-8473.01 +/- 280.90\n",
      "Episode length: 4.50 +/- 2.82\n",
      "Eval num_timesteps=204000, episode_reward=-7976.96 +/- 1792.51\n",
      "Episode length: 5.35 +/- 1.35\n",
      "Eval num_timesteps=205000, episode_reward=-7861.00 +/- 1789.84\n",
      "Episode length: 2.55 +/- 0.59\n",
      "Eval num_timesteps=206000, episode_reward=-8480.27 +/- 131.45\n",
      "Episode length: 4.85 +/- 1.49\n",
      "Eval num_timesteps=207000, episode_reward=-8235.02 +/- 1897.72\n",
      "Episode length: 6.55 +/- 3.17\n",
      "Eval num_timesteps=208000, episode_reward=-6460.68 +/- 3645.61\n",
      "Episode length: 4.25 +/- 1.61\n",
      "Eval num_timesteps=209000, episode_reward=-8690.54 +/- 169.52\n",
      "Episode length: 6.65 +/- 1.71\n",
      "Eval num_timesteps=210000, episode_reward=-8998.98 +/- 204.59\n",
      "Episode length: 10.00 +/- 2.05\n",
      "Eval num_timesteps=211000, episode_reward=-6401.25 +/- 3570.06\n",
      "Episode length: 3.55 +/- 1.36\n",
      "Eval num_timesteps=212000, episode_reward=-6895.67 +/- 3887.00\n",
      "Episode length: 8.75 +/- 3.92\n",
      "Eval num_timesteps=213000, episode_reward=-8335.66 +/- 100.73\n",
      "Episode length: 2.95 +/- 0.80\n",
      "Eval num_timesteps=214000, episode_reward=-8276.15 +/- 154.50\n",
      "Episode length: 2.65 +/- 1.42\n",
      "Eval num_timesteps=215000, episode_reward=-8427.49 +/- 151.03\n",
      "Episode length: 4.10 +/- 1.26\n",
      "Eval num_timesteps=216000, episode_reward=-8259.68 +/- 59.64\n",
      "Episode length: 2.55 +/- 0.50\n",
      "Eval num_timesteps=217000, episode_reward=-7757.62 +/- 1822.97\n",
      "Episode length: 2.10 +/- 0.44\n",
      "Eval num_timesteps=218000, episode_reward=-9088.01 +/- 68.11\n",
      "Episode length: 10.90 +/- 0.30\n",
      "Eval num_timesteps=219000, episode_reward=-7850.96 +/- 1788.92\n",
      "Episode length: 2.35 +/- 0.65\n",
      "Eval num_timesteps=220000, episode_reward=-8243.44 +/- 41.63\n",
      "Episode length: 2.15 +/- 0.36\n",
      "Eval num_timesteps=221000, episode_reward=-7522.17 +/- 2476.94\n",
      "Episode length: 3.15 +/- 2.74\n",
      "Eval num_timesteps=222000, episode_reward=-8548.37 +/- 1970.64\n",
      "Episode length: 9.25 +/- 3.25\n",
      "Eval num_timesteps=223000, episode_reward=-8071.66 +/- 1781.93\n",
      "Episode length: 3.95 +/- 0.92\n",
      "Eval num_timesteps=224000, episode_reward=-7949.91 +/- 2599.65\n",
      "Episode length: 7.00 +/- 2.49\n",
      "Eval num_timesteps=225000, episode_reward=-8435.10 +/- 1936.16\n",
      "Episode length: 8.15 +/- 2.87\n",
      "Eval num_timesteps=226000, episode_reward=-7815.95 +/- 2586.59\n",
      "Episode length: 7.90 +/- 2.45\n",
      "Eval num_timesteps=227000, episode_reward=-6595.30 +/- 3228.89\n",
      "Episode length: 2.00 +/- 0.45\n",
      "Eval num_timesteps=228000, episode_reward=-7890.52 +/- 1797.16\n",
      "Episode length: 2.75 +/- 0.83\n",
      "Eval num_timesteps=229000, episode_reward=-7869.01 +/- 1792.85\n",
      "Episode length: 2.60 +/- 0.97\n",
      "Eval num_timesteps=230000, episode_reward=-8133.71 +/- 2638.44\n",
      "Episode length: 9.30 +/- 3.21\n",
      "Eval num_timesteps=231000, episode_reward=-7954.99 +/- 2634.04\n",
      "Episode length: 8.45 +/- 2.82\n",
      "Eval num_timesteps=232000, episode_reward=-7883.86 +/- 1800.07\n",
      "Episode length: 2.45 +/- 0.97\n",
      "Eval num_timesteps=233000, episode_reward=-8310.99 +/- 139.91\n",
      "Episode length: 3.20 +/- 0.81\n",
      "Eval num_timesteps=234000, episode_reward=-7491.05 +/- 3075.51\n",
      "Episode length: 8.45 +/- 4.06\n",
      "Eval num_timesteps=235000, episode_reward=-8888.82 +/- 187.07\n",
      "Episode length: 8.35 +/- 2.24\n",
      "Eval num_timesteps=236000, episode_reward=-6840.27 +/- 3369.05\n",
      "Episode length: 4.35 +/- 1.74\n",
      "Eval num_timesteps=237000, episode_reward=-8022.69 +/- 1839.33\n",
      "Episode length: 3.90 +/- 2.00\n",
      "Eval num_timesteps=238000, episode_reward=-8479.42 +/- 185.02\n",
      "Episode length: 5.20 +/- 2.34\n",
      "Eval num_timesteps=239000, episode_reward=-7744.82 +/- 2553.94\n",
      "Episode length: 7.30 +/- 1.90\n",
      "Eval num_timesteps=240000, episode_reward=-8020.93 +/- 1827.04\n",
      "Episode length: 4.05 +/- 1.24\n",
      "Eval num_timesteps=241000, episode_reward=-7811.42 +/- 1796.12\n",
      "Episode length: 2.20 +/- 0.75\n",
      "Eval num_timesteps=242000, episode_reward=-6463.90 +/- 4022.71\n",
      "Episode length: 8.50 +/- 3.71\n",
      "Eval num_timesteps=243000, episode_reward=-8208.69 +/- 139.22\n",
      "Episode length: 2.30 +/- 0.90\n",
      "Eval num_timesteps=244000, episode_reward=-7530.62 +/- 2491.12\n",
      "Episode length: 3.10 +/- 1.41\n",
      "Eval num_timesteps=245000, episode_reward=-8572.16 +/- 116.73\n",
      "Episode length: 5.35 +/- 0.79\n",
      "Eval num_timesteps=246000, episode_reward=-8035.26 +/- 2643.32\n",
      "Episode length: 7.70 +/- 2.59\n",
      "Eval num_timesteps=247000, episode_reward=-7886.66 +/- 1802.31\n",
      "Episode length: 3.65 +/- 1.74\n",
      "Eval num_timesteps=248000, episode_reward=-8286.10 +/- 74.97\n",
      "Episode length: 3.35 +/- 1.19\n",
      "Eval num_timesteps=249000, episode_reward=-7449.28 +/- 2466.81\n",
      "Episode length: 3.20 +/- 1.21\n",
      "Eval num_timesteps=250000, episode_reward=-7717.70 +/- 1760.00\n",
      "Episode length: 2.35 +/- 0.57\n",
      "Eval num_timesteps=251000, episode_reward=-8883.73 +/- 257.22\n",
      "Episode length: 9.80 +/- 2.64\n",
      "Eval num_timesteps=252000, episode_reward=-8083.30 +/- 2594.11\n",
      "Episode length: 10.20 +/- 2.38\n",
      "Eval num_timesteps=253000, episode_reward=-8009.74 +/- 1801.92\n",
      "Episode length: 4.10 +/- 1.97\n",
      "Eval num_timesteps=254000, episode_reward=-5336.76 +/- 4123.20\n",
      "Episode length: 6.00 +/- 2.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=255000, episode_reward=-8287.89 +/- 1891.87\n",
      "Episode length: 8.70 +/- 2.57\n",
      "Eval num_timesteps=256000, episode_reward=-8215.79 +/- 59.75\n",
      "Episode length: 2.35 +/- 0.48\n",
      "Eval num_timesteps=257000, episode_reward=-6621.76 +/- 3251.32\n",
      "Episode length: 2.20 +/- 0.68\n",
      "Eval num_timesteps=258000, episode_reward=-8382.14 +/- 202.72\n",
      "Episode length: 3.70 +/- 1.68\n",
      "Eval num_timesteps=259000, episode_reward=-7650.49 +/- 2531.09\n",
      "Episode length: 4.65 +/- 2.24\n",
      "Eval num_timesteps=260000, episode_reward=-8851.07 +/- 316.46\n",
      "Episode length: 8.05 +/- 3.25\n",
      "Eval num_timesteps=261000, episode_reward=-7906.29 +/- 1850.07\n",
      "Episode length: 5.35 +/- 2.20\n",
      "Eval num_timesteps=262000, episode_reward=-8278.78 +/- 1787.63\n",
      "Episode length: 6.20 +/- 1.21\n",
      "Eval num_timesteps=263000, episode_reward=-8637.14 +/- 179.10\n",
      "Episode length: 6.20 +/- 1.91\n",
      "Eval num_timesteps=264000, episode_reward=-6614.79 +/- 3722.36\n",
      "Episode length: 6.15 +/- 2.78\n",
      "Eval num_timesteps=265000, episode_reward=-8088.83 +/- 87.25\n",
      "Episode length: 3.95 +/- 0.74\n",
      "Eval num_timesteps=266000, episode_reward=-8219.96 +/- 2679.74\n",
      "Episode length: 9.80 +/- 2.64\n",
      "Eval num_timesteps=267000, episode_reward=-7803.01 +/- 2529.52\n",
      "Episode length: 6.70 +/- 1.55\n",
      "Eval num_timesteps=268000, episode_reward=-7419.54 +/- 2452.85\n",
      "Episode length: 3.15 +/- 0.91\n",
      "Eval num_timesteps=269000, episode_reward=-8305.94 +/- 177.11\n",
      "Episode length: 2.65 +/- 1.15\n",
      "Eval num_timesteps=270000, episode_reward=-7274.50 +/- 3006.25\n",
      "Episode length: 4.80 +/- 3.53\n",
      "Eval num_timesteps=271000, episode_reward=-8712.25 +/- 159.52\n",
      "Episode length: 6.50 +/- 1.47\n",
      "Eval num_timesteps=272000, episode_reward=-7923.05 +/- 1817.38\n",
      "Episode length: 4.75 +/- 1.89\n",
      "Eval num_timesteps=273000, episode_reward=-8361.57 +/- 70.58\n",
      "Episode length: 5.65 +/- 1.35\n",
      "Eval num_timesteps=274000, episode_reward=-8132.85 +/- 1851.86\n",
      "Episode length: 7.25 +/- 2.51\n",
      "Eval num_timesteps=275000, episode_reward=-8323.88 +/- 176.95\n",
      "Episode length: 3.15 +/- 1.62\n",
      "Eval num_timesteps=276000, episode_reward=-7763.42 +/- 1767.24\n",
      "Episode length: 1.95 +/- 0.22\n",
      "Eval num_timesteps=277000, episode_reward=-7738.12 +/- 1835.93\n",
      "Episode length: 1.95 +/- 0.22\n",
      "Eval num_timesteps=278000, episode_reward=-8328.07 +/- 1897.60\n",
      "Episode length: 8.55 +/- 2.33\n",
      "Eval num_timesteps=279000, episode_reward=-7393.08 +/- 3002.28\n",
      "Episode length: 8.65 +/- 2.85\n",
      "Eval num_timesteps=280000, episode_reward=-7602.02 +/- 2496.27\n",
      "Episode length: 4.80 +/- 2.34\n",
      "Eval num_timesteps=281000, episode_reward=-8706.41 +/- 107.65\n",
      "Episode length: 9.70 +/- 1.79\n",
      "Eval num_timesteps=282000, episode_reward=-7089.90 +/- 2948.60\n",
      "Episode length: 3.70 +/- 1.82\n",
      "Eval num_timesteps=283000, episode_reward=-7907.94 +/- 2615.31\n",
      "Episode length: 9.35 +/- 2.73\n",
      "Eval num_timesteps=284000, episode_reward=-6494.98 +/- 3714.30\n",
      "Episode length: 6.80 +/- 3.47\n",
      "Eval num_timesteps=285000, episode_reward=-8608.74 +/- 83.04\n",
      "Episode length: 5.70 +/- 0.78\n",
      "Eval num_timesteps=286000, episode_reward=-8396.75 +/- 1913.85\n",
      "Episode length: 10.40 +/- 2.18\n",
      "Eval num_timesteps=287000, episode_reward=-8459.66 +/- 198.96\n",
      "Episode length: 4.40 +/- 1.56\n",
      "Eval num_timesteps=288000, episode_reward=-8695.90 +/- 1858.01\n",
      "Episode length: 10.40 +/- 1.24\n",
      "Eval num_timesteps=289000, episode_reward=-6717.29 +/- 3314.71\n",
      "Episode length: 4.35 +/- 2.89\n",
      "Eval num_timesteps=290000, episode_reward=-7771.76 +/- 1773.01\n",
      "Episode length: 2.40 +/- 0.73\n",
      "Eval num_timesteps=291000, episode_reward=-8579.93 +/- 1960.50\n",
      "Episode length: 9.30 +/- 2.33\n",
      "Eval num_timesteps=292000, episode_reward=-5992.11 +/- 3878.19\n",
      "Episode length: 5.35 +/- 2.52\n",
      "Eval num_timesteps=293000, episode_reward=-7984.50 +/- 2675.57\n",
      "Episode length: 7.75 +/- 4.44\n",
      "Eval num_timesteps=294000, episode_reward=-8127.93 +/- 1853.95\n",
      "Episode length: 6.35 +/- 1.65\n",
      "Eval num_timesteps=295000, episode_reward=-7114.36 +/- 3529.07\n",
      "Episode length: 9.00 +/- 4.00\n",
      "Eval num_timesteps=296000, episode_reward=-8660.68 +/- 102.75\n",
      "Episode length: 6.25 +/- 0.70\n",
      "Eval num_timesteps=297000, episode_reward=-7919.64 +/- 2620.47\n",
      "Episode length: 7.85 +/- 3.13\n",
      "Eval num_timesteps=298000, episode_reward=-8411.92 +/- 1915.51\n",
      "Episode length: 10.40 +/- 2.20\n",
      "Eval num_timesteps=299000, episode_reward=-8434.25 +/- 1924.71\n",
      "Episode length: 8.80 +/- 2.32\n",
      "Eval num_timesteps=300000, episode_reward=-8460.04 +/- 1931.15\n",
      "Episode length: 8.60 +/- 2.44\n",
      "Eval num_timesteps=301000, episode_reward=-8839.00 +/- 77.75\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-8199.78 +/- 1874.56\n",
      "Episode length: 7.25 +/- 1.97\n",
      "Eval num_timesteps=303000, episode_reward=-7710.99 +/- 2539.38\n",
      "Episode length: 6.40 +/- 2.96\n",
      "Eval num_timesteps=304000, episode_reward=-7815.41 +/- 2568.69\n",
      "Episode length: 8.25 +/- 2.02\n",
      "Eval num_timesteps=305000, episode_reward=-7756.10 +/- 2566.24\n",
      "Episode length: 6.00 +/- 2.05\n",
      "Eval num_timesteps=306000, episode_reward=-7356.26 +/- 3060.39\n",
      "Episode length: 5.10 +/- 2.98\n",
      "Eval num_timesteps=307000, episode_reward=-8885.12 +/- 111.72\n",
      "Episode length: 9.95 +/- 1.16\n",
      "Eval num_timesteps=308000, episode_reward=-7546.50 +/- 2491.08\n",
      "Episode length: 4.45 +/- 1.66\n",
      "Eval num_timesteps=309000, episode_reward=-7065.75 +/- 2941.25\n",
      "Episode length: 4.70 +/- 2.03\n",
      "Eval num_timesteps=310000, episode_reward=-8364.21 +/- 138.87\n",
      "Episode length: 4.15 +/- 1.88\n",
      "Eval num_timesteps=311000, episode_reward=-7437.64 +/- 2441.95\n",
      "Episode length: 2.50 +/- 0.92\n",
      "Eval num_timesteps=312000, episode_reward=-8189.12 +/- 82.52\n",
      "Episode length: 4.45 +/- 1.32\n",
      "Eval num_timesteps=313000, episode_reward=-6253.96 +/- 3548.79\n",
      "Episode length: 3.40 +/- 1.39\n",
      "Eval num_timesteps=314000, episode_reward=-8840.41 +/- 90.26\n",
      "Episode length: 7.70 +/- 1.05\n",
      "Eval num_timesteps=315000, episode_reward=-6164.85 +/- 3921.86\n",
      "Episode length: 7.00 +/- 4.45\n",
      "Eval num_timesteps=316000, episode_reward=-7474.07 +/- 3141.73\n",
      "Episode length: 7.15 +/- 3.85\n",
      "Eval num_timesteps=317000, episode_reward=-5816.48 +/- 4056.84\n",
      "Episode length: 7.15 +/- 2.26\n",
      "Eval num_timesteps=318000, episode_reward=-8028.92 +/- 1829.46\n",
      "Episode length: 5.20 +/- 2.09\n",
      "Eval num_timesteps=319000, episode_reward=-8384.64 +/- 82.96\n",
      "Episode length: 3.70 +/- 0.84\n",
      "Eval num_timesteps=320000, episode_reward=-8856.80 +/- 113.09\n",
      "Episode length: 10.20 +/- 1.47\n",
      "Eval num_timesteps=321000, episode_reward=-7188.16 +/- 3522.04\n",
      "Episode length: 8.90 +/- 3.66\n",
      "Eval num_timesteps=322000, episode_reward=-8532.07 +/- 1956.82\n",
      "Episode length: 9.25 +/- 2.53\n",
      "Eval num_timesteps=323000, episode_reward=-7785.55 +/- 2572.64\n",
      "Episode length: 8.15 +/- 2.46\n",
      "Eval num_timesteps=324000, episode_reward=-8920.88 +/- 174.02\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-7101.52 +/- 2936.61\n",
      "Episode length: 3.45 +/- 1.43\n",
      "Eval num_timesteps=326000, episode_reward=-8097.05 +/- 2590.47\n",
      "Episode length: 10.50 +/- 1.50\n",
      "Eval num_timesteps=327000, episode_reward=-7849.01 +/- 2590.40\n",
      "Episode length: 8.70 +/- 2.79\n",
      "Eval num_timesteps=328000, episode_reward=-6682.91 +/- 3320.43\n",
      "Episode length: 3.55 +/- 1.88\n",
      "Eval num_timesteps=329000, episode_reward=-6722.56 +/- 3269.05\n",
      "Episode length: 3.60 +/- 1.98\n",
      "Eval num_timesteps=330000, episode_reward=-7329.58 +/- 2973.15\n",
      "Episode length: 7.25 +/- 2.19\n",
      "Eval num_timesteps=331000, episode_reward=-7715.96 +/- 2563.89\n",
      "Episode length: 6.85 +/- 3.42\n",
      "Eval num_timesteps=332000, episode_reward=-6914.85 +/- 3318.36\n",
      "Episode length: 7.05 +/- 2.13\n",
      "Eval num_timesteps=333000, episode_reward=-7115.19 +/- 3367.84\n",
      "Episode length: 7.90 +/- 2.74\n",
      "Eval num_timesteps=334000, episode_reward=-7745.01 +/- 3255.09\n",
      "Episode length: 9.55 +/- 3.46\n",
      "Eval num_timesteps=335000, episode_reward=-8465.58 +/- 332.27\n",
      "Episode length: 4.55 +/- 3.31\n",
      "Eval num_timesteps=336000, episode_reward=-7995.98 +/- 2633.23\n",
      "Episode length: 9.15 +/- 2.73\n",
      "Eval num_timesteps=337000, episode_reward=-7916.89 +/- 2570.74\n",
      "Episode length: 10.00 +/- 1.61\n",
      "Eval num_timesteps=338000, episode_reward=-8239.63 +/- 228.98\n",
      "Episode length: 2.85 +/- 1.59\n",
      "Eval num_timesteps=339000, episode_reward=-7218.24 +/- 2918.50\n",
      "Episode length: 4.70 +/- 1.42\n",
      "Eval num_timesteps=340000, episode_reward=-7096.87 +/- 2942.74\n",
      "Episode length: 5.10 +/- 1.14\n",
      "Eval num_timesteps=341000, episode_reward=-8759.52 +/- 97.30\n",
      "Episode length: 10.20 +/- 1.12\n",
      "Eval num_timesteps=342000, episode_reward=-8164.22 +/- 81.60\n",
      "Episode length: 1.95 +/- 0.22\n",
      "Eval num_timesteps=343000, episode_reward=-8729.88 +/- 153.23\n",
      "Episode length: 9.10 +/- 1.81\n",
      "Eval num_timesteps=344000, episode_reward=-7544.59 +/- 3133.04\n",
      "Episode length: 9.40 +/- 3.46\n",
      "Eval num_timesteps=345000, episode_reward=-8166.05 +/- 2638.13\n",
      "Episode length: 9.95 +/- 2.40\n",
      "Eval num_timesteps=346000, episode_reward=-8243.51 +/- 1884.95\n",
      "Episode length: 5.75 +/- 1.81\n",
      "Eval num_timesteps=347000, episode_reward=-8455.89 +/- 1927.47\n",
      "Episode length: 10.30 +/- 2.22\n",
      "Eval num_timesteps=348000, episode_reward=-5571.80 +/- 3926.88\n",
      "Episode length: 7.85 +/- 2.67\n",
      "Eval num_timesteps=349000, episode_reward=-8469.43 +/- 1930.56\n",
      "Episode length: 9.45 +/- 2.36\n",
      "Eval num_timesteps=350000, episode_reward=-7811.91 +/- 2596.72\n",
      "Episode length: 7.75 +/- 4.44\n",
      "Eval num_timesteps=351000, episode_reward=-7294.15 +/- 3026.24\n",
      "Episode length: 4.60 +/- 1.83\n",
      "Eval num_timesteps=352000, episode_reward=-8654.39 +/- 1974.94\n",
      "Episode length: 10.10 +/- 2.26\n",
      "Eval num_timesteps=353000, episode_reward=-8204.42 +/- 1870.26\n",
      "Episode length: 6.45 +/- 2.38\n",
      "Eval num_timesteps=354000, episode_reward=-7941.83 +/- 1786.88\n",
      "Episode length: 3.40 +/- 1.24\n",
      "Eval num_timesteps=355000, episode_reward=-8739.35 +/- 131.58\n",
      "Episode length: 9.35 +/- 1.56\n",
      "Eval num_timesteps=356000, episode_reward=-7959.59 +/- 2484.06\n",
      "Episode length: 8.60 +/- 2.15\n",
      "Eval num_timesteps=357000, episode_reward=-8306.49 +/- 166.30\n",
      "Episode length: 4.15 +/- 2.85\n",
      "Eval num_timesteps=358000, episode_reward=-7287.11 +/- 3583.14\n",
      "Episode length: 9.15 +/- 3.72\n",
      "Eval num_timesteps=359000, episode_reward=-8345.43 +/- 98.98\n",
      "Episode length: 5.45 +/- 1.53\n",
      "Eval num_timesteps=360000, episode_reward=-8330.18 +/- 2440.96\n",
      "Episode length: 10.80 +/- 0.40\n",
      "Eval num_timesteps=361000, episode_reward=-8193.81 +/- 1818.57\n",
      "Episode length: 6.60 +/- 2.92\n",
      "Eval num_timesteps=362000, episode_reward=-7963.00 +/- 1816.24\n",
      "Episode length: 4.30 +/- 1.52\n",
      "Eval num_timesteps=363000, episode_reward=-7170.59 +/- 3555.40\n",
      "Episode length: 7.35 +/- 3.34\n",
      "Eval num_timesteps=364000, episode_reward=-7989.79 +/- 2635.39\n",
      "Episode length: 8.75 +/- 2.49\n",
      "Eval num_timesteps=365000, episode_reward=-7638.14 +/- 2526.49\n",
      "Episode length: 5.45 +/- 1.83\n",
      "Eval num_timesteps=366000, episode_reward=-8185.95 +/- 134.17\n",
      "Episode length: 2.40 +/- 0.66\n",
      "Eval num_timesteps=367000, episode_reward=-8726.11 +/- 428.27\n",
      "Episode length: 7.50 +/- 4.30\n",
      "Eval num_timesteps=368000, episode_reward=-8365.23 +/- 1914.57\n",
      "Episode length: 8.45 +/- 2.99\n",
      "Eval num_timesteps=369000, episode_reward=-7407.42 +/- 2450.36\n",
      "Episode length: 2.15 +/- 0.57\n",
      "Eval num_timesteps=370000, episode_reward=-8298.62 +/- 1767.88\n",
      "Episode length: 8.25 +/- 1.37\n",
      "Eval num_timesteps=371000, episode_reward=-9041.93 +/- 172.34\n",
      "Episode length: 10.70 +/- 0.71\n",
      "Eval num_timesteps=372000, episode_reward=-7927.86 +/- 1773.64\n",
      "Episode length: 5.75 +/- 1.41\n",
      "Eval num_timesteps=373000, episode_reward=-6946.68 +/- 3447.21\n",
      "Episode length: 5.70 +/- 3.16\n",
      "Eval num_timesteps=374000, episode_reward=-7594.73 +/- 2497.25\n",
      "Episode length: 5.55 +/- 1.50\n",
      "Eval num_timesteps=375000, episode_reward=-6874.00 +/- 3396.31\n",
      "Episode length: 5.20 +/- 1.83\n",
      "Eval num_timesteps=376000, episode_reward=-7025.85 +/- 3420.88\n",
      "Episode length: 7.80 +/- 3.82\n",
      "Eval num_timesteps=377000, episode_reward=-7865.23 +/- 2494.58\n",
      "Episode length: 7.70 +/- 2.10\n",
      "Eval num_timesteps=378000, episode_reward=-6526.16 +/- 3234.23\n",
      "Episode length: 2.50 +/- 1.12\n",
      "Eval num_timesteps=379000, episode_reward=-8439.55 +/- 89.99\n",
      "Episode length: 4.15 +/- 0.91\n",
      "Eval num_timesteps=380000, episode_reward=-8820.27 +/- 312.73\n",
      "Episode length: 8.15 +/- 2.26\n",
      "Eval num_timesteps=381000, episode_reward=-7483.14 +/- 3043.25\n",
      "Episode length: 6.50 +/- 2.56\n",
      "Eval num_timesteps=382000, episode_reward=-8943.79 +/- 158.39\n",
      "Episode length: 9.60 +/- 1.83\n",
      "Eval num_timesteps=383000, episode_reward=-8058.47 +/- 1859.88\n",
      "Episode length: 6.85 +/- 2.15\n",
      "Eval num_timesteps=384000, episode_reward=-7643.64 +/- 3189.28\n",
      "Episode length: 9.40 +/- 3.54\n",
      "Eval num_timesteps=385000, episode_reward=-7489.89 +/- 3034.18\n",
      "Episode length: 9.75 +/- 2.90\n",
      "Eval num_timesteps=386000, episode_reward=-7795.67 +/- 2368.05\n",
      "Episode length: 6.10 +/- 1.22\n",
      "Eval num_timesteps=387000, episode_reward=-8332.09 +/- 1876.43\n",
      "Episode length: 7.20 +/- 2.16\n",
      "Eval num_timesteps=388000, episode_reward=-8212.96 +/- 1841.06\n",
      "Episode length: 9.45 +/- 1.94\n",
      "Eval num_timesteps=389000, episode_reward=-7202.35 +/- 3516.22\n",
      "Episode length: 8.05 +/- 3.63\n",
      "Eval num_timesteps=390000, episode_reward=-6368.62 +/- 3641.26\n",
      "Episode length: 5.25 +/- 2.68\n",
      "Eval num_timesteps=391000, episode_reward=-7851.13 +/- 2603.93\n",
      "Episode length: 5.65 +/- 2.37\n",
      "Eval num_timesteps=392000, episode_reward=-9090.29 +/- 95.53\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-7474.94 +/- 2964.15\n",
      "Episode length: 7.15 +/- 2.03\n",
      "Eval num_timesteps=394000, episode_reward=-6896.37 +/- 3366.75\n",
      "Episode length: 5.30 +/- 2.83\n",
      "Eval num_timesteps=395000, episode_reward=-8004.93 +/- 2525.15\n",
      "Episode length: 9.30 +/- 2.24\n",
      "Eval num_timesteps=396000, episode_reward=-7394.13 +/- 3082.03\n",
      "Episode length: 7.20 +/- 3.03\n",
      "Eval num_timesteps=397000, episode_reward=-7539.90 +/- 2493.99\n",
      "Episode length: 4.30 +/- 2.15\n",
      "Eval num_timesteps=398000, episode_reward=-7893.12 +/- 2619.76\n",
      "Episode length: 8.05 +/- 3.25\n",
      "Eval num_timesteps=399000, episode_reward=-8036.70 +/- 1836.42\n",
      "Episode length: 5.45 +/- 1.53\n",
      "Eval num_timesteps=400000, episode_reward=-8949.87 +/- 262.44\n",
      "Episode length: 9.60 +/- 2.54\n",
      "Eval num_timesteps=401000, episode_reward=-8206.33 +/- 40.74\n",
      "Episode length: 2.10 +/- 0.30\n",
      "Eval num_timesteps=402000, episode_reward=-8544.59 +/- 1952.43\n",
      "Episode length: 9.85 +/- 2.43\n",
      "Eval num_timesteps=403000, episode_reward=-7758.62 +/- 3234.89\n",
      "Episode length: 9.50 +/- 3.57\n",
      "Eval num_timesteps=404000, episode_reward=-8534.35 +/- 1728.41\n",
      "Episode length: 10.55 +/- 0.92\n",
      "Eval num_timesteps=405000, episode_reward=-8961.31 +/- 266.40\n",
      "Episode length: 9.45 +/- 2.67\n",
      "Eval num_timesteps=406000, episode_reward=-5327.22 +/- 4129.78\n",
      "Episode length: 7.45 +/- 2.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=407000, episode_reward=-7086.13 +/- 2961.07\n",
      "Episode length: 5.20 +/- 3.04\n",
      "Eval num_timesteps=408000, episode_reward=-7349.87 +/- 3070.29\n",
      "Episode length: 6.05 +/- 2.99\n",
      "Eval num_timesteps=409000, episode_reward=-7189.31 +/- 3015.03\n",
      "Episode length: 4.90 +/- 0.99\n",
      "Eval num_timesteps=410000, episode_reward=-7652.91 +/- 2349.71\n",
      "Episode length: 5.70 +/- 2.61\n",
      "Eval num_timesteps=411000, episode_reward=-8534.90 +/- 168.26\n",
      "Episode length: 5.95 +/- 1.96\n",
      "Eval num_timesteps=412000, episode_reward=-7783.00 +/- 2519.83\n",
      "Episode length: 8.25 +/- 2.36\n",
      "Eval num_timesteps=413000, episode_reward=-6820.64 +/- 3336.19\n",
      "Episode length: 4.15 +/- 1.39\n",
      "Eval num_timesteps=414000, episode_reward=-7458.03 +/- 2492.95\n",
      "Episode length: 3.90 +/- 1.87\n",
      "Eval num_timesteps=415000, episode_reward=-7804.35 +/- 2501.66\n",
      "Episode length: 8.25 +/- 2.05\n",
      "Eval num_timesteps=416000, episode_reward=-8686.77 +/- 169.53\n",
      "Episode length: 7.05 +/- 1.83\n",
      "Eval num_timesteps=417000, episode_reward=-7843.70 +/- 2595.75\n",
      "Episode length: 9.30 +/- 2.74\n",
      "Eval num_timesteps=418000, episode_reward=-6931.98 +/- 3389.93\n",
      "Episode length: 6.10 +/- 2.53\n",
      "Eval num_timesteps=419000, episode_reward=-8061.60 +/- 1793.54\n",
      "Episode length: 6.60 +/- 0.97\n",
      "Eval num_timesteps=420000, episode_reward=-8412.62 +/- 1919.75\n",
      "Episode length: 10.50 +/- 2.18\n",
      "Eval num_timesteps=421000, episode_reward=-7203.25 +/- 2987.67\n",
      "Episode length: 5.85 +/- 1.77\n",
      "Eval num_timesteps=422000, episode_reward=-8346.12 +/- 86.43\n",
      "Episode length: 6.40 +/- 0.80\n",
      "Eval num_timesteps=423000, episode_reward=-7161.91 +/- 2941.36\n",
      "Episode length: 6.20 +/- 1.99\n",
      "Eval num_timesteps=424000, episode_reward=-7293.76 +/- 3605.44\n",
      "Episode length: 9.05 +/- 3.90\n",
      "Eval num_timesteps=425000, episode_reward=-8501.13 +/- 1936.36\n",
      "Episode length: 10.25 +/- 1.73\n",
      "Eval num_timesteps=426000, episode_reward=-8996.66 +/- 171.15\n",
      "Episode length: 10.75 +/- 1.09\n",
      "Eval num_timesteps=427000, episode_reward=-7790.27 +/- 2473.89\n",
      "Episode length: 8.30 +/- 1.49\n",
      "Eval num_timesteps=428000, episode_reward=-7622.97 +/- 2527.33\n",
      "Episode length: 5.85 +/- 2.97\n",
      "Eval num_timesteps=429000, episode_reward=-8512.68 +/- 121.49\n",
      "Episode length: 7.60 +/- 1.28\n",
      "Eval num_timesteps=430000, episode_reward=-7836.69 +/- 2599.68\n",
      "Episode length: 7.30 +/- 2.65\n",
      "Eval num_timesteps=431000, episode_reward=-7575.67 +/- 3152.37\n",
      "Episode length: 9.25 +/- 3.22\n",
      "Eval num_timesteps=432000, episode_reward=-7208.69 +/- 2899.32\n",
      "Episode length: 5.95 +/- 1.56\n",
      "Eval num_timesteps=433000, episode_reward=-6578.50 +/- 3721.25\n",
      "Episode length: 7.50 +/- 3.43\n",
      "Eval num_timesteps=434000, episode_reward=-7488.41 +/- 2471.39\n",
      "Episode length: 4.70 +/- 1.23\n",
      "Eval num_timesteps=435000, episode_reward=-7961.89 +/- 1830.42\n",
      "Episode length: 4.05 +/- 2.52\n",
      "Eval num_timesteps=436000, episode_reward=-8608.34 +/- 1939.64\n",
      "Episode length: 10.55 +/- 1.96\n",
      "Eval num_timesteps=437000, episode_reward=-6612.54 +/- 3787.87\n",
      "Episode length: 6.60 +/- 3.99\n",
      "Eval num_timesteps=438000, episode_reward=-6783.06 +/- 3333.73\n",
      "Episode length: 5.50 +/- 2.16\n",
      "Eval num_timesteps=439000, episode_reward=-7673.80 +/- 2503.02\n",
      "Episode length: 5.90 +/- 1.58\n",
      "Eval num_timesteps=440000, episode_reward=-6509.44 +/- 3632.76\n",
      "Episode length: 6.85 +/- 2.15\n",
      "Eval num_timesteps=441000, episode_reward=-6242.00 +/- 3778.61\n",
      "Episode length: 7.15 +/- 1.71\n",
      "Eval num_timesteps=442000, episode_reward=-7562.35 +/- 2482.16\n",
      "Episode length: 5.90 +/- 1.81\n",
      "Eval num_timesteps=443000, episode_reward=-8455.83 +/- 1907.69\n",
      "Episode length: 10.40 +/- 1.66\n",
      "Eval num_timesteps=444000, episode_reward=-7163.20 +/- 3431.44\n",
      "Episode length: 9.45 +/- 3.15\n",
      "Eval num_timesteps=445000, episode_reward=-8434.53 +/- 1899.60\n",
      "Episode length: 10.55 +/- 1.96\n",
      "Eval num_timesteps=446000, episode_reward=-8710.47 +/- 139.05\n",
      "Episode length: 9.80 +/- 1.60\n",
      "Eval num_timesteps=447000, episode_reward=-8188.18 +/- 1763.82\n",
      "Episode length: 6.45 +/- 0.97\n",
      "Eval num_timesteps=448000, episode_reward=-8536.02 +/- 92.31\n",
      "Episode length: 7.25 +/- 1.48\n",
      "Eval num_timesteps=449000, episode_reward=-7545.77 +/- 3046.71\n",
      "Episode length: 9.95 +/- 2.40\n",
      "Eval num_timesteps=450000, episode_reward=-7988.49 +/- 1819.37\n",
      "Episode length: 6.10 +/- 1.51\n",
      "Eval num_timesteps=451000, episode_reward=-8300.31 +/- 1906.59\n",
      "Episode length: 8.25 +/- 2.57\n",
      "Eval num_timesteps=452000, episode_reward=-7866.38 +/- 2604.61\n",
      "Episode length: 7.70 +/- 3.16\n",
      "Eval num_timesteps=453000, episode_reward=-8044.04 +/- 1833.67\n",
      "Episode length: 6.80 +/- 1.72\n",
      "Eval num_timesteps=454000, episode_reward=-7755.86 +/- 2565.20\n",
      "Episode length: 7.70 +/- 2.39\n",
      "Eval num_timesteps=455000, episode_reward=-6982.58 +/- 3433.48\n",
      "Episode length: 7.45 +/- 3.32\n",
      "Eval num_timesteps=456000, episode_reward=-8007.72 +/- 2599.94\n",
      "Episode length: 7.70 +/- 2.51\n",
      "Eval num_timesteps=457000, episode_reward=-7883.43 +/- 2609.24\n",
      "Episode length: 8.35 +/- 2.73\n",
      "Eval num_timesteps=458000, episode_reward=-7175.32 +/- 2991.38\n",
      "Episode length: 5.90 +/- 2.49\n",
      "Eval num_timesteps=459000, episode_reward=-8412.85 +/- 161.11\n",
      "Episode length: 6.50 +/- 1.63\n",
      "Eval num_timesteps=460000, episode_reward=-8884.73 +/- 150.67\n",
      "Episode length: 10.25 +/- 1.22\n",
      "Eval num_timesteps=461000, episode_reward=-7719.26 +/- 2537.52\n",
      "Episode length: 7.65 +/- 1.56\n",
      "Eval num_timesteps=462000, episode_reward=-8353.77 +/- 137.36\n",
      "Episode length: 4.70 +/- 2.35\n",
      "Eval num_timesteps=463000, episode_reward=-8276.01 +/- 1888.83\n",
      "Episode length: 9.20 +/- 2.27\n",
      "Eval num_timesteps=464000, episode_reward=-8655.22 +/- 259.89\n",
      "Episode length: 8.10 +/- 1.79\n",
      "Eval num_timesteps=465000, episode_reward=-8741.85 +/- 161.06\n",
      "Episode length: 9.70 +/- 1.62\n",
      "Eval num_timesteps=466000, episode_reward=-7547.17 +/- 3129.10\n",
      "Episode length: 9.50 +/- 3.07\n",
      "Eval num_timesteps=467000, episode_reward=-7672.52 +/- 2537.21\n",
      "Episode length: 6.55 +/- 2.52\n",
      "Eval num_timesteps=468000, episode_reward=-7377.97 +/- 3022.46\n",
      "Episode length: 8.35 +/- 2.92\n",
      "Eval num_timesteps=469000, episode_reward=-8251.52 +/- 1885.61\n",
      "Episode length: 8.00 +/- 2.32\n",
      "Eval num_timesteps=470000, episode_reward=-8111.94 +/- 1882.92\n",
      "Episode length: 8.25 +/- 1.84\n",
      "Eval num_timesteps=471000, episode_reward=-6959.32 +/- 3450.86\n",
      "Episode length: 7.95 +/- 3.58\n",
      "Eval num_timesteps=472000, episode_reward=-7781.39 +/- 2557.77\n",
      "Episode length: 7.85 +/- 2.74\n",
      "Eval num_timesteps=473000, episode_reward=-7234.15 +/- 3009.89\n",
      "Episode length: 5.70 +/- 1.55\n",
      "Eval num_timesteps=474000, episode_reward=-6456.51 +/- 3452.80\n",
      "Episode length: 6.45 +/- 1.20\n",
      "Eval num_timesteps=475000, episode_reward=-7439.27 +/- 2424.70\n",
      "Episode length: 5.30 +/- 0.64\n",
      "Eval num_timesteps=476000, episode_reward=-8051.36 +/- 1812.96\n",
      "Episode length: 6.95 +/- 0.92\n",
      "Eval num_timesteps=477000, episode_reward=-6835.53 +/- 3375.49\n",
      "Episode length: 5.80 +/- 2.34\n",
      "Eval num_timesteps=478000, episode_reward=-7834.60 +/- 2599.28\n",
      "Episode length: 7.35 +/- 2.41\n",
      "Eval num_timesteps=479000, episode_reward=-7522.02 +/- 2488.17\n",
      "Episode length: 5.55 +/- 1.77\n",
      "Eval num_timesteps=480000, episode_reward=-7416.01 +/- 3052.92\n",
      "Episode length: 8.50 +/- 3.02\n",
      "Eval num_timesteps=481000, episode_reward=-7624.04 +/- 2520.13\n",
      "Episode length: 6.50 +/- 2.09\n",
      "Eval num_timesteps=482000, episode_reward=-7751.33 +/- 2514.26\n",
      "Episode length: 7.55 +/- 1.66\n",
      "Eval num_timesteps=483000, episode_reward=-6047.98 +/- 3940.18\n",
      "Episode length: 6.40 +/- 3.67\n",
      "Eval num_timesteps=484000, episode_reward=-7130.90 +/- 2968.99\n",
      "Episode length: 5.55 +/- 2.18\n",
      "Eval num_timesteps=485000, episode_reward=-6728.17 +/- 3338.57\n",
      "Episode length: 4.95 +/- 2.13\n",
      "Eval num_timesteps=486000, episode_reward=-6441.16 +/- 3597.43\n",
      "Episode length: 7.15 +/- 2.17\n",
      "Eval num_timesteps=487000, episode_reward=-7513.99 +/- 3137.75\n",
      "Episode length: 8.45 +/- 3.31\n",
      "Eval num_timesteps=488000, episode_reward=-8503.06 +/- 48.85\n",
      "Episode length: 7.20 +/- 1.17\n",
      "Eval num_timesteps=489000, episode_reward=-7154.13 +/- 2982.97\n",
      "Episode length: 6.05 +/- 1.96\n",
      "Eval num_timesteps=490000, episode_reward=-5844.31 +/- 3779.68\n",
      "Episode length: 4.85 +/- 1.39\n",
      "Eval num_timesteps=491000, episode_reward=-7486.42 +/- 2468.30\n",
      "Episode length: 5.50 +/- 1.53\n",
      "Eval num_timesteps=492000, episode_reward=-6861.76 +/- 3434.77\n",
      "Episode length: 6.95 +/- 2.69\n",
      "Eval num_timesteps=493000, episode_reward=-7800.25 +/- 1782.35\n",
      "Episode length: 2.90 +/- 1.51\n",
      "Eval num_timesteps=494000, episode_reward=-8818.26 +/- 138.72\n",
      "Episode length: 10.60 +/- 1.02\n",
      "Eval num_timesteps=495000, episode_reward=-7463.61 +/- 3109.55\n",
      "Episode length: 9.00 +/- 3.11\n",
      "Eval num_timesteps=496000, episode_reward=-8176.93 +/- 1782.00\n",
      "Episode length: 8.25 +/- 2.05\n",
      "Eval num_timesteps=497000, episode_reward=-7662.30 +/- 2492.07\n",
      "Episode length: 6.00 +/- 1.52\n",
      "Eval num_timesteps=498000, episode_reward=-6779.12 +/- 3295.44\n",
      "Episode length: 5.85 +/- 1.90\n",
      "Eval num_timesteps=499000, episode_reward=-7923.11 +/- 1808.64\n",
      "Episode length: 4.45 +/- 1.43\n",
      "Eval num_timesteps=500000, episode_reward=-7433.18 +/- 2464.50\n",
      "Episode length: 3.90 +/- 2.02\n",
      "Eval num_timesteps=501000, episode_reward=-7538.62 +/- 2493.75\n",
      "Episode length: 5.85 +/- 2.08\n",
      "Eval num_timesteps=502000, episode_reward=-8503.37 +/- 103.71\n",
      "Episode length: 7.50 +/- 0.87\n",
      "Eval num_timesteps=503000, episode_reward=-7925.25 +/- 1823.08\n",
      "Episode length: 5.80 +/- 2.52\n",
      "Eval num_timesteps=504000, episode_reward=-7845.69 +/- 2540.85\n",
      "Episode length: 9.00 +/- 2.37\n",
      "Eval num_timesteps=505000, episode_reward=-8193.43 +/- 1875.72\n",
      "Episode length: 8.10 +/- 2.49\n",
      "Eval num_timesteps=506000, episode_reward=-6590.94 +/- 3663.91\n",
      "Episode length: 8.00 +/- 3.11\n",
      "Eval num_timesteps=507000, episode_reward=-5654.33 +/- 4023.57\n",
      "Episode length: 6.75 +/- 2.86\n",
      "Eval num_timesteps=508000, episode_reward=-7793.45 +/- 2578.58\n",
      "Episode length: 7.90 +/- 2.95\n",
      "Eval num_timesteps=509000, episode_reward=-7410.44 +/- 3089.59\n",
      "Episode length: 8.25 +/- 3.51\n",
      "Eval num_timesteps=510000, episode_reward=-6820.22 +/- 3381.16\n",
      "Episode length: 6.20 +/- 3.04\n",
      "Eval num_timesteps=511000, episode_reward=-6136.15 +/- 3900.90\n",
      "Episode length: 7.50 +/- 3.83\n",
      "Eval num_timesteps=512000, episode_reward=-6968.68 +/- 3454.25\n",
      "Episode length: 7.60 +/- 3.72\n",
      "Eval num_timesteps=513000, episode_reward=-7515.98 +/- 3061.98\n",
      "Episode length: 8.80 +/- 2.80\n",
      "Eval num_timesteps=514000, episode_reward=-6183.05 +/- 3534.61\n",
      "Episode length: 3.95 +/- 1.77\n",
      "Eval num_timesteps=515000, episode_reward=-7083.25 +/- 3389.14\n",
      "Episode length: 9.60 +/- 2.46\n",
      "Eval num_timesteps=516000, episode_reward=-6894.71 +/- 3355.75\n",
      "Episode length: 7.20 +/- 2.98\n",
      "Eval num_timesteps=517000, episode_reward=-7753.69 +/- 2534.31\n",
      "Episode length: 7.90 +/- 2.32\n",
      "Eval num_timesteps=518000, episode_reward=-8292.86 +/- 1840.35\n",
      "Episode length: 8.90 +/- 1.64\n",
      "Eval num_timesteps=519000, episode_reward=-8350.14 +/- 127.50\n",
      "Episode length: 5.65 +/- 1.01\n",
      "Eval num_timesteps=520000, episode_reward=-8905.28 +/- 136.84\n",
      "Episode length: 10.95 +/- 0.22\n",
      "Eval num_timesteps=521000, episode_reward=-6866.65 +/- 3332.46\n",
      "Episode length: 6.70 +/- 1.65\n",
      "Eval num_timesteps=522000, episode_reward=-6972.35 +/- 3361.76\n",
      "Episode length: 8.25 +/- 2.19\n",
      "Eval num_timesteps=523000, episode_reward=-5717.63 +/- 4073.24\n",
      "Episode length: 6.60 +/- 3.40\n",
      "Eval num_timesteps=524000, episode_reward=-8398.66 +/- 81.11\n",
      "Episode length: 6.45 +/- 1.12\n",
      "Eval num_timesteps=525000, episode_reward=-7623.98 +/- 2527.43\n",
      "Episode length: 6.40 +/- 2.69\n",
      "Eval num_timesteps=526000, episode_reward=-6852.92 +/- 3291.48\n",
      "Episode length: 6.25 +/- 1.64\n",
      "Eval num_timesteps=527000, episode_reward=-5364.13 +/- 4107.57\n",
      "Episode length: 7.80 +/- 3.16\n",
      "Eval num_timesteps=528000, episode_reward=-6559.95 +/- 3749.52\n",
      "Episode length: 8.10 +/- 3.36\n",
      "Eval num_timesteps=529000, episode_reward=-7766.58 +/- 2570.15\n",
      "Episode length: 7.90 +/- 2.75\n",
      "Eval num_timesteps=530000, episode_reward=-6963.72 +/- 3452.74\n",
      "Episode length: 7.80 +/- 4.01\n",
      "Eval num_timesteps=531000, episode_reward=-6584.59 +/- 3263.79\n",
      "Episode length: 4.35 +/- 1.24\n",
      "Eval num_timesteps=532000, episode_reward=-8216.43 +/- 1880.31\n",
      "Episode length: 7.75 +/- 2.30\n",
      "Eval num_timesteps=533000, episode_reward=-6906.81 +/- 3376.26\n",
      "Episode length: 7.70 +/- 2.49\n",
      "Eval num_timesteps=534000, episode_reward=-5116.41 +/- 4130.23\n",
      "Episode length: 4.80 +/- 3.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=535000, episode_reward=-8137.52 +/- 1856.30\n",
      "Episode length: 7.90 +/- 1.92\n",
      "Eval num_timesteps=536000, episode_reward=-7416.82 +/- 3039.07\n",
      "Episode length: 8.00 +/- 3.07\n",
      "Eval num_timesteps=537000, episode_reward=-8069.46 +/- 1845.22\n",
      "Episode length: 7.15 +/- 2.31\n",
      "Eval num_timesteps=538000, episode_reward=-7418.14 +/- 3042.66\n",
      "Episode length: 8.15 +/- 3.12\n",
      "Eval num_timesteps=539000, episode_reward=-7276.20 +/- 3032.63\n",
      "Episode length: 6.85 +/- 2.71\n",
      "Eval num_timesteps=540000, episode_reward=-7915.99 +/- 2539.88\n",
      "Episode length: 8.25 +/- 1.89\n",
      "Eval num_timesteps=541000, episode_reward=-7307.05 +/- 3019.83\n",
      "Episode length: 7.85 +/- 2.82\n",
      "Eval num_timesteps=542000, episode_reward=-7027.07 +/- 3485.06\n",
      "Episode length: 9.25 +/- 3.01\n",
      "Eval num_timesteps=543000, episode_reward=-7399.76 +/- 3082.74\n",
      "Episode length: 7.75 +/- 3.24\n",
      "Eval num_timesteps=544000, episode_reward=-7316.00 +/- 3103.71\n",
      "Episode length: 7.65 +/- 2.71\n",
      "Eval num_timesteps=545000, episode_reward=-8222.50 +/- 1877.01\n",
      "Episode length: 8.80 +/- 2.54\n",
      "Eval num_timesteps=546000, episode_reward=-6570.27 +/- 3779.77\n",
      "Episode length: 8.35 +/- 3.60\n",
      "Eval num_timesteps=547000, episode_reward=-7539.77 +/- 3110.20\n",
      "Episode length: 9.50 +/- 3.26\n",
      "Eval num_timesteps=548000, episode_reward=-7775.58 +/- 2583.62\n",
      "Episode length: 7.10 +/- 2.34\n",
      "Eval num_timesteps=549000, episode_reward=-6914.19 +/- 3421.53\n",
      "Episode length: 7.00 +/- 2.88\n",
      "Eval num_timesteps=550000, episode_reward=-7181.82 +/- 2991.52\n",
      "Episode length: 4.70 +/- 1.65\n",
      "Eval num_timesteps=551000, episode_reward=-7321.54 +/- 3053.05\n",
      "Episode length: 8.05 +/- 2.97\n",
      "Eval num_timesteps=552000, episode_reward=-7691.41 +/- 2550.54\n",
      "Episode length: 7.45 +/- 2.94\n",
      "Eval num_timesteps=553000, episode_reward=-7386.59 +/- 3068.69\n",
      "Episode length: 7.80 +/- 2.48\n",
      "Eval num_timesteps=554000, episode_reward=-6912.79 +/- 3384.53\n",
      "Episode length: 7.35 +/- 2.80\n",
      "Eval num_timesteps=555000, episode_reward=-6887.28 +/- 3406.31\n",
      "Episode length: 6.55 +/- 2.60\n",
      "Eval num_timesteps=556000, episode_reward=-7109.79 +/- 2961.56\n",
      "Episode length: 5.20 +/- 2.06\n",
      "Eval num_timesteps=557000, episode_reward=-7352.75 +/- 3072.01\n",
      "Episode length: 6.95 +/- 3.47\n",
      "Eval num_timesteps=558000, episode_reward=-6142.26 +/- 3851.83\n",
      "Episode length: 7.90 +/- 3.18\n",
      "Eval num_timesteps=559000, episode_reward=-7694.37 +/- 2489.28\n",
      "Episode length: 6.50 +/- 1.66\n",
      "Eval num_timesteps=560000, episode_reward=-5889.80 +/- 3807.52\n",
      "Episode length: 5.30 +/- 2.43\n",
      "Eval num_timesteps=561000, episode_reward=-6371.12 +/- 3653.69\n",
      "Episode length: 5.95 +/- 2.82\n",
      "Eval num_timesteps=562000, episode_reward=-6800.72 +/- 3383.81\n",
      "Episode length: 5.50 +/- 1.88\n",
      "Eval num_timesteps=563000, episode_reward=-7886.58 +/- 2610.14\n",
      "Episode length: 9.40 +/- 2.94\n",
      "Eval num_timesteps=564000, episode_reward=-7005.46 +/- 3472.49\n",
      "Episode length: 8.00 +/- 3.61\n",
      "Eval num_timesteps=565000, episode_reward=-6482.56 +/- 3702.82\n",
      "Episode length: 6.80 +/- 3.66\n",
      "Eval num_timesteps=566000, episode_reward=-8587.93 +/- 98.10\n",
      "Episode length: 7.75 +/- 1.55\n",
      "Eval num_timesteps=567000, episode_reward=-7345.03 +/- 2990.59\n",
      "Episode length: 7.60 +/- 2.52\n",
      "Eval num_timesteps=568000, episode_reward=-6803.06 +/- 3372.02\n",
      "Episode length: 5.80 +/- 2.82\n",
      "Eval num_timesteps=569000, episode_reward=-7955.35 +/- 1769.00\n",
      "Episode length: 6.35 +/- 1.42\n",
      "Eval num_timesteps=570000, episode_reward=-6898.36 +/- 3415.87\n",
      "Episode length: 7.05 +/- 2.80\n",
      "Eval num_timesteps=571000, episode_reward=-6807.20 +/- 3418.01\n",
      "Episode length: 6.60 +/- 2.58\n",
      "Eval num_timesteps=572000, episode_reward=-6991.88 +/- 3449.60\n",
      "Episode length: 8.30 +/- 3.33\n",
      "Eval num_timesteps=573000, episode_reward=-4405.49 +/- 4311.70\n",
      "Episode length: 6.80 +/- 3.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=574000, episode_reward=-7886.72 +/- 2610.37\n",
      "Episode length: 8.75 +/- 2.83\n",
      "Eval num_timesteps=575000, episode_reward=-7739.43 +/- 2415.42\n",
      "Episode length: 6.75 +/- 0.94\n",
      "Eval num_timesteps=576000, episode_reward=-5645.28 +/- 4047.30\n",
      "Episode length: 6.40 +/- 3.31\n",
      "Eval num_timesteps=577000, episode_reward=-8188.81 +/- 1722.66\n",
      "Episode length: 7.00 +/- 1.34\n",
      "Eval num_timesteps=578000, episode_reward=-8167.77 +/- 1868.31\n",
      "Episode length: 7.20 +/- 2.29\n",
      "Eval num_timesteps=579000, episode_reward=-6525.65 +/- 3621.69\n",
      "Episode length: 7.95 +/- 2.48\n",
      "Eval num_timesteps=580000, episode_reward=-6029.74 +/- 3842.77\n",
      "Episode length: 7.05 +/- 3.07\n",
      "Eval num_timesteps=581000, episode_reward=-7042.87 +/- 3425.91\n",
      "Episode length: 8.15 +/- 2.73\n",
      "Eval num_timesteps=582000, episode_reward=-6055.61 +/- 3912.19\n",
      "Episode length: 6.55 +/- 2.64\n",
      "Eval num_timesteps=583000, episode_reward=-6335.87 +/- 3513.87\n",
      "Episode length: 5.65 +/- 1.62\n",
      "Eval num_timesteps=584000, episode_reward=-7611.09 +/- 2521.75\n",
      "Episode length: 6.10 +/- 2.61\n",
      "Eval num_timesteps=585000, episode_reward=-7782.17 +/- 1774.62\n",
      "Episode length: 2.95 +/- 0.92\n",
      "Eval num_timesteps=586000, episode_reward=-6390.15 +/- 3687.36\n",
      "Episode length: 6.05 +/- 2.85\n",
      "Eval num_timesteps=587000, episode_reward=-7010.22 +/- 3430.24\n",
      "Episode length: 7.95 +/- 2.87\n",
      "Eval num_timesteps=588000, episode_reward=-6190.19 +/- 4013.14\n",
      "Episode length: 7.75 +/- 4.23\n",
      "Eval num_timesteps=589000, episode_reward=-7538.31 +/- 3143.92\n",
      "Episode length: 9.50 +/- 3.57\n",
      "Eval num_timesteps=590000, episode_reward=-5658.98 +/- 4107.52\n",
      "Episode length: 6.70 +/- 4.46\n",
      "Eval num_timesteps=591000, episode_reward=-7299.92 +/- 3053.62\n",
      "Episode length: 7.35 +/- 2.71\n",
      "Eval num_timesteps=592000, episode_reward=-4401.01 +/- 4275.78\n",
      "Episode length: 6.05 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=593000, episode_reward=-6283.41 +/- 3558.23\n",
      "Episode length: 5.05 +/- 1.94\n",
      "Eval num_timesteps=594000, episode_reward=-6761.65 +/- 3357.62\n",
      "Episode length: 5.85 +/- 2.57\n",
      "Eval num_timesteps=595000, episode_reward=-7480.10 +/- 2481.49\n",
      "Episode length: 5.35 +/- 1.24\n",
      "Eval num_timesteps=596000, episode_reward=-7679.84 +/- 2545.98\n",
      "Episode length: 7.00 +/- 2.95\n",
      "Eval num_timesteps=597000, episode_reward=-7335.11 +/- 3050.13\n",
      "Episode length: 7.25 +/- 2.81\n",
      "Eval num_timesteps=598000, episode_reward=-5798.69 +/- 4010.45\n",
      "Episode length: 8.85 +/- 2.17\n",
      "Eval num_timesteps=599000, episode_reward=-8023.75 +/- 1838.24\n",
      "Episode length: 7.05 +/- 2.04\n",
      "Eval num_timesteps=600000, episode_reward=-7605.28 +/- 2453.09\n",
      "Episode length: 6.20 +/- 1.57\n",
      "Eval num_timesteps=601000, episode_reward=-7595.57 +/- 2495.60\n",
      "Episode length: 5.90 +/- 1.45\n",
      "Eval num_timesteps=602000, episode_reward=-4882.01 +/- 4041.53\n",
      "Episode length: 6.65 +/- 3.57\n",
      "Eval num_timesteps=603000, episode_reward=-8402.72 +/- 154.60\n",
      "Episode length: 5.75 +/- 1.84\n",
      "Eval num_timesteps=604000, episode_reward=-5975.35 +/- 3892.27\n",
      "Episode length: 5.40 +/- 2.48\n",
      "Eval num_timesteps=605000, episode_reward=-4751.19 +/- 4164.61\n",
      "Episode length: 6.10 +/- 2.23\n",
      "Eval num_timesteps=606000, episode_reward=-8244.90 +/- 1901.69\n",
      "Episode length: 8.70 +/- 2.63\n",
      "Eval num_timesteps=607000, episode_reward=-6021.91 +/- 3904.99\n",
      "Episode length: 5.75 +/- 3.55\n",
      "Eval num_timesteps=608000, episode_reward=-7232.10 +/- 3016.27\n",
      "Episode length: 6.40 +/- 2.80\n",
      "Eval num_timesteps=609000, episode_reward=-6787.11 +/- 3384.61\n",
      "Episode length: 5.95 +/- 2.73\n",
      "Eval num_timesteps=610000, episode_reward=-6819.23 +/- 3308.43\n",
      "Episode length: 6.30 +/- 1.35\n",
      "Eval num_timesteps=611000, episode_reward=-5136.08 +/- 4038.08\n",
      "Episode length: 5.60 +/- 1.85\n",
      "Eval num_timesteps=612000, episode_reward=-4802.08 +/- 4241.55\n",
      "Episode length: 6.60 +/- 3.25\n",
      "Eval num_timesteps=613000, episode_reward=-6807.42 +/- 3347.22\n",
      "Episode length: 6.15 +/- 2.41\n",
      "Eval num_timesteps=614000, episode_reward=-7691.49 +/- 2502.82\n",
      "Episode length: 7.25 +/- 3.78\n",
      "Eval num_timesteps=615000, episode_reward=-6386.15 +/- 3668.16\n",
      "Episode length: 6.00 +/- 2.95\n",
      "Eval num_timesteps=616000, episode_reward=-6815.80 +/- 3383.06\n",
      "Episode length: 5.70 +/- 2.57\n",
      "Eval num_timesteps=617000, episode_reward=-7491.21 +/- 3124.89\n",
      "Episode length: 9.50 +/- 3.57\n",
      "Eval num_timesteps=618000, episode_reward=-6485.60 +/- 3652.62\n",
      "Episode length: 7.40 +/- 2.97\n",
      "Eval num_timesteps=619000, episode_reward=-6415.93 +/- 3673.46\n",
      "Episode length: 5.95 +/- 2.44\n",
      "Eval num_timesteps=620000, episode_reward=-6015.20 +/- 3771.34\n",
      "Episode length: 7.00 +/- 2.26\n",
      "Eval num_timesteps=621000, episode_reward=-7815.97 +/- 2592.83\n",
      "Episode length: 8.10 +/- 2.36\n",
      "Eval num_timesteps=622000, episode_reward=-5512.41 +/- 3943.47\n",
      "Episode length: 4.60 +/- 3.38\n",
      "Eval num_timesteps=623000, episode_reward=-7505.46 +/- 3075.49\n",
      "Episode length: 9.20 +/- 2.79\n",
      "Eval num_timesteps=624000, episode_reward=-6810.25 +/- 3355.86\n",
      "Episode length: 6.10 +/- 2.98\n",
      "Eval num_timesteps=625000, episode_reward=-5055.85 +/- 4081.50\n",
      "Episode length: 5.05 +/- 2.13\n",
      "Eval num_timesteps=626000, episode_reward=-6449.34 +/- 3667.61\n",
      "Episode length: 7.20 +/- 2.94\n",
      "Eval num_timesteps=627000, episode_reward=-7449.91 +/- 3113.93\n",
      "Episode length: 8.70 +/- 2.67\n",
      "Eval num_timesteps=628000, episode_reward=-5709.35 +/- 3953.63\n",
      "Episode length: 6.90 +/- 2.74\n",
      "Eval num_timesteps=629000, episode_reward=-5315.32 +/- 4131.26\n",
      "Episode length: 7.35 +/- 3.66\n",
      "Eval num_timesteps=630000, episode_reward=-7603.54 +/- 2560.29\n",
      "Episode length: 6.25 +/- 1.22\n",
      "Eval num_timesteps=631000, episode_reward=-6615.69 +/- 3278.95\n",
      "Episode length: 4.20 +/- 2.09\n",
      "Eval num_timesteps=632000, episode_reward=-7681.87 +/- 2542.82\n",
      "Episode length: 7.15 +/- 2.85\n",
      "Eval num_timesteps=633000, episode_reward=-6581.40 +/- 3758.97\n",
      "Episode length: 8.00 +/- 3.70\n",
      "Eval num_timesteps=634000, episode_reward=-8057.93 +/- 1837.88\n",
      "Episode length: 6.90 +/- 1.79\n",
      "Eval num_timesteps=635000, episode_reward=-7247.61 +/- 3030.46\n",
      "Episode length: 6.50 +/- 2.16\n",
      "Eval num_timesteps=636000, episode_reward=-7272.05 +/- 3069.90\n",
      "Episode length: 7.30 +/- 2.57\n",
      "Eval num_timesteps=637000, episode_reward=-7277.91 +/- 2987.64\n",
      "Episode length: 7.25 +/- 1.97\n",
      "Eval num_timesteps=638000, episode_reward=-6409.69 +/- 3673.93\n",
      "Episode length: 5.60 +/- 2.58\n",
      "Eval num_timesteps=639000, episode_reward=-6687.13 +/- 3319.77\n",
      "Episode length: 4.70 +/- 2.39\n",
      "Eval num_timesteps=640000, episode_reward=-6572.71 +/- 3735.19\n",
      "Episode length: 7.50 +/- 3.32\n",
      "Eval num_timesteps=641000, episode_reward=-7708.06 +/- 2530.16\n",
      "Episode length: 7.60 +/- 2.27\n",
      "Eval num_timesteps=642000, episode_reward=-5454.35 +/- 3967.23\n",
      "Episode length: 4.05 +/- 2.29\n",
      "Eval num_timesteps=643000, episode_reward=-6808.15 +/- 3391.62\n",
      "Episode length: 6.30 +/- 3.07\n",
      "Eval num_timesteps=644000, episode_reward=-5632.82 +/- 4084.83\n",
      "Episode length: 6.10 +/- 2.59\n",
      "Eval num_timesteps=645000, episode_reward=-7798.48 +/- 1785.94\n",
      "Episode length: 4.60 +/- 0.66\n",
      "Eval num_timesteps=646000, episode_reward=-6973.21 +/- 3468.11\n",
      "Episode length: 8.30 +/- 3.80\n",
      "Eval num_timesteps=647000, episode_reward=-6165.43 +/- 3616.41\n",
      "Episode length: 4.25 +/- 2.47\n",
      "Eval num_timesteps=648000, episode_reward=-6502.11 +/- 3659.75\n",
      "Episode length: 7.20 +/- 3.19\n",
      "Eval num_timesteps=649000, episode_reward=-5967.15 +/- 3899.86\n",
      "Episode length: 6.05 +/- 3.14\n",
      "Eval num_timesteps=650000, episode_reward=-8051.16 +/- 1842.09\n",
      "Episode length: 5.90 +/- 2.19\n",
      "Eval num_timesteps=651000, episode_reward=-7018.38 +/- 2922.67\n",
      "Episode length: 4.40 +/- 1.53\n",
      "Eval num_timesteps=652000, episode_reward=-8065.97 +/- 1843.30\n",
      "Episode length: 7.15 +/- 2.43\n",
      "Eval num_timesteps=653000, episode_reward=-5672.21 +/- 4147.23\n",
      "Episode length: 7.30 +/- 3.18\n",
      "Eval num_timesteps=654000, episode_reward=-8494.96 +/- 78.05\n",
      "Episode length: 7.55 +/- 1.12\n",
      "Eval num_timesteps=655000, episode_reward=-5135.25 +/- 4094.20\n",
      "Episode length: 5.15 +/- 2.97\n",
      "Eval num_timesteps=656000, episode_reward=-6174.12 +/- 4046.54\n",
      "Episode length: 8.45 +/- 3.98\n",
      "Eval num_timesteps=657000, episode_reward=-7870.21 +/- 1801.56\n",
      "Episode length: 5.40 +/- 0.86\n",
      "Eval num_timesteps=658000, episode_reward=-7039.55 +/- 3461.64\n",
      "Episode length: 9.60 +/- 2.91\n",
      "Eval num_timesteps=659000, episode_reward=-7467.01 +/- 3113.06\n",
      "Episode length: 9.00 +/- 3.48\n",
      "Eval num_timesteps=660000, episode_reward=-6860.71 +/- 3418.61\n",
      "Episode length: 6.55 +/- 2.99\n",
      "Eval num_timesteps=661000, episode_reward=-6459.80 +/- 3662.02\n",
      "Episode length: 7.25 +/- 2.62\n",
      "Eval num_timesteps=662000, episode_reward=-5515.87 +/- 3999.94\n",
      "Episode length: 5.25 +/- 2.49\n",
      "Eval num_timesteps=663000, episode_reward=-7325.22 +/- 2986.90\n",
      "Episode length: 7.95 +/- 2.46\n",
      "Eval num_timesteps=664000, episode_reward=-7009.82 +/- 2921.68\n",
      "Episode length: 3.95 +/- 1.40\n",
      "Eval num_timesteps=665000, episode_reward=-6540.50 +/- 3663.56\n",
      "Episode length: 7.50 +/- 3.14\n",
      "Eval num_timesteps=666000, episode_reward=-7116.68 +/- 2967.00\n",
      "Episode length: 5.20 +/- 2.23\n",
      "Eval num_timesteps=667000, episode_reward=-7270.78 +/- 3049.00\n",
      "Episode length: 7.00 +/- 2.41\n",
      "Eval num_timesteps=668000, episode_reward=-6128.93 +/- 3998.65\n",
      "Episode length: 7.75 +/- 4.16\n",
      "Eval num_timesteps=669000, episode_reward=-7638.15 +/- 2451.99\n",
      "Episode length: 5.55 +/- 2.31\n",
      "Eval num_timesteps=670000, episode_reward=-5698.55 +/- 4042.10\n",
      "Episode length: 7.60 +/- 2.85\n",
      "Eval num_timesteps=671000, episode_reward=-5646.06 +/- 4058.55\n",
      "Episode length: 6.15 +/- 3.35\n",
      "Eval num_timesteps=672000, episode_reward=-7948.99 +/- 1814.68\n",
      "Episode length: 5.80 +/- 1.29\n",
      "Eval num_timesteps=673000, episode_reward=-7462.83 +/- 2470.99\n",
      "Episode length: 5.05 +/- 1.86\n",
      "Eval num_timesteps=674000, episode_reward=-7330.98 +/- 3108.27\n",
      "Episode length: 7.90 +/- 2.32\n",
      "Eval num_timesteps=675000, episode_reward=-6451.19 +/- 3667.83\n",
      "Episode length: 6.50 +/- 3.14\n",
      "Eval num_timesteps=676000, episode_reward=-6996.94 +/- 2917.25\n",
      "Episode length: 3.25 +/- 1.34\n",
      "Eval num_timesteps=677000, episode_reward=-6996.34 +/- 3436.07\n",
      "Episode length: 8.60 +/- 2.76\n",
      "Eval num_timesteps=678000, episode_reward=-6613.75 +/- 3757.48\n",
      "Episode length: 8.00 +/- 3.71\n",
      "Eval num_timesteps=679000, episode_reward=-6228.17 +/- 3610.20\n",
      "Episode length: 5.25 +/- 2.05\n",
      "Eval num_timesteps=680000, episode_reward=-6679.74 +/- 3313.44\n",
      "Episode length: 4.85 +/- 2.06\n",
      "Eval num_timesteps=681000, episode_reward=-5257.51 +/- 4182.64\n",
      "Episode length: 6.65 +/- 3.79\n",
      "Eval num_timesteps=682000, episode_reward=-5703.87 +/- 4149.71\n",
      "Episode length: 7.10 +/- 4.18\n",
      "Eval num_timesteps=683000, episode_reward=-6957.65 +/- 3451.87\n",
      "Episode length: 7.25 +/- 3.56\n",
      "Eval num_timesteps=684000, episode_reward=-5146.56 +/- 4149.37\n",
      "Episode length: 6.10 +/- 2.90\n",
      "Eval num_timesteps=685000, episode_reward=-6814.10 +/- 3330.76\n",
      "Episode length: 6.55 +/- 2.73\n",
      "Eval num_timesteps=686000, episode_reward=-8081.09 +/- 1846.70\n",
      "Episode length: 6.90 +/- 2.12\n",
      "Eval num_timesteps=687000, episode_reward=-6077.64 +/- 3867.20\n",
      "Episode length: 7.10 +/- 2.91\n",
      "Eval num_timesteps=688000, episode_reward=-7098.73 +/- 3423.57\n",
      "Episode length: 9.50 +/- 2.80\n",
      "Eval num_timesteps=689000, episode_reward=-6399.35 +/- 3656.27\n",
      "Episode length: 6.40 +/- 2.99\n",
      "Eval num_timesteps=690000, episode_reward=-7470.25 +/- 3121.44\n",
      "Episode length: 9.25 +/- 3.31\n",
      "Eval num_timesteps=691000, episode_reward=-7530.01 +/- 2491.43\n",
      "Episode length: 5.15 +/- 1.59\n",
      "Eval num_timesteps=692000, episode_reward=-7668.42 +/- 2530.06\n",
      "Episode length: 7.10 +/- 2.07\n",
      "Eval num_timesteps=693000, episode_reward=-5921.28 +/- 3881.88\n",
      "Episode length: 5.55 +/- 1.96\n",
      "Eval num_timesteps=694000, episode_reward=-7364.71 +/- 3078.27\n",
      "Episode length: 7.55 +/- 3.41\n",
      "Eval num_timesteps=695000, episode_reward=-7389.16 +/- 3096.94\n",
      "Episode length: 8.40 +/- 3.48\n",
      "Eval num_timesteps=696000, episode_reward=-7761.18 +/- 2498.31\n",
      "Episode length: 7.90 +/- 2.07\n",
      "Eval num_timesteps=697000, episode_reward=-5878.26 +/- 3790.49\n",
      "Episode length: 4.95 +/- 2.40\n",
      "Eval num_timesteps=698000, episode_reward=-6103.30 +/- 3873.85\n",
      "Episode length: 7.05 +/- 2.78\n",
      "Eval num_timesteps=699000, episode_reward=-5998.45 +/- 3875.64\n",
      "Episode length: 6.30 +/- 2.69\n",
      "Eval num_timesteps=700000, episode_reward=-6458.57 +/- 3659.29\n",
      "Episode length: 6.85 +/- 2.87\n",
      "Eval num_timesteps=701000, episode_reward=-7819.70 +/- 2605.44\n",
      "Episode length: 7.40 +/- 2.89\n",
      "Eval num_timesteps=702000, episode_reward=-7604.97 +/- 2510.54\n",
      "Episode length: 6.25 +/- 1.51\n",
      "Eval num_timesteps=703000, episode_reward=-7733.50 +/- 2527.92\n",
      "Episode length: 7.75 +/- 2.21\n",
      "Eval num_timesteps=704000, episode_reward=-7148.34 +/- 2982.87\n",
      "Episode length: 5.90 +/- 2.77\n",
      "Eval num_timesteps=705000, episode_reward=-6206.52 +/- 3864.44\n",
      "Episode length: 8.20 +/- 3.41\n"
     ]
    }
   ],
   "source": [
    "# 设置评估频率\n",
    "eval_freq = 1000  # 每训练500步评估一次\n",
    "\n",
    "# 配置EvalCallback\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=20,\n",
    "    best_model_save_path=\"./best_model/DQN\",  # 更改保存路径为DQN\n",
    "    log_path=\"./logs/DQN\",  # 更改日志路径为DQN\n",
    "    eval_freq=eval_freq,\n",
    ")\n",
    "\n",
    "# QRDQN的超参数设置，包括优先经验回放\n",
    "model = QRDQN(\n",
    "    'MlpPolicy', \n",
    "    env, \n",
    "    verbose=1, \n",
    "    tensorboard_log=\"./logs/QRDQN\",\n",
    "    learning_rate=0.0005,  # 默认学习率，通常比A2C更小\n",
    "    buffer_size=100000,  # 经验回放缓冲区大小\n",
    "    learning_starts=1000,  # 开始学习前的步数\n",
    "    batch_size=32,  # 批量大小\n",
    "    tau=1.0,  # 目标网络软更新的系数\n",
    "    gamma=0.99,  # 折扣因子\n",
    "    train_freq=4,  # 训练的频率（每4步训练一次）\n",
    "    target_update_interval=1000,  # 目标网络更新的频率\n",
    "    exploration_fraction=0.1,  # 探索率衰减部分\n",
    "    exploration_final_eps=0.02,  # 最小探索率\n",
    "    max_grad_norm=10,  # 梯度裁剪的最大范数\n",
    "    replay_buffer_class=PrioritizedReplayBuffer,  # 使用优先经验回放缓冲区\n",
    "    replay_buffer_kwargs=dict(\n",
    "        alpha=0.6,  # 优先经验回放的 alpha 参数\n",
    "        beta=0.4,   # 优先经验回放的 beta 参数\n",
    "        beta_schedule=\"linear\",  # beta 的调度，可以是\"linear\"或其他策略\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# 训练模型，并将TensorBoard日志存储到log目录中\n",
    "model.learn(total_timesteps=1000000, tb_log_name=\"QRDQN\", callback=eval_callback)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"qrdqn_electric_vehicle\")\n",
    "\n",
    "# 加载模型（如果需要）\n",
    "# model = DQN.load(\"dqn_electric_vehicle\")\n",
    "\n",
    "# 评估模型\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# 测试训练好的模型\n",
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-09-01T08:00:15.584893800Z"
    }
   },
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
