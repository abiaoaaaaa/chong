{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO,DDPG,DQN\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env\n",
    "env = env.ElectricVehicleEnv()\n",
    "#vec_env = make_vec_env(env, n_envs=4)\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-03T06:04:59.684775100Z",
     "start_time": "2024-09-03T06:04:53.035097600Z"
    }
   },
   "id": "376f27caa974f1e8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-8328.59 +/- 112.33\n",
      "Episode length: 2.95 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-8167.17 +/- 68.16\n",
      "Episode length: 2.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-7356.18 +/- 2432.24\n",
      "Episode length: 1.90 +/- 0.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-6578.96 +/- 3258.64\n",
      "Episode length: 2.25 +/- 0.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-8177.62 +/- 75.69\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-7901.77 +/- 1802.38\n",
      "Episode length: 2.75 +/- 1.04\n",
      "Eval num_timesteps=7000, episode_reward=-8225.68 +/- 67.71\n",
      "Episode length: 2.40 +/- 0.66\n",
      "Eval num_timesteps=8000, episode_reward=-8274.64 +/- 117.98\n",
      "Episode length: 2.70 +/- 0.78\n",
      "Eval num_timesteps=9000, episode_reward=-5777.72 +/- 3744.31\n",
      "Episode length: 2.05 +/- 0.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-7057.56 +/- 2941.81\n",
      "Episode length: 2.70 +/- 1.31\n",
      "Eval num_timesteps=11000, episode_reward=-4676.04 +/- 4157.55\n",
      "Episode length: 3.40 +/- 3.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-8219.41 +/- 18.35\n",
      "Episode length: 2.05 +/- 0.22\n",
      "Eval num_timesteps=13000, episode_reward=-8285.04 +/- 110.81\n",
      "Episode length: 2.90 +/- 1.48\n",
      "Eval num_timesteps=14000, episode_reward=-8264.43 +/- 114.35\n",
      "Episode length: 3.15 +/- 1.62\n",
      "Eval num_timesteps=15000, episode_reward=-3426.34 +/- 4124.00\n",
      "Episode length: 2.30 +/- 2.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-8199.85 +/- 51.86\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-7559.32 +/- 2506.31\n",
      "Episode length: 3.65 +/- 1.71\n",
      "Eval num_timesteps=18000, episode_reward=-6180.47 +/- 3534.19\n",
      "Episode length: 1.75 +/- 0.43\n",
      "Eval num_timesteps=19000, episode_reward=-6216.44 +/- 3557.87\n",
      "Episode length: 2.10 +/- 1.22\n",
      "Eval num_timesteps=20000, episode_reward=-7687.69 +/- 2555.76\n",
      "Episode length: 4.55 +/- 2.94\n",
      "Eval num_timesteps=21000, episode_reward=-8645.09 +/- 325.73\n",
      "Episode length: 6.95 +/- 3.04\n",
      "Eval num_timesteps=22000, episode_reward=-8387.56 +/- 90.38\n",
      "Episode length: 3.50 +/- 0.81\n",
      "Eval num_timesteps=23000, episode_reward=-6255.22 +/- 3577.93\n",
      "Episode length: 2.45 +/- 0.97\n",
      "Eval num_timesteps=24000, episode_reward=-8054.10 +/- 1892.65\n",
      "Episode length: 5.25 +/- 3.83\n",
      "Eval num_timesteps=25000, episode_reward=-7605.56 +/- 2534.72\n",
      "Episode length: 4.20 +/- 2.66\n",
      "Eval num_timesteps=26000, episode_reward=-7531.37 +/- 2496.67\n",
      "Episode length: 3.40 +/- 1.20\n",
      "Eval num_timesteps=27000, episode_reward=-5857.31 +/- 3793.76\n",
      "Episode length: 2.40 +/- 1.32\n",
      "Eval num_timesteps=28000, episode_reward=-6509.59 +/- 3665.49\n",
      "Episode length: 4.90 +/- 3.97\n",
      "Eval num_timesteps=29000, episode_reward=-5139.91 +/- 4148.40\n",
      "Episode length: 3.20 +/- 2.06\n",
      "Eval num_timesteps=30000, episode_reward=-6623.45 +/- 3249.53\n",
      "Episode length: 2.85 +/- 1.53\n",
      "Eval num_timesteps=31000, episode_reward=-8205.16 +/- 10.25\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-6940.98 +/- 3418.76\n",
      "Episode length: 5.20 +/- 3.56\n",
      "Eval num_timesteps=33000, episode_reward=-7824.68 +/- 1786.05\n",
      "Episode length: 3.90 +/- 2.39\n",
      "Eval num_timesteps=34000, episode_reward=-8477.09 +/- 158.79\n",
      "Episode length: 4.40 +/- 1.50\n",
      "Eval num_timesteps=35000, episode_reward=-5748.97 +/- 4194.83\n",
      "Episode length: 5.70 +/- 4.81\n",
      "Eval num_timesteps=36000, episode_reward=-3872.80 +/- 4182.33\n",
      "Episode length: 2.60 +/- 1.53\n",
      "Eval num_timesteps=37000, episode_reward=-3934.39 +/- 4280.86\n",
      "Episode length: 3.15 +/- 2.55\n",
      "Eval num_timesteps=38000, episode_reward=-5313.97 +/- 4194.95\n",
      "Episode length: 4.45 +/- 2.11\n",
      "Eval num_timesteps=39000, episode_reward=-5900.11 +/- 3804.87\n",
      "Episode length: 4.05 +/- 2.77\n",
      "Eval num_timesteps=40000, episode_reward=-7117.74 +/- 2965.06\n",
      "Episode length: 2.70 +/- 1.00\n",
      "Eval num_timesteps=41000, episode_reward=-7100.63 +/- 2961.38\n",
      "Episode length: 2.80 +/- 1.40\n",
      "Eval num_timesteps=42000, episode_reward=-7177.15 +/- 2997.88\n",
      "Episode length: 3.40 +/- 2.11\n",
      "Eval num_timesteps=43000, episode_reward=-7947.87 +/- 1827.80\n",
      "Episode length: 3.30 +/- 2.33\n",
      "Eval num_timesteps=44000, episode_reward=-7806.42 +/- 1831.84\n",
      "Episode length: 2.05 +/- 0.38\n",
      "Eval num_timesteps=45000, episode_reward=-8198.29 +/- 66.62\n",
      "Episode length: 2.05 +/- 0.22\n",
      "Eval num_timesteps=46000, episode_reward=-5141.80 +/- 4078.19\n",
      "Episode length: 3.15 +/- 1.65\n",
      "Eval num_timesteps=47000, episode_reward=-4859.49 +/- 4289.45\n",
      "Episode length: 4.65 +/- 2.71\n",
      "Eval num_timesteps=48000, episode_reward=-7620.22 +/- 2389.02\n",
      "Episode length: 4.70 +/- 3.74\n",
      "Eval num_timesteps=49000, episode_reward=-3351.87 +/- 4027.07\n",
      "Episode length: 1.65 +/- 1.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-8256.73 +/- 86.73\n",
      "Episode length: 2.40 +/- 0.49\n",
      "Eval num_timesteps=51000, episode_reward=-6720.38 +/- 3327.30\n",
      "Episode length: 3.30 +/- 1.19\n",
      "Eval num_timesteps=52000, episode_reward=-6260.48 +/- 3582.76\n",
      "Episode length: 2.45 +/- 1.60\n",
      "Eval num_timesteps=53000, episode_reward=-4663.11 +/- 4162.76\n",
      "Episode length: 2.75 +/- 2.34\n",
      "Eval num_timesteps=54000, episode_reward=-3845.70 +/- 4159.16\n",
      "Episode length: 2.65 +/- 2.13\n",
      "Eval num_timesteps=55000, episode_reward=-4608.71 +/- 4404.25\n",
      "Episode length: 5.90 +/- 4.02\n",
      "Eval num_timesteps=56000, episode_reward=-6388.34 +/- 3655.71\n",
      "Episode length: 3.45 +/- 1.91\n",
      "Eval num_timesteps=57000, episode_reward=-4217.95 +/- 4033.00\n",
      "Episode length: 3.35 +/- 1.56\n",
      "Eval num_timesteps=58000, episode_reward=-4061.16 +/- 4199.26\n",
      "Episode length: 4.35 +/- 2.59\n",
      "Eval num_timesteps=59000, episode_reward=-7415.57 +/- 2452.31\n",
      "Episode length: 2.05 +/- 0.50\n",
      "Eval num_timesteps=60000, episode_reward=-5803.49 +/- 3757.70\n",
      "Episode length: 1.90 +/- 0.83\n",
      "Eval num_timesteps=61000, episode_reward=-4700.16 +/- 4060.74\n",
      "Episode length: 2.90 +/- 1.61\n",
      "Eval num_timesteps=62000, episode_reward=-7955.92 +/- 1814.00\n",
      "Episode length: 3.60 +/- 1.16\n",
      "Eval num_timesteps=63000, episode_reward=-2524.22 +/- 3681.92\n",
      "Episode length: 1.55 +/- 0.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=-7014.46 +/- 2920.32\n",
      "Episode length: 2.00 +/- 0.55\n",
      "Eval num_timesteps=65000, episode_reward=-7629.34 +/- 2558.04\n",
      "Episode length: 4.05 +/- 4.03\n",
      "Eval num_timesteps=66000, episode_reward=-6003.13 +/- 3894.59\n",
      "Episode length: 3.90 +/- 2.68\n",
      "Eval num_timesteps=67000, episode_reward=-6654.50 +/- 3298.08\n",
      "Episode length: 2.60 +/- 1.46\n",
      "Eval num_timesteps=68000, episode_reward=-5649.58 +/- 4051.05\n",
      "Episode length: 4.65 +/- 2.63\n",
      "Eval num_timesteps=69000, episode_reward=-4299.88 +/- 4204.96\n",
      "Episode length: 2.85 +/- 1.62\n",
      "Eval num_timesteps=70000, episode_reward=-5894.67 +/- 3785.95\n",
      "Episode length: 2.85 +/- 1.42\n",
      "Eval num_timesteps=71000, episode_reward=-5084.11 +/- 4080.17\n",
      "Episode length: 2.75 +/- 1.70\n",
      "Eval num_timesteps=72000, episode_reward=-3865.34 +/- 4096.21\n",
      "Episode length: 2.65 +/- 1.59\n",
      "Eval num_timesteps=73000, episode_reward=-6574.68 +/- 3230.47\n",
      "Episode length: 2.25 +/- 0.54\n",
      "Eval num_timesteps=74000, episode_reward=-6651.01 +/- 3258.93\n",
      "Episode length: 2.40 +/- 1.16\n",
      "Eval num_timesteps=75000, episode_reward=-4742.12 +/- 4234.08\n",
      "Episode length: 3.10 +/- 2.02\n",
      "Eval num_timesteps=76000, episode_reward=-3984.18 +/- 4258.91\n",
      "Episode length: 3.70 +/- 3.20\n",
      "Eval num_timesteps=77000, episode_reward=-4962.88 +/- 4025.11\n",
      "Episode length: 1.75 +/- 0.70\n",
      "Eval num_timesteps=78000, episode_reward=-6196.96 +/- 3520.20\n",
      "Episode length: 2.15 +/- 0.91\n",
      "Eval num_timesteps=79000, episode_reward=-5505.37 +/- 3999.23\n",
      "Episode length: 3.00 +/- 2.53\n",
      "Eval num_timesteps=80000, episode_reward=-4258.00 +/- 4196.99\n",
      "Episode length: 2.55 +/- 1.75\n",
      "Eval num_timesteps=81000, episode_reward=-7033.89 +/- 2898.82\n",
      "Episode length: 2.15 +/- 0.65\n",
      "Eval num_timesteps=82000, episode_reward=-5597.77 +/- 4067.79\n",
      "Episode length: 3.60 +/- 2.31\n",
      "Eval num_timesteps=83000, episode_reward=-6701.99 +/- 3328.59\n",
      "Episode length: 2.80 +/- 2.25\n",
      "Eval num_timesteps=84000, episode_reward=-4583.74 +/- 4023.42\n",
      "Episode length: 1.90 +/- 0.62\n",
      "Eval num_timesteps=85000, episode_reward=-6427.81 +/- 3692.74\n",
      "Episode length: 4.70 +/- 3.76\n",
      "Eval num_timesteps=86000, episode_reward=-5474.86 +/- 3952.67\n",
      "Episode length: 2.65 +/- 1.88\n",
      "Eval num_timesteps=87000, episode_reward=-6201.89 +/- 3544.72\n",
      "Episode length: 1.95 +/- 0.67\n",
      "Eval num_timesteps=88000, episode_reward=-3580.08 +/- 4260.87\n",
      "Episode length: 4.10 +/- 3.55\n",
      "Eval num_timesteps=89000, episode_reward=-7619.98 +/- 2530.28\n",
      "Episode length: 3.80 +/- 2.16\n",
      "Eval num_timesteps=90000, episode_reward=-3051.32 +/- 3861.33\n",
      "Episode length: 2.80 +/- 2.25\n",
      "Eval num_timesteps=91000, episode_reward=-4131.12 +/- 4283.01\n",
      "Episode length: 5.35 +/- 4.36\n",
      "Eval num_timesteps=92000, episode_reward=-4304.13 +/- 4069.03\n",
      "Episode length: 2.90 +/- 2.30\n",
      "Eval num_timesteps=93000, episode_reward=-6415.72 +/- 3668.13\n",
      "Episode length: 3.55 +/- 1.88\n",
      "Eval num_timesteps=94000, episode_reward=-3995.65 +/- 4357.58\n",
      "Episode length: 4.05 +/- 4.36\n",
      "Eval num_timesteps=95000, episode_reward=-6692.51 +/- 3320.35\n",
      "Episode length: 2.75 +/- 1.64\n",
      "Eval num_timesteps=96000, episode_reward=-3953.06 +/- 4059.73\n",
      "Episode length: 3.30 +/- 1.85\n",
      "Eval num_timesteps=97000, episode_reward=-7107.81 +/- 3537.82\n",
      "Episode length: 6.70 +/- 4.08\n",
      "Eval num_timesteps=98000, episode_reward=-3855.11 +/- 4195.40\n",
      "Episode length: 2.55 +/- 1.77\n",
      "Eval num_timesteps=99000, episode_reward=-4688.36 +/- 4136.54\n",
      "Episode length: 3.25 +/- 1.76\n",
      "Eval num_timesteps=100000, episode_reward=-5061.48 +/- 4081.02\n",
      "Episode length: 2.75 +/- 1.61\n",
      "Eval num_timesteps=101000, episode_reward=-6831.68 +/- 3379.85\n",
      "Episode length: 4.15 +/- 3.04\n",
      "Eval num_timesteps=102000, episode_reward=-7446.80 +/- 2468.77\n",
      "Episode length: 2.80 +/- 1.25\n",
      "Eval num_timesteps=103000, episode_reward=-7188.76 +/- 2995.38\n",
      "Episode length: 3.35 +/- 1.28\n",
      "Eval num_timesteps=104000, episode_reward=-5360.55 +/- 3889.55\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=105000, episode_reward=-5741.06 +/- 3697.33\n",
      "Episode length: 1.80 +/- 0.40\n",
      "Eval num_timesteps=106000, episode_reward=-5019.50 +/- 4026.67\n",
      "Episode length: 2.20 +/- 1.08\n",
      "Eval num_timesteps=107000, episode_reward=-4585.38 +/- 4066.57\n",
      "Episode length: 2.25 +/- 1.22\n",
      "Eval num_timesteps=108000, episode_reward=-6192.06 +/- 3539.16\n",
      "Episode length: 1.85 +/- 0.57\n",
      "Eval num_timesteps=109000, episode_reward=-5030.18 +/- 3995.02\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=110000, episode_reward=-5806.59 +/- 3758.72\n",
      "Episode length: 1.95 +/- 0.74\n",
      "Eval num_timesteps=111000, episode_reward=-4961.32 +/- 4000.90\n",
      "Episode length: 1.70 +/- 0.64\n",
      "Eval num_timesteps=112000, episode_reward=-5085.85 +/- 4105.75\n",
      "Episode length: 2.85 +/- 1.62\n",
      "Eval num_timesteps=113000, episode_reward=-6654.60 +/- 3300.41\n",
      "Episode length: 3.15 +/- 1.80\n",
      "Eval num_timesteps=114000, episode_reward=-2979.98 +/- 3954.83\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=115000, episode_reward=-5946.15 +/- 3830.50\n",
      "Episode length: 3.85 +/- 1.93\n",
      "Eval num_timesteps=116000, episode_reward=-2926.03 +/- 3871.10\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=117000, episode_reward=-3265.48 +/- 4069.85\n",
      "Episode length: 1.55 +/- 0.80\n",
      "Eval num_timesteps=118000, episode_reward=-3741.53 +/- 4070.86\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=119000, episode_reward=-3961.33 +/- 4294.68\n",
      "Episode length: 3.95 +/- 3.71\n",
      "Eval num_timesteps=120000, episode_reward=-4168.64 +/- 4109.03\n",
      "Episode length: 1.85 +/- 1.11\n",
      "Eval num_timesteps=121000, episode_reward=-5008.31 +/- 4037.56\n",
      "Episode length: 2.05 +/- 1.07\n",
      "Eval num_timesteps=122000, episode_reward=-4602.88 +/- 4086.52\n",
      "Episode length: 1.95 +/- 1.12\n",
      "Eval num_timesteps=123000, episode_reward=-5883.32 +/- 3802.21\n",
      "Episode length: 2.75 +/- 1.58\n",
      "Eval num_timesteps=124000, episode_reward=-4269.92 +/- 4205.60\n",
      "Episode length: 2.50 +/- 1.60\n",
      "Eval num_timesteps=125000, episode_reward=-3756.70 +/- 4085.67\n",
      "Episode length: 1.75 +/- 1.04\n",
      "Eval num_timesteps=126000, episode_reward=-5490.20 +/- 3965.82\n",
      "Episode length: 3.35 +/- 2.15\n",
      "Eval num_timesteps=127000, episode_reward=-5099.82 +/- 4115.03\n",
      "Episode length: 2.95 +/- 2.04\n",
      "Eval num_timesteps=128000, episode_reward=-4012.42 +/- 3992.25\n",
      "Episode length: 4.10 +/- 3.16\n",
      "Eval num_timesteps=129000, episode_reward=-7160.52 +/- 2965.78\n",
      "Episode length: 3.15 +/- 1.56\n",
      "Eval num_timesteps=130000, episode_reward=-5508.76 +/- 3961.30\n",
      "Episode length: 2.80 +/- 1.54\n",
      "Eval num_timesteps=131000, episode_reward=-6546.51 +/- 3242.62\n",
      "Episode length: 1.80 +/- 0.40\n",
      "Eval num_timesteps=132000, episode_reward=-3700.48 +/- 4083.01\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=133000, episode_reward=-3835.97 +/- 4171.75\n",
      "Episode length: 2.30 +/- 1.52\n",
      "Eval num_timesteps=134000, episode_reward=-4949.94 +/- 4425.90\n",
      "Episode length: 5.15 +/- 4.38\n",
      "Eval num_timesteps=135000, episode_reward=-3877.77 +/- 4179.45\n",
      "Episode length: 2.60 +/- 1.43\n",
      "Eval num_timesteps=136000, episode_reward=-3811.56 +/- 4149.00\n",
      "Episode length: 2.10 +/- 1.64\n",
      "Eval num_timesteps=137000, episode_reward=-3746.36 +/- 4052.24\n",
      "Episode length: 1.80 +/- 0.87\n",
      "Eval num_timesteps=138000, episode_reward=-3039.44 +/- 4014.57\n",
      "Episode length: 2.35 +/- 1.53\n",
      "Eval num_timesteps=139000, episode_reward=-4588.78 +/- 4053.34\n",
      "Episode length: 1.90 +/- 0.62\n",
      "Eval num_timesteps=140000, episode_reward=-4368.11 +/- 4283.13\n",
      "Episode length: 3.85 +/- 3.53\n",
      "Eval num_timesteps=141000, episode_reward=-6470.31 +/- 3700.97\n",
      "Episode length: 4.05 +/- 2.01\n",
      "Eval num_timesteps=142000, episode_reward=-3045.29 +/- 4023.81\n",
      "Episode length: 2.45 +/- 1.63\n",
      "Eval num_timesteps=143000, episode_reward=-3770.42 +/- 4069.71\n",
      "Episode length: 2.05 +/- 1.28\n",
      "Eval num_timesteps=144000, episode_reward=-5120.93 +/- 4130.57\n",
      "Episode length: 3.05 +/- 2.16\n",
      "Eval num_timesteps=145000, episode_reward=-3462.62 +/- 4089.38\n",
      "Episode length: 2.90 +/- 2.32\n",
      "Eval num_timesteps=146000, episode_reward=-3089.36 +/- 4031.84\n",
      "Episode length: 3.00 +/- 2.39\n",
      "Eval num_timesteps=147000, episode_reward=-6092.17 +/- 3955.14\n",
      "Episode length: 4.40 +/- 3.17\n",
      "Eval num_timesteps=148000, episode_reward=-3881.01 +/- 4212.09\n",
      "Episode length: 2.65 +/- 2.13\n",
      "Eval num_timesteps=149000, episode_reward=-2960.06 +/- 3925.19\n",
      "Episode length: 1.70 +/- 1.05\n",
      "Eval num_timesteps=150000, episode_reward=-5840.92 +/- 4239.72\n",
      "Episode length: 6.05 +/- 4.42\n",
      "Eval num_timesteps=151000, episode_reward=-5057.40 +/- 4077.99\n",
      "Episode length: 3.10 +/- 1.84\n",
      "Eval num_timesteps=152000, episode_reward=-4612.16 +/- 4096.76\n",
      "Episode length: 2.75 +/- 2.21\n",
      "Eval num_timesteps=153000, episode_reward=-3155.11 +/- 4185.39\n",
      "Episode length: 3.50 +/- 3.26\n",
      "Eval num_timesteps=154000, episode_reward=-5486.54 +/- 3974.17\n",
      "Episode length: 2.95 +/- 2.11\n",
      "Eval num_timesteps=155000, episode_reward=-3045.02 +/- 4046.10\n",
      "Episode length: 2.45 +/- 1.83\n",
      "Eval num_timesteps=156000, episode_reward=-3868.20 +/- 4198.75\n",
      "Episode length: 2.85 +/- 2.57\n",
      "Eval num_timesteps=157000, episode_reward=-4155.75 +/- 4094.31\n",
      "Episode length: 1.90 +/- 1.22\n",
      "Eval num_timesteps=158000, episode_reward=-3058.40 +/- 4059.56\n",
      "Episode length: 2.65 +/- 2.24\n",
      "Eval num_timesteps=159000, episode_reward=-4026.03 +/- 4323.63\n",
      "Episode length: 4.20 +/- 3.37\n",
      "Eval num_timesteps=160000, episode_reward=-4387.81 +/- 4314.70\n",
      "Episode length: 3.65 +/- 2.59\n",
      "Eval num_timesteps=161000, episode_reward=-4017.89 +/- 4327.35\n",
      "Episode length: 4.05 +/- 3.07\n",
      "Eval num_timesteps=162000, episode_reward=-4903.12 +/- 4370.79\n",
      "Episode length: 5.20 +/- 4.21\n",
      "Eval num_timesteps=163000, episode_reward=-2206.67 +/- 3664.65\n",
      "Episode length: 2.15 +/- 1.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=164000, episode_reward=-4646.70 +/- 4110.53\n",
      "Episode length: 2.85 +/- 1.74\n",
      "Eval num_timesteps=165000, episode_reward=-3034.91 +/- 4018.53\n",
      "Episode length: 2.55 +/- 1.86\n",
      "Eval num_timesteps=166000, episode_reward=-4180.57 +/- 4073.88\n",
      "Episode length: 2.05 +/- 0.86\n",
      "Eval num_timesteps=167000, episode_reward=-5995.58 +/- 3884.21\n",
      "Episode length: 3.55 +/- 1.80\n",
      "Eval num_timesteps=168000, episode_reward=-4190.60 +/- 4099.53\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=169000, episode_reward=-3510.17 +/- 4160.92\n",
      "Episode length: 3.15 +/- 2.03\n",
      "Eval num_timesteps=170000, episode_reward=-3936.96 +/- 4156.55\n",
      "Episode length: 3.60 +/- 2.08\n",
      "Eval num_timesteps=171000, episode_reward=-4602.55 +/- 4107.09\n",
      "Episode length: 2.15 +/- 1.31\n",
      "Eval num_timesteps=172000, episode_reward=-4011.85 +/- 4335.86\n",
      "Episode length: 4.00 +/- 3.05\n",
      "Eval num_timesteps=173000, episode_reward=-4657.64 +/- 4146.42\n",
      "Episode length: 2.45 +/- 1.47\n",
      "Eval num_timesteps=174000, episode_reward=-2624.17 +/- 3879.67\n",
      "Episode length: 2.20 +/- 1.54\n",
      "Eval num_timesteps=175000, episode_reward=-4910.43 +/- 4374.32\n",
      "Episode length: 4.95 +/- 3.97\n",
      "Eval num_timesteps=176000, episode_reward=-5894.10 +/- 4268.11\n",
      "Episode length: 6.55 +/- 4.55\n",
      "Eval num_timesteps=177000, episode_reward=-2324.47 +/- 3775.41\n",
      "Episode length: 4.00 +/- 4.02\n",
      "Eval num_timesteps=178000, episode_reward=-5444.80 +/- 3938.83\n",
      "Episode length: 2.40 +/- 1.11\n",
      "Eval num_timesteps=179000, episode_reward=-3113.11 +/- 4126.84\n",
      "Episode length: 3.25 +/- 3.13\n",
      "Eval num_timesteps=180000, episode_reward=-3838.47 +/- 4114.40\n",
      "Episode length: 2.45 +/- 1.40\n",
      "Eval num_timesteps=181000, episode_reward=-5623.44 +/- 4086.27\n",
      "Episode length: 4.45 +/- 3.67\n",
      "Eval num_timesteps=182000, episode_reward=-3990.03 +/- 4311.47\n",
      "Episode length: 3.70 +/- 2.90\n",
      "Eval num_timesteps=183000, episode_reward=-3911.96 +/- 4258.83\n",
      "Episode length: 3.35 +/- 3.02\n",
      "Eval num_timesteps=184000, episode_reward=-5256.16 +/- 4246.16\n",
      "Episode length: 4.65 +/- 3.45\n",
      "Eval num_timesteps=185000, episode_reward=-4853.60 +/- 4254.64\n",
      "Episode length: 4.40 +/- 2.75\n",
      "Eval num_timesteps=186000, episode_reward=-4280.65 +/- 4191.51\n",
      "Episode length: 2.60 +/- 1.71\n",
      "Eval num_timesteps=187000, episode_reward=-3536.06 +/- 4252.39\n",
      "Episode length: 3.30 +/- 2.95\n",
      "Eval num_timesteps=188000, episode_reward=-5596.44 +/- 4034.11\n",
      "Episode length: 3.45 +/- 1.66\n",
      "Eval num_timesteps=189000, episode_reward=-4120.39 +/- 4336.11\n",
      "Episode length: 5.20 +/- 3.93\n",
      "Eval num_timesteps=190000, episode_reward=-3124.55 +/- 4154.15\n",
      "Episode length: 3.30 +/- 3.52\n",
      "Eval num_timesteps=191000, episode_reward=-3820.91 +/- 4157.96\n",
      "Episode length: 2.35 +/- 1.53\n",
      "Eval num_timesteps=192000, episode_reward=-3049.63 +/- 4024.85\n",
      "Episode length: 2.55 +/- 2.33\n",
      "Eval num_timesteps=193000, episode_reward=-3005.14 +/- 3963.35\n",
      "Episode length: 2.35 +/- 1.49\n",
      "Eval num_timesteps=194000, episode_reward=-5184.59 +/- 4176.09\n",
      "Episode length: 3.70 +/- 2.97\n",
      "Eval num_timesteps=195000, episode_reward=-4712.68 +/- 4175.74\n",
      "Episode length: 3.35 +/- 1.93\n",
      "Eval num_timesteps=196000, episode_reward=-3873.73 +/- 4217.98\n",
      "Episode length: 2.85 +/- 2.69\n",
      "Eval num_timesteps=197000, episode_reward=-4355.80 +/- 4280.65\n",
      "Episode length: 3.45 +/- 2.50\n",
      "Eval num_timesteps=198000, episode_reward=-5577.54 +/- 4049.16\n",
      "Episode length: 3.80 +/- 2.09\n",
      "Eval num_timesteps=199000, episode_reward=-5219.53 +/- 4216.42\n",
      "Episode length: 4.30 +/- 3.41\n",
      "Eval num_timesteps=200000, episode_reward=-4681.76 +/- 4180.27\n",
      "Episode length: 3.15 +/- 2.22\n",
      "Eval num_timesteps=201000, episode_reward=-5022.87 +/- 4021.48\n",
      "Episode length: 2.25 +/- 1.89\n",
      "Eval num_timesteps=202000, episode_reward=-4655.81 +/- 4162.24\n",
      "Episode length: 3.20 +/- 3.01\n",
      "Eval num_timesteps=203000, episode_reward=-4598.31 +/- 4104.13\n",
      "Episode length: 2.55 +/- 2.20\n",
      "Eval num_timesteps=204000, episode_reward=-5145.61 +/- 4160.54\n",
      "Episode length: 3.55 +/- 3.15\n",
      "Eval num_timesteps=205000, episode_reward=-5027.27 +/- 4055.60\n",
      "Episode length: 2.50 +/- 2.06\n",
      "Eval num_timesteps=206000, episode_reward=-2970.41 +/- 3931.53\n",
      "Episode length: 1.95 +/- 1.50\n",
      "Eval num_timesteps=207000, episode_reward=-3770.82 +/- 4085.01\n",
      "Episode length: 2.20 +/- 1.36\n",
      "Eval num_timesteps=208000, episode_reward=-3105.90 +/- 4118.33\n",
      "Episode length: 3.15 +/- 3.07\n",
      "Eval num_timesteps=209000, episode_reward=-3958.22 +/- 4260.49\n",
      "Episode length: 3.65 +/- 2.67\n",
      "Eval num_timesteps=210000, episode_reward=-5704.33 +/- 4128.02\n",
      "Episode length: 5.20 +/- 3.93\n",
      "Eval num_timesteps=211000, episode_reward=-4241.39 +/- 4148.73\n",
      "Episode length: 2.85 +/- 2.61\n",
      "Eval num_timesteps=212000, episode_reward=-4294.54 +/- 4203.90\n",
      "Episode length: 2.85 +/- 2.20\n",
      "Eval num_timesteps=213000, episode_reward=-3798.13 +/- 4097.66\n",
      "Episode length: 2.15 +/- 1.39\n",
      "Eval num_timesteps=214000, episode_reward=-3858.67 +/- 4177.37\n",
      "Episode length: 2.65 +/- 2.03\n",
      "Eval num_timesteps=215000, episode_reward=-4051.20 +/- 4367.09\n",
      "Episode length: 4.85 +/- 4.53\n",
      "Eval num_timesteps=216000, episode_reward=-3920.62 +/- 4221.96\n",
      "Episode length: 3.35 +/- 3.41\n",
      "Eval num_timesteps=217000, episode_reward=-4618.51 +/- 4100.40\n",
      "Episode length: 2.70 +/- 2.00\n",
      "Eval num_timesteps=218000, episode_reward=-4751.81 +/- 4116.23\n",
      "Episode length: 3.65 +/- 3.26\n",
      "Eval num_timesteps=219000, episode_reward=-5708.65 +/- 4119.41\n",
      "Episode length: 5.55 +/- 3.96\n",
      "Eval num_timesteps=220000, episode_reward=-4305.80 +/- 4216.59\n",
      "Episode length: 3.15 +/- 3.09\n",
      "Eval num_timesteps=221000, episode_reward=-3829.24 +/- 4146.02\n",
      "Episode length: 2.75 +/- 2.49\n",
      "Eval num_timesteps=222000, episode_reward=-4457.54 +/- 4346.30\n",
      "Episode length: 5.00 +/- 3.71\n",
      "Eval num_timesteps=223000, episode_reward=-3938.78 +/- 4248.52\n",
      "Episode length: 4.10 +/- 4.05\n",
      "Eval num_timesteps=224000, episode_reward=-4793.52 +/- 4237.40\n",
      "Episode length: 4.80 +/- 4.15\n",
      "Eval num_timesteps=225000, episode_reward=-6045.52 +/- 3932.97\n",
      "Episode length: 4.70 +/- 4.05\n",
      "Eval num_timesteps=226000, episode_reward=-5069.66 +/- 4025.10\n",
      "Episode length: 3.10 +/- 2.83\n",
      "Eval num_timesteps=227000, episode_reward=-4430.31 +/- 4336.90\n",
      "Episode length: 4.50 +/- 4.31\n",
      "Eval num_timesteps=228000, episode_reward=-5248.00 +/- 4211.14\n",
      "Episode length: 4.50 +/- 3.99\n",
      "Eval num_timesteps=229000, episode_reward=-4800.70 +/- 4164.13\n",
      "Episode length: 3.85 +/- 1.77\n",
      "Eval num_timesteps=230000, episode_reward=-4369.99 +/- 4228.87\n",
      "Episode length: 4.20 +/- 3.67\n",
      "Eval num_timesteps=231000, episode_reward=-5493.83 +/- 3969.19\n",
      "Episode length: 3.60 +/- 3.18\n",
      "Eval num_timesteps=232000, episode_reward=-6327.43 +/- 3595.35\n",
      "Episode length: 4.05 +/- 2.13\n",
      "Eval num_timesteps=233000, episode_reward=-5046.19 +/- 4060.42\n",
      "Episode length: 2.90 +/- 2.28\n",
      "Eval num_timesteps=234000, episode_reward=-5643.40 +/- 4035.28\n",
      "Episode length: 3.95 +/- 1.83\n",
      "Eval num_timesteps=235000, episode_reward=-4306.36 +/- 4136.11\n",
      "Episode length: 3.30 +/- 2.30\n",
      "Eval num_timesteps=236000, episode_reward=-5069.44 +/- 4069.84\n",
      "Episode length: 2.55 +/- 1.47\n",
      "Eval num_timesteps=237000, episode_reward=-3781.87 +/- 4064.67\n",
      "Episode length: 2.15 +/- 1.46\n",
      "Eval num_timesteps=238000, episode_reward=-3488.53 +/- 4139.35\n",
      "Episode length: 3.10 +/- 2.51\n",
      "Eval num_timesteps=239000, episode_reward=-4771.87 +/- 4255.11\n",
      "Episode length: 3.90 +/- 3.01\n",
      "Eval num_timesteps=240000, episode_reward=-3421.46 +/- 4088.33\n",
      "Episode length: 2.55 +/- 1.63\n",
      "Eval num_timesteps=241000, episode_reward=-3543.85 +/- 4222.26\n",
      "Episode length: 3.65 +/- 3.82\n",
      "Eval num_timesteps=242000, episode_reward=-4217.05 +/- 4143.81\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=243000, episode_reward=-4297.37 +/- 4178.45\n",
      "Episode length: 2.95 +/- 1.75\n",
      "Eval num_timesteps=244000, episode_reward=-1928.47 +/- 3567.05\n",
      "Episode length: 3.60 +/- 3.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=245000, episode_reward=-4829.48 +/- 4189.84\n",
      "Episode length: 4.55 +/- 3.29\n",
      "Eval num_timesteps=246000, episode_reward=-3019.05 +/- 3983.62\n",
      "Episode length: 2.60 +/- 1.80\n",
      "Eval num_timesteps=247000, episode_reward=-3463.99 +/- 4133.88\n",
      "Episode length: 3.10 +/- 2.34\n",
      "Eval num_timesteps=248000, episode_reward=-4288.72 +/- 4196.78\n",
      "Episode length: 3.45 +/- 2.25\n",
      "Eval num_timesteps=249000, episode_reward=-3709.81 +/- 4287.43\n",
      "Episode length: 5.50 +/- 4.46\n",
      "Eval num_timesteps=250000, episode_reward=-5248.55 +/- 4236.07\n",
      "Episode length: 3.90 +/- 2.70\n",
      "Eval num_timesteps=251000, episode_reward=-3806.70 +/- 4101.48\n",
      "Episode length: 2.85 +/- 1.68\n",
      "Eval num_timesteps=252000, episode_reward=-4687.17 +/- 4183.39\n",
      "Episode length: 2.45 +/- 1.36\n",
      "Eval num_timesteps=253000, episode_reward=-5787.50 +/- 4201.09\n",
      "Episode length: 5.35 +/- 3.21\n",
      "Eval num_timesteps=254000, episode_reward=-3753.88 +/- 4007.30\n",
      "Episode length: 1.95 +/- 1.02\n",
      "Eval num_timesteps=255000, episode_reward=-4716.79 +/- 4214.86\n",
      "Episode length: 3.20 +/- 2.66\n",
      "Eval num_timesteps=256000, episode_reward=-4646.05 +/- 4146.37\n",
      "Episode length: 2.95 +/- 1.99\n",
      "Eval num_timesteps=257000, episode_reward=-5596.36 +/- 4065.70\n",
      "Episode length: 4.05 +/- 2.58\n",
      "Eval num_timesteps=258000, episode_reward=-5548.01 +/- 4028.38\n",
      "Episode length: 3.15 +/- 2.01\n",
      "Eval num_timesteps=259000, episode_reward=-6506.42 +/- 3724.62\n",
      "Episode length: 5.40 +/- 3.26\n",
      "Eval num_timesteps=260000, episode_reward=-4861.20 +/- 4299.73\n",
      "Episode length: 4.50 +/- 3.37\n",
      "Eval num_timesteps=261000, episode_reward=-3468.04 +/- 4034.29\n",
      "Episode length: 3.00 +/- 2.07\n",
      "Eval num_timesteps=262000, episode_reward=-4483.11 +/- 4426.31\n",
      "Episode length: 4.75 +/- 4.47\n",
      "Eval num_timesteps=263000, episode_reward=-3943.20 +/- 4176.63\n",
      "Episode length: 3.50 +/- 2.64\n",
      "Eval num_timesteps=264000, episode_reward=-3857.55 +/- 4039.32\n",
      "Episode length: 3.15 +/- 1.88\n",
      "Eval num_timesteps=265000, episode_reward=-4544.78 +/- 4441.87\n",
      "Episode length: 5.50 +/- 4.50\n",
      "Eval num_timesteps=266000, episode_reward=-4299.72 +/- 4239.85\n",
      "Episode length: 3.65 +/- 3.20\n",
      "Eval num_timesteps=267000, episode_reward=-3446.68 +/- 4064.13\n",
      "Episode length: 2.60 +/- 1.56\n",
      "Eval num_timesteps=268000, episode_reward=-4054.50 +/- 4375.98\n",
      "Episode length: 4.45 +/- 4.08\n",
      "Eval num_timesteps=269000, episode_reward=-3084.01 +/- 4091.51\n",
      "Episode length: 2.95 +/- 2.50\n",
      "Eval num_timesteps=270000, episode_reward=-3926.23 +/- 4280.81\n",
      "Episode length: 3.70 +/- 3.63\n",
      "Eval num_timesteps=271000, episode_reward=-3005.11 +/- 3999.36\n",
      "Episode length: 2.30 +/- 1.79\n",
      "Eval num_timesteps=272000, episode_reward=-3949.09 +/- 4298.55\n",
      "Episode length: 3.55 +/- 3.28\n",
      "Eval num_timesteps=273000, episode_reward=-5219.07 +/- 4091.83\n",
      "Episode length: 3.90 +/- 1.87\n",
      "Eval num_timesteps=274000, episode_reward=-3910.86 +/- 4253.40\n",
      "Episode length: 2.75 +/- 1.95\n",
      "Eval num_timesteps=275000, episode_reward=-5242.39 +/- 4231.07\n",
      "Episode length: 4.15 +/- 2.59\n",
      "Eval num_timesteps=276000, episode_reward=-3218.32 +/- 4300.72\n",
      "Episode length: 4.15 +/- 4.53\n",
      "Eval num_timesteps=277000, episode_reward=-3532.94 +/- 4088.64\n",
      "Episode length: 3.00 +/- 1.55\n",
      "Eval num_timesteps=278000, episode_reward=-4211.19 +/- 4070.87\n",
      "Episode length: 2.35 +/- 1.46\n",
      "Eval num_timesteps=279000, episode_reward=-2728.04 +/- 4073.95\n",
      "Episode length: 3.15 +/- 3.64\n",
      "Eval num_timesteps=280000, episode_reward=-1823.86 +/- 3343.21\n",
      "Episode length: 2.45 +/- 1.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=281000, episode_reward=-3190.53 +/- 4178.78\n",
      "Episode length: 4.10 +/- 3.87\n",
      "Eval num_timesteps=282000, episode_reward=-3533.79 +/- 4093.71\n",
      "Episode length: 3.80 +/- 2.32\n",
      "Eval num_timesteps=283000, episode_reward=-2262.27 +/- 3654.64\n",
      "Episode length: 2.85 +/- 2.35\n",
      "Eval num_timesteps=284000, episode_reward=-2600.65 +/- 3817.98\n",
      "Episode length: 2.75 +/- 1.61\n",
      "Eval num_timesteps=285000, episode_reward=-3563.63 +/- 4288.85\n",
      "Episode length: 3.50 +/- 3.22\n",
      "Eval num_timesteps=286000, episode_reward=-2588.33 +/- 3770.65\n",
      "Episode length: 2.40 +/- 1.43\n",
      "Eval num_timesteps=287000, episode_reward=-4260.41 +/- 4199.04\n",
      "Episode length: 2.75 +/- 1.79\n",
      "Eval num_timesteps=288000, episode_reward=-3542.99 +/- 4027.34\n",
      "Episode length: 3.35 +/- 2.31\n",
      "Eval num_timesteps=289000, episode_reward=-2616.62 +/- 3843.36\n",
      "Episode length: 2.25 +/- 1.61\n",
      "Eval num_timesteps=290000, episode_reward=-3963.32 +/- 4317.09\n",
      "Episode length: 3.90 +/- 3.39\n",
      "Eval num_timesteps=291000, episode_reward=-5329.99 +/- 4307.85\n",
      "Episode length: 5.40 +/- 4.34\n",
      "Eval num_timesteps=292000, episode_reward=-3221.41 +/- 4115.26\n",
      "Episode length: 4.10 +/- 3.81\n",
      "Eval num_timesteps=293000, episode_reward=-4418.70 +/- 4357.34\n",
      "Episode length: 4.00 +/- 3.18\n",
      "Eval num_timesteps=294000, episode_reward=-1845.09 +/- 3326.12\n",
      "Episode length: 2.45 +/- 1.96\n",
      "Eval num_timesteps=295000, episode_reward=-4052.05 +/- 4408.72\n",
      "Episode length: 4.30 +/- 3.74\n",
      "Eval num_timesteps=296000, episode_reward=-1511.28 +/- 3203.77\n",
      "Episode length: 3.15 +/- 3.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=297000, episode_reward=-3765.82 +/- 4419.24\n",
      "Episode length: 5.50 +/- 4.72\n",
      "Eval num_timesteps=298000, episode_reward=-4283.20 +/- 4225.27\n",
      "Episode length: 2.70 +/- 2.19\n",
      "Eval num_timesteps=299000, episode_reward=-4149.12 +/- 4517.27\n",
      "Episode length: 5.35 +/- 4.82\n",
      "Eval num_timesteps=300000, episode_reward=-5762.14 +/- 4187.83\n",
      "Episode length: 5.40 +/- 3.88\n",
      "Eval num_timesteps=301000, episode_reward=-5374.23 +/- 4340.32\n",
      "Episode length: 5.50 +/- 4.18\n",
      "Eval num_timesteps=302000, episode_reward=-2186.75 +/- 3593.50\n",
      "Episode length: 2.00 +/- 2.02\n",
      "Eval num_timesteps=303000, episode_reward=-3471.65 +/- 4164.34\n",
      "Episode length: 2.75 +/- 2.19\n",
      "Eval num_timesteps=304000, episode_reward=-3592.28 +/- 4225.39\n",
      "Episode length: 3.65 +/- 2.71\n",
      "Eval num_timesteps=305000, episode_reward=-4332.47 +/- 4245.89\n",
      "Episode length: 3.55 +/- 2.46\n",
      "Eval num_timesteps=306000, episode_reward=-3523.21 +/- 4190.57\n",
      "Episode length: 3.10 +/- 2.34\n",
      "Eval num_timesteps=307000, episode_reward=-5446.81 +/- 4317.18\n",
      "Episode length: 6.45 +/- 3.90\n",
      "Eval num_timesteps=308000, episode_reward=-2744.28 +/- 3987.48\n",
      "Episode length: 3.60 +/- 3.35\n",
      "Eval num_timesteps=309000, episode_reward=-4766.03 +/- 4217.49\n",
      "Episode length: 3.40 +/- 2.13\n",
      "Eval num_timesteps=310000, episode_reward=-4036.43 +/- 4336.00\n",
      "Episode length: 4.25 +/- 3.51\n",
      "Eval num_timesteps=311000, episode_reward=-3680.70 +/- 4308.93\n",
      "Episode length: 4.85 +/- 4.22\n",
      "Eval num_timesteps=312000, episode_reward=-4418.32 +/- 4357.03\n",
      "Episode length: 3.85 +/- 2.92\n",
      "Eval num_timesteps=313000, episode_reward=-2609.79 +/- 3777.44\n",
      "Episode length: 2.15 +/- 1.35\n",
      "Eval num_timesteps=314000, episode_reward=-4498.39 +/- 4439.27\n",
      "Episode length: 4.80 +/- 4.17\n",
      "Eval num_timesteps=315000, episode_reward=-5988.78 +/- 3887.90\n",
      "Episode length: 4.00 +/- 2.79\n",
      "Eval num_timesteps=316000, episode_reward=-2177.59 +/- 3649.86\n",
      "Episode length: 1.95 +/- 1.53\n",
      "Eval num_timesteps=317000, episode_reward=-945.50 +/- 2623.02\n",
      "Episode length: 1.75 +/- 1.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=318000, episode_reward=-5340.21 +/- 4312.53\n",
      "Episode length: 5.20 +/- 3.87\n",
      "Eval num_timesteps=319000, episode_reward=-2305.85 +/- 3711.89\n",
      "Episode length: 3.00 +/- 2.81\n",
      "Eval num_timesteps=320000, episode_reward=-1938.74 +/- 3270.24\n",
      "Episode length: 3.95 +/- 3.07\n",
      "Eval num_timesteps=321000, episode_reward=-3985.39 +/- 4302.94\n",
      "Episode length: 3.90 +/- 3.49\n",
      "Eval num_timesteps=322000, episode_reward=-4781.58 +/- 4268.37\n",
      "Episode length: 3.90 +/- 2.75\n",
      "Eval num_timesteps=323000, episode_reward=-5177.05 +/- 4176.99\n",
      "Episode length: 3.75 +/- 2.68\n",
      "Eval num_timesteps=324000, episode_reward=-3089.14 +/- 4080.74\n",
      "Episode length: 2.95 +/- 2.62\n",
      "Eval num_timesteps=325000, episode_reward=-3509.67 +/- 4173.50\n",
      "Episode length: 3.25 +/- 2.30\n",
      "Eval num_timesteps=326000, episode_reward=-2720.46 +/- 3969.15\n",
      "Episode length: 3.65 +/- 3.93\n",
      "Eval num_timesteps=327000, episode_reward=-3615.14 +/- 4353.56\n",
      "Episode length: 4.30 +/- 4.26\n",
      "Eval num_timesteps=328000, episode_reward=-3201.25 +/- 4136.89\n",
      "Episode length: 4.40 +/- 3.92\n",
      "Eval num_timesteps=329000, episode_reward=-3962.67 +/- 4313.01\n",
      "Episode length: 3.45 +/- 2.84\n",
      "Eval num_timesteps=330000, episode_reward=-1346.67 +/- 3083.73\n",
      "Episode length: 2.00 +/- 1.97\n",
      "Eval num_timesteps=331000, episode_reward=-3069.94 +/- 4123.28\n",
      "Episode length: 3.20 +/- 3.23\n",
      "Eval num_timesteps=332000, episode_reward=-3588.65 +/- 4243.48\n",
      "Episode length: 3.95 +/- 3.72\n",
      "Eval num_timesteps=333000, episode_reward=-3035.60 +/- 4025.83\n",
      "Episode length: 2.75 +/- 2.00\n",
      "Eval num_timesteps=334000, episode_reward=-5510.18 +/- 3998.68\n",
      "Episode length: 3.15 +/- 1.85\n",
      "Eval num_timesteps=335000, episode_reward=-6005.30 +/- 3902.57\n",
      "Episode length: 3.95 +/- 3.06\n",
      "Eval num_timesteps=336000, episode_reward=-4332.21 +/- 4271.40\n",
      "Episode length: 3.50 +/- 2.75\n",
      "Eval num_timesteps=337000, episode_reward=-3656.31 +/- 4407.82\n",
      "Episode length: 4.60 +/- 4.41\n",
      "Eval num_timesteps=338000, episode_reward=-3105.44 +/- 4146.58\n",
      "Episode length: 3.05 +/- 2.85\n",
      "Eval num_timesteps=339000, episode_reward=-3581.69 +/- 4240.53\n",
      "Episode length: 3.80 +/- 3.04\n",
      "Eval num_timesteps=340000, episode_reward=-2282.53 +/- 3599.09\n",
      "Episode length: 3.20 +/- 2.64\n",
      "Eval num_timesteps=341000, episode_reward=-3383.20 +/- 4073.90\n",
      "Episode length: 2.30 +/- 1.85\n",
      "Eval num_timesteps=342000, episode_reward=-5070.46 +/- 4090.31\n",
      "Episode length: 2.75 +/- 1.76\n",
      "Eval num_timesteps=343000, episode_reward=-2706.71 +/- 3793.69\n",
      "Episode length: 3.10 +/- 1.81\n",
      "Eval num_timesteps=344000, episode_reward=-2216.81 +/- 3714.30\n",
      "Episode length: 2.75 +/- 2.34\n",
      "Eval num_timesteps=345000, episode_reward=-4330.70 +/- 4221.44\n",
      "Episode length: 3.80 +/- 2.42\n",
      "Eval num_timesteps=346000, episode_reward=-3413.36 +/- 4115.52\n",
      "Episode length: 2.85 +/- 2.33\n",
      "Eval num_timesteps=347000, episode_reward=-3014.05 +/- 3994.07\n",
      "Episode length: 2.70 +/- 2.15\n",
      "Eval num_timesteps=348000, episode_reward=-3133.65 +/- 3947.59\n",
      "Episode length: 4.00 +/- 3.15\n",
      "Eval num_timesteps=349000, episode_reward=-3559.45 +/- 4290.83\n",
      "Episode length: 3.95 +/- 4.20\n",
      "Eval num_timesteps=350000, episode_reward=-3117.61 +/- 4046.84\n",
      "Episode length: 3.45 +/- 3.01\n",
      "Eval num_timesteps=351000, episode_reward=-4242.54 +/- 4180.29\n",
      "Episode length: 3.05 +/- 2.20\n",
      "Eval num_timesteps=352000, episode_reward=-4329.72 +/- 4277.28\n",
      "Episode length: 3.80 +/- 3.88\n",
      "Eval num_timesteps=353000, episode_reward=-4269.44 +/- 4160.82\n",
      "Episode length: 3.20 +/- 2.62\n",
      "Eval num_timesteps=354000, episode_reward=-3948.58 +/- 4153.99\n",
      "Episode length: 3.65 +/- 3.00\n",
      "Eval num_timesteps=355000, episode_reward=-4569.97 +/- 4077.44\n",
      "Episode length: 2.20 +/- 1.29\n",
      "Eval num_timesteps=356000, episode_reward=-3817.88 +/- 4151.77\n",
      "Episode length: 2.55 +/- 2.09\n",
      "Eval num_timesteps=357000, episode_reward=-4315.35 +/- 4256.58\n",
      "Episode length: 3.10 +/- 2.57\n",
      "Eval num_timesteps=358000, episode_reward=-4865.94 +/- 4350.80\n",
      "Episode length: 5.00 +/- 4.56\n",
      "Eval num_timesteps=359000, episode_reward=-2303.96 +/- 3663.98\n",
      "Episode length: 3.10 +/- 2.30\n",
      "Eval num_timesteps=360000, episode_reward=-4564.07 +/- 4078.35\n",
      "Episode length: 2.80 +/- 2.14\n",
      "Eval num_timesteps=361000, episode_reward=-4337.72 +/- 4249.81\n",
      "Episode length: 3.55 +/- 3.15\n",
      "Eval num_timesteps=362000, episode_reward=-4624.38 +/- 4131.11\n",
      "Episode length: 2.45 +/- 2.11\n",
      "Eval num_timesteps=363000, episode_reward=-4727.60 +/- 4138.15\n",
      "Episode length: 3.80 +/- 2.58\n",
      "Eval num_timesteps=364000, episode_reward=-3909.47 +/- 4097.46\n",
      "Episode length: 3.45 +/- 3.09\n",
      "Eval num_timesteps=365000, episode_reward=-2581.92 +/- 3735.95\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=366000, episode_reward=-4781.47 +/- 4271.82\n",
      "Episode length: 4.20 +/- 3.57\n",
      "Eval num_timesteps=367000, episode_reward=-3956.54 +/- 4275.90\n",
      "Episode length: 4.05 +/- 3.68\n",
      "Eval num_timesteps=368000, episode_reward=-4794.89 +/- 4276.46\n",
      "Episode length: 4.70 +/- 3.80\n",
      "Eval num_timesteps=369000, episode_reward=-1031.05 +/- 2462.49\n",
      "Episode length: 2.70 +/- 1.90\n",
      "Eval num_timesteps=370000, episode_reward=-3975.16 +/- 4330.27\n",
      "Episode length: 4.15 +/- 4.26\n",
      "Eval num_timesteps=371000, episode_reward=-3826.20 +/- 4159.39\n",
      "Episode length: 2.75 +/- 1.95\n",
      "Eval num_timesteps=372000, episode_reward=-3401.57 +/- 4070.19\n",
      "Episode length: 2.75 +/- 1.92\n",
      "Eval num_timesteps=373000, episode_reward=-2929.73 +/- 3918.57\n",
      "Episode length: 2.00 +/- 1.61\n",
      "Eval num_timesteps=374000, episode_reward=-3834.93 +/- 4077.73\n",
      "Episode length: 3.60 +/- 2.54\n",
      "Eval num_timesteps=375000, episode_reward=-2694.19 +/- 4021.10\n",
      "Episode length: 3.40 +/- 4.05\n",
      "Eval num_timesteps=376000, episode_reward=-3421.85 +/- 4059.10\n",
      "Episode length: 2.25 +/- 1.37\n",
      "Eval num_timesteps=377000, episode_reward=-3022.61 +/- 3979.71\n",
      "Episode length: 2.40 +/- 1.71\n",
      "Eval num_timesteps=378000, episode_reward=-3737.60 +/- 4075.73\n",
      "Episode length: 2.15 +/- 1.56\n",
      "Eval num_timesteps=379000, episode_reward=-4840.96 +/- 4347.31\n",
      "Episode length: 5.25 +/- 3.86\n",
      "Eval num_timesteps=380000, episode_reward=-3855.79 +/- 4170.85\n",
      "Episode length: 3.45 +/- 2.96\n",
      "Eval num_timesteps=381000, episode_reward=-2814.09 +/- 3915.83\n",
      "Episode length: 4.00 +/- 3.41\n",
      "Eval num_timesteps=382000, episode_reward=-3291.15 +/- 4029.92\n",
      "Episode length: 2.10 +/- 1.45\n",
      "Eval num_timesteps=383000, episode_reward=-5514.83 +/- 4000.21\n",
      "Episode length: 3.95 +/- 2.27\n",
      "Eval num_timesteps=384000, episode_reward=-2602.13 +/- 3937.32\n",
      "Episode length: 2.45 +/- 2.13\n",
      "Eval num_timesteps=385000, episode_reward=-1826.59 +/- 3526.48\n",
      "Episode length: 3.00 +/- 4.00\n",
      "Eval num_timesteps=386000, episode_reward=-2551.95 +/- 3818.22\n",
      "Episode length: 2.20 +/- 1.63\n",
      "Eval num_timesteps=387000, episode_reward=-3060.82 +/- 4086.59\n",
      "Episode length: 2.90 +/- 2.81\n",
      "Eval num_timesteps=388000, episode_reward=-1890.04 +/- 3426.28\n",
      "Episode length: 3.25 +/- 3.11\n",
      "Eval num_timesteps=389000, episode_reward=-1349.99 +/- 3134.14\n",
      "Episode length: 2.25 +/- 3.11\n",
      "Eval num_timesteps=390000, episode_reward=-1820.81 +/- 3655.23\n",
      "Episode length: 3.15 +/- 3.94\n",
      "Eval num_timesteps=391000, episode_reward=-2614.65 +/- 3911.60\n",
      "Episode length: 3.05 +/- 2.69\n",
      "Eval num_timesteps=392000, episode_reward=-3142.50 +/- 4112.13\n",
      "Episode length: 3.60 +/- 3.14\n",
      "Eval num_timesteps=393000, episode_reward=-2971.93 +/- 3900.08\n",
      "Episode length: 2.50 +/- 1.63\n",
      "Eval num_timesteps=394000, episode_reward=-907.04 +/- 2510.37\n",
      "Episode length: 1.55 +/- 1.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=395000, episode_reward=-4083.96 +/- 4131.99\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=396000, episode_reward=-2530.41 +/- 3826.53\n",
      "Episode length: 1.80 +/- 1.29\n",
      "Eval num_timesteps=397000, episode_reward=-3020.46 +/- 4032.37\n",
      "Episode length: 2.95 +/- 2.56\n",
      "Eval num_timesteps=398000, episode_reward=-3685.29 +/- 4471.70\n",
      "Episode length: 5.10 +/- 4.83\n",
      "Eval num_timesteps=399000, episode_reward=-2538.78 +/- 3783.42\n",
      "Episode length: 2.40 +/- 1.98\n",
      "Eval num_timesteps=400000, episode_reward=-4011.02 +/- 4369.42\n",
      "Episode length: 4.20 +/- 3.44\n",
      "Eval num_timesteps=401000, episode_reward=-4227.79 +/- 4251.58\n",
      "Episode length: 3.80 +/- 3.43\n",
      "Eval num_timesteps=402000, episode_reward=-1281.08 +/- 2978.47\n",
      "Episode length: 1.75 +/- 1.37\n",
      "Eval num_timesteps=403000, episode_reward=-1370.85 +/- 3100.97\n",
      "Episode length: 2.65 +/- 2.54\n",
      "Eval num_timesteps=404000, episode_reward=-1768.39 +/- 3459.99\n",
      "Episode length: 2.60 +/- 2.63\n",
      "Eval num_timesteps=405000, episode_reward=-2934.15 +/- 3965.56\n",
      "Episode length: 2.45 +/- 1.60\n",
      "Eval num_timesteps=406000, episode_reward=-450.03 +/- 1940.66\n",
      "Episode length: 1.90 +/- 2.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=407000, episode_reward=-2572.37 +/- 3896.55\n",
      "Episode length: 2.55 +/- 2.52\n",
      "Eval num_timesteps=408000, episode_reward=-1665.05 +/- 3415.57\n",
      "Episode length: 1.90 +/- 1.81\n",
      "Eval num_timesteps=409000, episode_reward=-2978.22 +/- 4040.84\n",
      "Episode length: 2.65 +/- 2.17\n",
      "Eval num_timesteps=410000, episode_reward=-839.45 +/- 2468.05\n",
      "Episode length: 1.75 +/- 1.04\n",
      "Eval num_timesteps=411000, episode_reward=-1333.11 +/- 3031.57\n",
      "Episode length: 2.15 +/- 2.22\n",
      "Eval num_timesteps=412000, episode_reward=-821.71 +/- 2538.29\n",
      "Episode length: 1.65 +/- 1.28\n",
      "Eval num_timesteps=413000, episode_reward=-1624.41 +/- 3320.76\n",
      "Episode length: 1.95 +/- 0.92\n",
      "Eval num_timesteps=414000, episode_reward=-779.72 +/- 2489.55\n",
      "Episode length: 2.05 +/- 1.12\n",
      "Eval num_timesteps=415000, episode_reward=-1234.75 +/- 2976.18\n",
      "Episode length: 1.85 +/- 1.24\n",
      "Eval num_timesteps=416000, episode_reward=-1637.49 +/- 3355.13\n",
      "Episode length: 2.25 +/- 1.58\n",
      "Eval num_timesteps=417000, episode_reward=-1695.57 +/- 3358.53\n",
      "Episode length: 2.05 +/- 1.60\n",
      "Eval num_timesteps=418000, episode_reward=-2521.62 +/- 3761.59\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=419000, episode_reward=-2578.45 +/- 3882.23\n",
      "Episode length: 2.45 +/- 2.11\n",
      "Eval num_timesteps=420000, episode_reward=-2467.56 +/- 3726.74\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=421000, episode_reward=-1717.86 +/- 3383.59\n",
      "Episode length: 1.95 +/- 1.77\n",
      "Eval num_timesteps=422000, episode_reward=-391.74 +/- 1853.07\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=423000, episode_reward=-2956.20 +/- 4028.94\n",
      "Episode length: 3.10 +/- 2.21\n",
      "Eval num_timesteps=424000, episode_reward=-1334.27 +/- 3084.59\n",
      "Episode length: 2.05 +/- 2.38\n",
      "Eval num_timesteps=425000, episode_reward=-839.81 +/- 2495.94\n",
      "Episode length: 1.50 +/- 1.50\n",
      "Eval num_timesteps=426000, episode_reward=-2149.29 +/- 3648.83\n",
      "Episode length: 2.05 +/- 1.72\n",
      "Eval num_timesteps=427000, episode_reward=-829.31 +/- 2529.99\n",
      "Episode length: 1.95 +/- 1.56\n",
      "Eval num_timesteps=428000, episode_reward=-475.43 +/- 1858.28\n",
      "Episode length: 1.60 +/- 1.20\n",
      "Eval num_timesteps=429000, episode_reward=-863.07 +/- 2493.16\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=430000, episode_reward=-444.87 +/- 1790.36\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=431000, episode_reward=-24.97 +/- 81.77\n",
      "Episode length: 1.15 +/- 0.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=432000, episode_reward=-1718.79 +/- 3265.79\n",
      "Episode length: 2.25 +/- 1.97\n",
      "Eval num_timesteps=433000, episode_reward=-819.88 +/- 2475.82\n",
      "Episode length: 1.70 +/- 1.05\n",
      "Eval num_timesteps=434000, episode_reward=-860.65 +/- 2461.02\n",
      "Episode length: 1.80 +/- 1.08\n",
      "Eval num_timesteps=435000, episode_reward=-1233.36 +/- 3028.32\n",
      "Episode length: 1.80 +/- 1.78\n",
      "Eval num_timesteps=436000, episode_reward=-1773.72 +/- 3391.46\n",
      "Episode length: 2.35 +/- 1.77\n",
      "Eval num_timesteps=437000, episode_reward=-1710.91 +/- 3405.22\n",
      "Episode length: 1.95 +/- 1.56\n",
      "Eval num_timesteps=438000, episode_reward=-2634.73 +/- 4030.60\n",
      "Episode length: 3.25 +/- 3.16\n",
      "Eval num_timesteps=439000, episode_reward=-953.05 +/- 2511.73\n",
      "Episode length: 2.05 +/- 1.53\n",
      "Eval num_timesteps=440000, episode_reward=-2149.59 +/- 3605.79\n",
      "Episode length: 2.15 +/- 1.56\n",
      "Eval num_timesteps=441000, episode_reward=-2663.34 +/- 3862.08\n",
      "Episode length: 3.05 +/- 2.11\n",
      "Eval num_timesteps=442000, episode_reward=-908.36 +/- 2561.00\n",
      "Episode length: 1.60 +/- 1.36\n",
      "Eval num_timesteps=443000, episode_reward=-2190.53 +/- 3660.94\n",
      "Episode length: 2.45 +/- 1.75\n",
      "Eval num_timesteps=444000, episode_reward=-579.60 +/- 1784.03\n",
      "Episode length: 2.50 +/- 1.91\n",
      "Eval num_timesteps=445000, episode_reward=-2603.84 +/- 3794.51\n",
      "Episode length: 2.35 +/- 1.31\n",
      "Eval num_timesteps=446000, episode_reward=-2992.16 +/- 3879.42\n",
      "Episode length: 2.70 +/- 1.71\n",
      "Eval num_timesteps=447000, episode_reward=-2179.18 +/- 3523.38\n",
      "Episode length: 2.30 +/- 1.23\n",
      "Eval num_timesteps=448000, episode_reward=-3733.98 +/- 4138.81\n",
      "Episode length: 2.45 +/- 2.04\n",
      "Eval num_timesteps=449000, episode_reward=-2096.49 +/- 3598.11\n",
      "Episode length: 2.40 +/- 2.27\n",
      "Eval num_timesteps=450000, episode_reward=-1702.39 +/- 3433.64\n",
      "Episode length: 2.10 +/- 1.70\n",
      "Eval num_timesteps=451000, episode_reward=-844.35 +/- 2545.96\n",
      "Episode length: 2.00 +/- 2.12\n",
      "Eval num_timesteps=452000, episode_reward=-503.30 +/- 1902.51\n",
      "Episode length: 2.20 +/- 2.34\n",
      "Eval num_timesteps=453000, episode_reward=-439.67 +/- 1837.76\n",
      "Episode length: 1.65 +/- 1.06\n",
      "Eval num_timesteps=454000, episode_reward=-1309.72 +/- 2994.83\n",
      "Episode length: 1.95 +/- 1.12\n",
      "Eval num_timesteps=455000, episode_reward=-82.93 +/- 48.36\n",
      "Episode length: 1.45 +/- 0.74\n",
      "Eval num_timesteps=456000, episode_reward=-464.60 +/- 1887.55\n",
      "Episode length: 1.85 +/- 1.42\n",
      "Eval num_timesteps=457000, episode_reward=-2150.76 +/- 3630.08\n",
      "Episode length: 2.25 +/- 1.09\n",
      "Eval num_timesteps=458000, episode_reward=-1338.51 +/- 3071.25\n",
      "Episode length: 2.60 +/- 2.37\n",
      "Eval num_timesteps=459000, episode_reward=-1729.78 +/- 3297.98\n",
      "Episode length: 2.00 +/- 1.41\n",
      "Eval num_timesteps=460000, episode_reward=-2114.94 +/- 3559.54\n",
      "Episode length: 2.15 +/- 1.28\n",
      "Eval num_timesteps=461000, episode_reward=-1732.68 +/- 3297.27\n",
      "Episode length: 1.90 +/- 1.04\n",
      "Eval num_timesteps=462000, episode_reward=-458.36 +/- 1808.12\n",
      "Episode length: 1.70 +/- 0.90\n",
      "Eval num_timesteps=463000, episode_reward=-2515.58 +/- 3795.08\n",
      "Episode length: 2.55 +/- 1.66\n",
      "Eval num_timesteps=464000, episode_reward=-2899.95 +/- 3893.04\n",
      "Episode length: 2.00 +/- 1.10\n",
      "Eval num_timesteps=465000, episode_reward=-1699.86 +/- 3252.68\n",
      "Episode length: 1.85 +/- 0.85\n",
      "Eval num_timesteps=466000, episode_reward=-1331.67 +/- 3036.45\n",
      "Episode length: 2.05 +/- 2.11\n",
      "Eval num_timesteps=467000, episode_reward=-481.03 +/- 1800.08\n",
      "Episode length: 1.75 +/- 0.54\n",
      "Eval num_timesteps=468000, episode_reward=-72.21 +/- 173.47\n",
      "Episode length: 2.05 +/- 1.07\n",
      "Eval num_timesteps=469000, episode_reward=-122.76 +/- 142.25\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=470000, episode_reward=-73.04 +/- 121.19\n",
      "Episode length: 1.80 +/- 0.93\n",
      "Eval num_timesteps=471000, episode_reward=-80.10 +/- 81.71\n",
      "Episode length: 1.85 +/- 0.91\n",
      "Eval num_timesteps=472000, episode_reward=-131.64 +/- 88.34\n",
      "Episode length: 2.00 +/- 0.77\n",
      "Eval num_timesteps=473000, episode_reward=-59.04 +/- 97.54\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=474000, episode_reward=-454.52 +/- 1878.84\n",
      "Episode length: 1.85 +/- 1.28\n",
      "Eval num_timesteps=475000, episode_reward=-504.71 +/- 1866.45\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=476000, episode_reward=-930.26 +/- 2570.30\n",
      "Episode length: 2.10 +/- 1.58\n",
      "Eval num_timesteps=477000, episode_reward=-920.03 +/- 2643.26\n",
      "Episode length: 2.25 +/- 2.07\n",
      "Eval num_timesteps=478000, episode_reward=-1287.17 +/- 3038.66\n",
      "Episode length: 1.95 +/- 1.53\n",
      "Eval num_timesteps=479000, episode_reward=-1289.91 +/- 2966.01\n",
      "Episode length: 1.75 +/- 1.13\n",
      "Eval num_timesteps=480000, episode_reward=-45.38 +/- 81.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=481000, episode_reward=-53.70 +/- 147.72\n",
      "Episode length: 1.75 +/- 1.13\n",
      "Eval num_timesteps=482000, episode_reward=-55.07 +/- 72.25\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=483000, episode_reward=-912.63 +/- 2449.77\n",
      "Episode length: 2.50 +/- 1.69\n",
      "Eval num_timesteps=484000, episode_reward=-53.01 +/- 144.83\n",
      "Episode length: 1.60 +/- 1.16\n",
      "Eval num_timesteps=485000, episode_reward=-1317.66 +/- 2910.90\n",
      "Episode length: 2.25 +/- 1.44\n",
      "Eval num_timesteps=486000, episode_reward=-67.33 +/- 171.75\n",
      "Episode length: 1.85 +/- 1.59\n",
      "Eval num_timesteps=487000, episode_reward=-123.34 +/- 270.81\n",
      "Episode length: 2.25 +/- 2.47\n",
      "Eval num_timesteps=488000, episode_reward=-104.31 +/- 226.35\n",
      "Episode length: 2.30 +/- 2.03\n",
      "Eval num_timesteps=489000, episode_reward=-21.02 +/- 143.46\n",
      "Episode length: 1.70 +/- 1.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=490000, episode_reward=-49.97 +/- 166.80\n",
      "Episode length: 1.90 +/- 1.04\n",
      "Eval num_timesteps=491000, episode_reward=-460.11 +/- 1831.64\n",
      "Episode length: 1.60 +/- 0.86\n",
      "Eval num_timesteps=492000, episode_reward=-884.03 +/- 2518.08\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=493000, episode_reward=-489.75 +/- 1850.77\n",
      "Episode length: 1.90 +/- 1.37\n",
      "Eval num_timesteps=494000, episode_reward=-512.07 +/- 1857.25\n",
      "Episode length: 1.95 +/- 1.36\n",
      "Eval num_timesteps=495000, episode_reward=-1789.68 +/- 3457.39\n",
      "Episode length: 2.75 +/- 2.72\n",
      "Eval num_timesteps=496000, episode_reward=-80.73 +/- 84.39\n",
      "Episode length: 1.55 +/- 0.80\n",
      "Eval num_timesteps=497000, episode_reward=-15.19 +/- 102.36\n",
      "Episode length: 1.40 +/- 0.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=498000, episode_reward=-941.62 +/- 2693.18\n",
      "Episode length: 2.20 +/- 2.99\n",
      "Eval num_timesteps=499000, episode_reward=-23.35 +/- 110.74\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=500000, episode_reward=-62.08 +/- 101.56\n",
      "Episode length: 1.70 +/- 0.90\n",
      "Eval num_timesteps=501000, episode_reward=-870.13 +/- 2397.13\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=502000, episode_reward=-2.56 +/- 99.79\n",
      "Episode length: 1.60 +/- 0.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=503000, episode_reward=-38.88 +/- 81.68\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=504000, episode_reward=-64.75 +/- 103.52\n",
      "Episode length: 1.75 +/- 0.94\n",
      "Eval num_timesteps=505000, episode_reward=-26.69 +/- 93.51\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=506000, episode_reward=-42.16 +/- 90.11\n",
      "Episode length: 1.75 +/- 0.89\n",
      "Eval num_timesteps=507000, episode_reward=-71.91 +/- 64.08\n",
      "Episode length: 1.75 +/- 0.83\n",
      "Eval num_timesteps=508000, episode_reward=-24.30 +/- 70.08\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=509000, episode_reward=-455.12 +/- 1792.64\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=510000, episode_reward=-1302.00 +/- 2923.27\n",
      "Episode length: 1.75 +/- 0.70\n",
      "Eval num_timesteps=511000, episode_reward=-874.82 +/- 2462.87\n",
      "Episode length: 2.15 +/- 0.79\n",
      "Eval num_timesteps=512000, episode_reward=-844.47 +/- 2473.28\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=513000, episode_reward=-1272.43 +/- 2936.11\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=514000, episode_reward=-1258.09 +/- 2941.83\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=515000, episode_reward=-2110.84 +/- 3578.77\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=516000, episode_reward=-943.43 +/- 2622.78\n",
      "Episode length: 2.55 +/- 2.04\n",
      "Eval num_timesteps=517000, episode_reward=-950.60 +/- 2620.41\n",
      "Episode length: 2.45 +/- 2.06\n",
      "Eval num_timesteps=518000, episode_reward=-1276.99 +/- 3014.74\n",
      "Episode length: 2.05 +/- 1.16\n",
      "Eval num_timesteps=519000, episode_reward=-950.92 +/- 2621.42\n",
      "Episode length: 2.60 +/- 2.01\n",
      "Eval num_timesteps=520000, episode_reward=-1366.32 +/- 3127.54\n",
      "Episode length: 2.75 +/- 2.30\n",
      "Eval num_timesteps=521000, episode_reward=-2170.89 +/- 3634.34\n",
      "Episode length: 2.80 +/- 0.98\n",
      "Eval num_timesteps=522000, episode_reward=-1365.09 +/- 3126.95\n",
      "Episode length: 2.40 +/- 2.44\n",
      "Eval num_timesteps=523000, episode_reward=-1343.63 +/- 3136.70\n",
      "Episode length: 2.30 +/- 2.45\n",
      "Eval num_timesteps=524000, episode_reward=-942.55 +/- 2725.10\n",
      "Episode length: 2.30 +/- 2.93\n",
      "Eval num_timesteps=525000, episode_reward=-1388.97 +/- 3246.86\n",
      "Episode length: 2.95 +/- 3.47\n",
      "Eval num_timesteps=526000, episode_reward=-481.45 +/- 1982.21\n",
      "Episode length: 1.85 +/- 2.20\n",
      "Eval num_timesteps=527000, episode_reward=-920.45 +/- 2733.12\n",
      "Episode length: 2.60 +/- 2.92\n",
      "Eval num_timesteps=528000, episode_reward=-34.99 +/- 71.47\n",
      "Episode length: 1.25 +/- 0.62\n",
      "Eval num_timesteps=529000, episode_reward=-42.60 +/- 23.15\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=530000, episode_reward=-4.18 +/- 77.95\n",
      "Episode length: 1.75 +/- 0.77\n",
      "Eval num_timesteps=531000, episode_reward=-31.99 +/- 52.41\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=532000, episode_reward=-28.29 +/- 70.99\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=533000, episode_reward=-50.81 +/- 21.52\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=534000, episode_reward=-63.65 +/- 37.66\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=535000, episode_reward=-41.88 +/- 108.01\n",
      "Episode length: 1.60 +/- 0.86\n",
      "Eval num_timesteps=536000, episode_reward=-51.72 +/- 88.66\n",
      "Episode length: 1.25 +/- 0.62\n",
      "Eval num_timesteps=537000, episode_reward=-55.23 +/- 74.18\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=538000, episode_reward=-17.06 +/- 103.25\n",
      "Episode length: 1.30 +/- 0.64\n",
      "Eval num_timesteps=539000, episode_reward=-44.68 +/- 81.34\n",
      "Episode length: 1.25 +/- 0.62\n",
      "Eval num_timesteps=540000, episode_reward=-80.12 +/- 56.96\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=541000, episode_reward=-64.29 +/- 80.81\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=542000, episode_reward=-44.49 +/- 64.72\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=543000, episode_reward=-12.64 +/- 81.50\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=544000, episode_reward=-21.66 +/- 69.34\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=545000, episode_reward=-23.89 +/- 70.55\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=546000, episode_reward=0.63 +/- 89.65\n",
      "Episode length: 1.45 +/- 0.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=547000, episode_reward=-44.64 +/- 21.02\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=548000, episode_reward=-31.16 +/- 71.44\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=549000, episode_reward=-18.10 +/- 83.56\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=550000, episode_reward=-44.65 +/- 22.56\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=551000, episode_reward=-49.11 +/- 22.05\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=552000, episode_reward=-16.26 +/- 82.99\n",
      "Episode length: 1.30 +/- 0.56\n",
      "Eval num_timesteps=553000, episode_reward=-23.40 +/- 93.66\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=554000, episode_reward=-41.48 +/- 54.39\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=555000, episode_reward=-25.55 +/- 69.71\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=556000, episode_reward=-36.61 +/- 53.86\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=557000, episode_reward=-42.50 +/- 85.50\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=558000, episode_reward=-43.41 +/- 24.40\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=559000, episode_reward=-26.91 +/- 70.80\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=560000, episode_reward=-53.88 +/- 16.71\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=561000, episode_reward=-17.01 +/- 82.85\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=562000, episode_reward=-30.23 +/- 70.91\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=563000, episode_reward=-40.26 +/- 53.89\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=564000, episode_reward=-27.25 +/- 69.98\n",
      "Episode length: 1.45 +/- 0.74\n",
      "Eval num_timesteps=565000, episode_reward=-1.40 +/- 90.67\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=566000, episode_reward=-49.50 +/- 99.33\n",
      "Episode length: 1.80 +/- 0.87\n",
      "Eval num_timesteps=567000, episode_reward=-1669.73 +/- 3265.40\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=568000, episode_reward=-41.15 +/- 76.05\n",
      "Episode length: 1.20 +/- 0.51\n",
      "Eval num_timesteps=569000, episode_reward=-48.91 +/- 83.99\n",
      "Episode length: 1.70 +/- 0.90\n",
      "Eval num_timesteps=570000, episode_reward=-14.99 +/- 100.91\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=571000, episode_reward=-48.50 +/- 69.43\n",
      "Episode length: 1.70 +/- 0.78\n",
      "Eval num_timesteps=572000, episode_reward=12.46 +/- 96.57\n",
      "Episode length: 1.40 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=573000, episode_reward=-44.69 +/- 20.34\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=574000, episode_reward=-39.53 +/- 73.60\n",
      "Episode length: 1.30 +/- 0.56\n",
      "Eval num_timesteps=575000, episode_reward=-53.78 +/- 65.13\n",
      "Episode length: 1.30 +/- 0.64\n",
      "Eval num_timesteps=576000, episode_reward=-74.80 +/- 106.69\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=577000, episode_reward=-47.23 +/- 23.28\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=578000, episode_reward=-19.49 +/- 68.17\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=579000, episode_reward=-62.89 +/- 41.37\n",
      "Episode length: 1.70 +/- 0.84\n",
      "Eval num_timesteps=580000, episode_reward=-46.69 +/- 68.01\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=581000, episode_reward=-20.62 +/- 69.01\n",
      "Episode length: 1.70 +/- 0.78\n",
      "Eval num_timesteps=582000, episode_reward=-45.86 +/- 21.36\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=583000, episode_reward=-29.85 +/- 92.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=584000, episode_reward=-11.76 +/- 95.69\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=585000, episode_reward=24.73 +/- 100.58\n",
      "Episode length: 1.55 +/- 0.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=586000, episode_reward=-9.58 +/- 80.13\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=587000, episode_reward=-30.55 +/- 70.73\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=588000, episode_reward=-27.17 +/- 70.45\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=589000, episode_reward=-35.27 +/- 53.52\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=590000, episode_reward=-38.94 +/- 53.04\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=591000, episode_reward=-38.43 +/- 53.22\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=592000, episode_reward=-25.13 +/- 69.92\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=593000, episode_reward=-31.80 +/- 52.51\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=594000, episode_reward=-39.93 +/- 21.71\n",
      "Episode length: 1.85 +/- 0.79\n",
      "Eval num_timesteps=595000, episode_reward=-37.34 +/- 52.71\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=596000, episode_reward=-54.60 +/- 17.82\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=597000, episode_reward=-43.67 +/- 53.52\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=598000, episode_reward=-18.91 +/- 68.70\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=599000, episode_reward=-51.42 +/- 20.90\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=600000, episode_reward=-6.93 +/- 93.63\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=601000, episode_reward=1.44 +/- 90.08\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=602000, episode_reward=-54.10 +/- 21.90\n",
      "Episode length: 1.75 +/- 0.77\n",
      "Eval num_timesteps=603000, episode_reward=-60.43 +/- 12.60\n",
      "Episode length: 1.15 +/- 0.48\n",
      "Eval num_timesteps=604000, episode_reward=-27.77 +/- 70.45\n",
      "Episode length: 1.55 +/- 0.80\n",
      "Eval num_timesteps=605000, episode_reward=-47.60 +/- 63.53\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=606000, episode_reward=-57.74 +/- 73.18\n",
      "Episode length: 1.60 +/- 0.86\n",
      "Eval num_timesteps=607000, episode_reward=-21.03 +/- 84.17\n",
      "Episode length: 1.20 +/- 0.51\n",
      "Eval num_timesteps=608000, episode_reward=3.04 +/- 88.45\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=609000, episode_reward=-11.83 +/- 81.15\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=610000, episode_reward=-50.61 +/- 20.33\n",
      "Episode length: 1.30 +/- 0.56\n",
      "Eval num_timesteps=611000, episode_reward=-28.78 +/- 69.91\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=612000, episode_reward=-28.98 +/- 71.08\n",
      "Episode length: 1.30 +/- 0.56\n",
      "Eval num_timesteps=613000, episode_reward=-12.35 +/- 81.39\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=614000, episode_reward=-45.57 +/- 21.62\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=615000, episode_reward=-23.24 +/- 69.49\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=616000, episode_reward=-16.46 +/- 68.43\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=617000, episode_reward=-17.80 +/- 83.29\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=618000, episode_reward=-24.64 +/- 70.03\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=619000, episode_reward=-17.78 +/- 82.81\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=620000, episode_reward=-32.21 +/- 51.68\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=621000, episode_reward=-30.51 +/- 52.28\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=622000, episode_reward=-12.76 +/- 81.55\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=623000, episode_reward=-24.42 +/- 69.09\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=624000, episode_reward=-39.62 +/- 52.58\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=625000, episode_reward=-22.31 +/- 69.49\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=626000, episode_reward=-14.78 +/- 82.81\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=627000, episode_reward=-9.24 +/- 80.34\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=628000, episode_reward=-19.37 +/- 83.90\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=629000, episode_reward=4.26 +/- 100.30\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=630000, episode_reward=-35.27 +/- 71.59\n",
      "Episode length: 1.25 +/- 0.62\n",
      "Eval num_timesteps=631000, episode_reward=-16.41 +/- 67.84\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=632000, episode_reward=-37.63 +/- 52.94\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=633000, episode_reward=-39.84 +/- 25.24\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=634000, episode_reward=-44.37 +/- 24.58\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=635000, episode_reward=-20.43 +/- 68.99\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=636000, episode_reward=-15.44 +/- 82.29\n",
      "Episode length: 1.45 +/- 0.74\n",
      "Eval num_timesteps=637000, episode_reward=-34.75 +/- 74.11\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=638000, episode_reward=-19.88 +/- 84.16\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=639000, episode_reward=-40.85 +/- 22.26\n",
      "Episode length: 1.75 +/- 0.77\n",
      "Eval num_timesteps=640000, episode_reward=-46.09 +/- 22.32\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=641000, episode_reward=-4.61 +/- 92.36\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=642000, episode_reward=-29.11 +/- 70.58\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=643000, episode_reward=-9.11 +/- 80.02\n",
      "Episode length: 1.75 +/- 0.83\n",
      "Eval num_timesteps=644000, episode_reward=-19.52 +/- 83.52\n",
      "Episode length: 1.30 +/- 0.64\n",
      "Eval num_timesteps=645000, episode_reward=-19.77 +/- 68.81\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=646000, episode_reward=-38.39 +/- 52.24\n",
      "Episode length: 1.55 +/- 0.80\n",
      "Eval num_timesteps=647000, episode_reward=-14.07 +/- 67.15\n",
      "Episode length: 1.60 +/- 0.58\n",
      "Eval num_timesteps=648000, episode_reward=8.52 +/- 98.31\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=649000, episode_reward=-46.08 +/- 59.05\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=650000, episode_reward=-19.91 +/- 85.83\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=651000, episode_reward=0.50 +/- 89.66\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=652000, episode_reward=-17.36 +/- 67.98\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=653000, episode_reward=4.38 +/- 88.19\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=654000, episode_reward=-31.63 +/- 52.15\n",
      "Episode length: 1.70 +/- 0.78\n",
      "Eval num_timesteps=655000, episode_reward=-24.14 +/- 70.21\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=656000, episode_reward=-0.10 +/- 90.55\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=657000, episode_reward=-30.55 +/- 52.92\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=658000, episode_reward=-23.95 +/- 69.73\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=659000, episode_reward=-16.53 +/- 83.00\n",
      "Episode length: 1.30 +/- 0.56\n",
      "Eval num_timesteps=660000, episode_reward=4.44 +/- 88.38\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=661000, episode_reward=-28.00 +/- 70.83\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=662000, episode_reward=-28.75 +/- 52.85\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=663000, episode_reward=-25.38 +/- 70.51\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=664000, episode_reward=-11.82 +/- 80.76\n",
      "Episode length: 1.85 +/- 0.91\n",
      "Eval num_timesteps=665000, episode_reward=-22.89 +/- 69.81\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=666000, episode_reward=-18.15 +/- 83.57\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=667000, episode_reward=-37.70 +/- 85.54\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=668000, episode_reward=0.63 +/- 89.61\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=669000, episode_reward=-22.45 +/- 69.78\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=670000, episode_reward=-9.78 +/- 81.16\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=671000, episode_reward=-54.98 +/- 16.66\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=672000, episode_reward=-0.95 +/- 90.73\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=673000, episode_reward=-35.38 +/- 23.49\n",
      "Episode length: 1.80 +/- 0.68\n",
      "Eval num_timesteps=674000, episode_reward=-4.99 +/- 79.09\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=675000, episode_reward=-8.81 +/- 93.69\n",
      "Episode length: 1.30 +/- 0.64\n",
      "Eval num_timesteps=676000, episode_reward=-16.73 +/- 67.82\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=677000, episode_reward=-11.44 +/- 81.80\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=678000, episode_reward=-32.29 +/- 53.41\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=679000, episode_reward=-7.69 +/- 96.42\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=680000, episode_reward=-29.54 +/- 70.52\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=681000, episode_reward=-5.81 +/- 79.37\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=682000, episode_reward=-41.38 +/- 62.46\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=683000, episode_reward=-39.74 +/- 53.27\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=684000, episode_reward=-24.77 +/- 69.78\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=685000, episode_reward=-35.07 +/- 53.41\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=686000, episode_reward=-23.53 +/- 69.89\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=687000, episode_reward=-51.92 +/- 89.24\n",
      "Episode length: 1.90 +/- 0.83\n",
      "Eval num_timesteps=688000, episode_reward=-38.09 +/- 53.85\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=689000, episode_reward=16.37 +/- 105.75\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=690000, episode_reward=-12.99 +/- 82.16\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=691000, episode_reward=-22.30 +/- 69.74\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=692000, episode_reward=-22.98 +/- 69.99\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=693000, episode_reward=-12.15 +/- 81.65\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=694000, episode_reward=-19.55 +/- 73.27\n",
      "Episode length: 1.75 +/- 0.70\n",
      "Eval num_timesteps=695000, episode_reward=8.04 +/- 98.69\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=696000, episode_reward=-12.25 +/- 81.61\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=697000, episode_reward=-41.31 +/- 24.38\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=698000, episode_reward=2.82 +/- 75.21\n",
      "Episode length: 1.70 +/- 0.56\n",
      "Eval num_timesteps=699000, episode_reward=-30.92 +/- 52.78\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=700000, episode_reward=-38.33 +/- 53.72\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=701000, episode_reward=-38.91 +/- 53.07\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=702000, episode_reward=12.71 +/- 96.26\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=703000, episode_reward=-0.36 +/- 90.45\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=704000, episode_reward=-16.84 +/- 68.84\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=705000, episode_reward=-39.92 +/- 23.38\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=706000, episode_reward=-3.21 +/- 78.12\n",
      "Episode length: 1.65 +/- 0.65\n",
      "Eval num_timesteps=707000, episode_reward=20.50 +/- 103.67\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=708000, episode_reward=-30.12 +/- 52.50\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=709000, episode_reward=-21.41 +/- 68.74\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=710000, episode_reward=24.32 +/- 100.69\n",
      "Episode length: 1.70 +/- 0.84\n",
      "Eval num_timesteps=711000, episode_reward=-7.02 +/- 92.71\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=712000, episode_reward=-5.30 +/- 92.48\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=713000, episode_reward=-50.98 +/- 21.45\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=714000, episode_reward=-10.54 +/- 80.85\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=715000, episode_reward=-12.43 +/- 81.88\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=716000, episode_reward=-23.83 +/- 51.50\n",
      "Episode length: 1.65 +/- 0.57\n",
      "Eval num_timesteps=717000, episode_reward=-14.00 +/- 66.10\n",
      "Episode length: 1.95 +/- 0.80\n",
      "Eval num_timesteps=718000, episode_reward=-10.44 +/- 81.24\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=719000, episode_reward=14.45 +/- 95.30\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=720000, episode_reward=5.55 +/- 99.96\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=721000, episode_reward=-16.30 +/- 67.61\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=722000, episode_reward=-25.45 +/- 70.29\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=723000, episode_reward=-50.43 +/- 108.61\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=724000, episode_reward=-38.00 +/- 118.24\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=725000, episode_reward=-9.39 +/- 130.23\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=726000, episode_reward=-44.25 +/- 80.91\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=727000, episode_reward=-49.51 +/- 23.03\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=728000, episode_reward=-35.93 +/- 21.28\n",
      "Episode length: 2.00 +/- 0.77\n",
      "Eval num_timesteps=729000, episode_reward=-37.34 +/- 53.50\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=730000, episode_reward=-21.87 +/- 69.45\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=731000, episode_reward=-50.76 +/- 21.70\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=732000, episode_reward=-14.89 +/- 66.91\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=733000, episode_reward=-63.25 +/- 63.33\n",
      "Episode length: 1.35 +/- 0.73\n",
      "Eval num_timesteps=734000, episode_reward=-21.95 +/- 69.55\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=735000, episode_reward=-46.71 +/- 53.39\n",
      "Episode length: 1.20 +/- 0.51\n",
      "Eval num_timesteps=736000, episode_reward=-51.48 +/- 20.81\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=737000, episode_reward=-19.85 +/- 68.88\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=738000, episode_reward=2.08 +/- 75.37\n",
      "Episode length: 1.75 +/- 0.62\n",
      "Eval num_timesteps=739000, episode_reward=-42.59 +/- 24.25\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=740000, episode_reward=-49.02 +/- 19.08\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=741000, episode_reward=-18.64 +/- 68.26\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=742000, episode_reward=-20.99 +/- 68.66\n",
      "Episode length: 1.75 +/- 0.83\n",
      "Eval num_timesteps=743000, episode_reward=-1.53 +/- 77.79\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=744000, episode_reward=19.67 +/- 103.88\n",
      "Episode length: 1.30 +/- 0.56\n",
      "Eval num_timesteps=745000, episode_reward=-24.06 +/- 69.76\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=746000, episode_reward=-42.85 +/- 24.43\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=747000, episode_reward=-23.25 +/- 70.03\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=748000, episode_reward=19.91 +/- 103.97\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=749000, episode_reward=-49.32 +/- 22.04\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=750000, episode_reward=-17.26 +/- 68.15\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=751000, episode_reward=-48.89 +/- 63.90\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=752000, episode_reward=-37.16 +/- 84.23\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=753000, episode_reward=-23.30 +/- 70.04\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=754000, episode_reward=-25.70 +/- 70.14\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=755000, episode_reward=-47.60 +/- 21.30\n",
      "Episode length: 1.70 +/- 0.84\n",
      "Eval num_timesteps=756000, episode_reward=-3.51 +/- 91.53\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=757000, episode_reward=-19.21 +/- 68.92\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=758000, episode_reward=-48.87 +/- 19.04\n",
      "Episode length: 1.75 +/- 0.89\n",
      "Eval num_timesteps=759000, episode_reward=-15.14 +/- 82.87\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=760000, episode_reward=-5.31 +/- 92.36\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=761000, episode_reward=-14.29 +/- 82.15\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=762000, episode_reward=-11.87 +/- 82.68\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=763000, episode_reward=15.25 +/- 95.13\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=764000, episode_reward=-9.38 +/- 94.12\n",
      "Episode length: 1.15 +/- 0.36\n",
      "Eval num_timesteps=765000, episode_reward=-3.60 +/- 78.03\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=766000, episode_reward=-33.34 +/- 71.83\n",
      "Episode length: 1.15 +/- 0.36\n",
      "Eval num_timesteps=767000, episode_reward=-34.89 +/- 57.43\n",
      "Episode length: 1.80 +/- 0.60\n",
      "Eval num_timesteps=768000, episode_reward=-15.46 +/- 82.74\n",
      "Episode length: 1.40 +/- 0.66\n",
      "Eval num_timesteps=769000, episode_reward=-22.23 +/- 69.14\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=770000, episode_reward=-35.97 +/- 53.87\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=771000, episode_reward=-22.76 +/- 69.55\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=772000, episode_reward=-41.49 +/- 26.05\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=773000, episode_reward=-9.09 +/- 80.61\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=774000, episode_reward=-24.63 +/- 69.71\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=775000, episode_reward=-40.80 +/- 59.29\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=776000, episode_reward=-45.34 +/- 53.86\n",
      "Episode length: 1.15 +/- 0.36\n",
      "Eval num_timesteps=777000, episode_reward=3.72 +/- 88.94\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=778000, episode_reward=-2.38 +/- 70.16\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=779000, episode_reward=13.44 +/- 93.09\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=780000, episode_reward=-22.39 +/- 56.32\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=781000, episode_reward=-30.27 +/- 126.83\n",
      "Episode length: 1.50 +/- 0.67\n",
      "Eval num_timesteps=782000, episode_reward=-17.14 +/- 68.32\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=783000, episode_reward=-23.98 +/- 45.29\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=784000, episode_reward=-21.75 +/- 73.82\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=785000, episode_reward=18.46 +/- 96.24\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=786000, episode_reward=49.52 +/- 101.74\n",
      "Episode length: 1.35 +/- 0.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=787000, episode_reward=-13.69 +/- 62.85\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=788000, episode_reward=-18.80 +/- 73.37\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=789000, episode_reward=-15.20 +/- 67.31\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=790000, episode_reward=-12.69 +/- 75.01\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=791000, episode_reward=-46.35 +/- 63.01\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=792000, episode_reward=-3.56 +/- 85.99\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=793000, episode_reward=12.09 +/- 90.38\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=794000, episode_reward=-5.92 +/- 76.01\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=795000, episode_reward=-16.35 +/- 75.72\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=796000, episode_reward=-8.20 +/- 69.35\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=797000, episode_reward=-18.86 +/- 84.37\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=798000, episode_reward=-16.69 +/- 105.27\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=799000, episode_reward=-17.57 +/- 72.72\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=800000, episode_reward=-16.38 +/- 72.47\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=801000, episode_reward=-34.22 +/- 53.81\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=802000, episode_reward=-27.72 +/- 71.59\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=803000, episode_reward=-10.08 +/- 61.49\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=804000, episode_reward=-26.53 +/- 74.88\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=805000, episode_reward=-44.71 +/- 34.50\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=806000, episode_reward=-6.22 +/- 83.33\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=807000, episode_reward=10.66 +/- 84.97\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=808000, episode_reward=-33.67 +/- 59.28\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=809000, episode_reward=-28.50 +/- 62.51\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=810000, episode_reward=-18.47 +/- 60.23\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=811000, episode_reward=10.71 +/- 74.31\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=812000, episode_reward=-38.66 +/- 58.92\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=813000, episode_reward=-3.75 +/- 66.51\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=814000, episode_reward=-30.76 +/- 40.89\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=815000, episode_reward=-3.21 +/- 57.30\n",
      "Episode length: 1.70 +/- 0.46\n",
      "Eval num_timesteps=816000, episode_reward=-10.22 +/- 77.72\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=817000, episode_reward=-7.33 +/- 68.58\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=818000, episode_reward=-20.42 +/- 60.84\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=819000, episode_reward=-18.86 +/- 76.84\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=820000, episode_reward=4.77 +/- 85.02\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=821000, episode_reward=-13.81 +/- 75.32\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=822000, episode_reward=-11.56 +/- 85.30\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=823000, episode_reward=-31.56 +/- 47.41\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=824000, episode_reward=6.93 +/- 93.55\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=825000, episode_reward=-9.53 +/- 84.72\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=826000, episode_reward=-25.23 +/- 58.00\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=827000, episode_reward=-44.95 +/- 53.68\n",
      "Episode length: 1.15 +/- 0.36\n",
      "Eval num_timesteps=828000, episode_reward=33.48 +/- 91.95\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=829000, episode_reward=-19.82 +/- 75.33\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=830000, episode_reward=-9.29 +/- 73.50\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=831000, episode_reward=-7.77 +/- 76.68\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=832000, episode_reward=-15.32 +/- 101.50\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=833000, episode_reward=-28.20 +/- 40.75\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=834000, episode_reward=32.48 +/- 95.36\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=835000, episode_reward=2.79 +/- 92.65\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=836000, episode_reward=-21.97 +/- 69.93\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=837000, episode_reward=13.61 +/- 93.09\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=838000, episode_reward=5.89 +/- 84.48\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=839000, episode_reward=1.00 +/- 90.50\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=840000, episode_reward=-23.65 +/- 74.22\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=841000, episode_reward=-7.62 +/- 76.58\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=842000, episode_reward=-28.67 +/- 40.90\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=843000, episode_reward=16.37 +/- 115.22\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=844000, episode_reward=-12.18 +/- 74.68\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=845000, episode_reward=-21.51 +/- 60.98\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=846000, episode_reward=-11.08 +/- 81.80\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=847000, episode_reward=-13.62 +/- 71.24\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=848000, episode_reward=-15.94 +/- 59.67\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=849000, episode_reward=-9.30 +/- 73.32\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=850000, episode_reward=-11.44 +/- 71.36\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=851000, episode_reward=-34.01 +/- 34.15\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=852000, episode_reward=4.05 +/- 74.78\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=853000, episode_reward=0.42 +/- 83.90\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=854000, episode_reward=-14.15 +/- 95.63\n",
      "Episode length: 1.05 +/- 0.22\n",
      "Eval num_timesteps=855000, episode_reward=-19.32 +/- 64.40\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=856000, episode_reward=-21.63 +/- 45.08\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=857000, episode_reward=9.11 +/- 89.09\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=858000, episode_reward=-20.19 +/- 73.24\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=859000, episode_reward=-4.39 +/- 95.42\n",
      "Episode length: 1.15 +/- 0.36\n",
      "Eval num_timesteps=860000, episode_reward=-21.18 +/- 61.61\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=861000, episode_reward=-17.37 +/- 64.31\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=862000, episode_reward=-4.99 +/- 75.36\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=863000, episode_reward=48.19 +/- 102.96\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=864000, episode_reward=-29.44 +/- 47.72\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=865000, episode_reward=-34.79 +/- 63.87\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=866000, episode_reward=-2.09 +/- 88.43\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=867000, episode_reward=-30.96 +/- 58.41\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=868000, episode_reward=-21.53 +/- 73.87\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=869000, episode_reward=-32.72 +/- 58.86\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=870000, episode_reward=-14.21 +/- 63.33\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=871000, episode_reward=-10.67 +/- 77.91\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=872000, episode_reward=11.78 +/- 73.39\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=873000, episode_reward=-18.09 +/- 36.66\n",
      "Episode length: 1.75 +/- 0.43\n",
      "Eval num_timesteps=874000, episode_reward=-16.28 +/- 90.72\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=875000, episode_reward=-68.37 +/- 43.30\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=876000, episode_reward=-9.56 +/- 90.98\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=877000, episode_reward=8.70 +/- 62.67\n",
      "Episode length: 1.75 +/- 0.43\n",
      "Eval num_timesteps=878000, episode_reward=-3.50 +/- 82.40\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=879000, episode_reward=-40.52 +/- 69.40\n",
      "Episode length: 1.60 +/- 0.58\n",
      "Eval num_timesteps=880000, episode_reward=-42.98 +/- 61.44\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=881000, episode_reward=21.33 +/- 97.42\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=882000, episode_reward=-19.84 +/- 68.81\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=883000, episode_reward=-23.32 +/- 69.99\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=884000, episode_reward=-17.34 +/- 49.36\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=885000, episode_reward=27.95 +/- 95.95\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=886000, episode_reward=-16.34 +/- 76.02\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=887000, episode_reward=-24.01 +/- 46.09\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=888000, episode_reward=-24.56 +/- 65.96\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=889000, episode_reward=-37.69 +/- 41.91\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=890000, episode_reward=-8.47 +/- 84.41\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=891000, episode_reward=-19.51 +/- 50.06\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=892000, episode_reward=-18.06 +/- 60.20\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=893000, episode_reward=-0.22 +/- 90.59\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=894000, episode_reward=-17.41 +/- 64.11\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=895000, episode_reward=-5.97 +/- 72.31\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=896000, episode_reward=25.57 +/- 84.88\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=897000, episode_reward=-11.35 +/- 81.15\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=898000, episode_reward=24.19 +/- 95.53\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=899000, episode_reward=-20.66 +/- 64.78\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=900000, episode_reward=4.90 +/- 70.51\n",
      "Episode length: 1.75 +/- 0.54\n",
      "Eval num_timesteps=901000, episode_reward=-40.13 +/- 25.14\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=902000, episode_reward=7.08 +/- 118.77\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=903000, episode_reward=-6.32 +/- 62.96\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=904000, episode_reward=6.01 +/- 80.61\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=905000, episode_reward=-16.72 +/- 68.26\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=906000, episode_reward=13.88 +/- 82.53\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=907000, episode_reward=37.73 +/- 97.26\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=908000, episode_reward=-9.59 +/- 82.78\n",
      "Episode length: 1.75 +/- 0.62\n",
      "Eval num_timesteps=909000, episode_reward=1.22 +/- 71.72\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=910000, episode_reward=10.99 +/- 77.95\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=911000, episode_reward=-17.40 +/- 59.95\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=912000, episode_reward=-18.09 +/- 54.25\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=913000, episode_reward=-16.80 +/- 63.79\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=914000, episode_reward=-6.13 +/- 75.59\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=915000, episode_reward=-47.37 +/- 21.96\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=916000, episode_reward=-33.34 +/- 41.94\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=917000, episode_reward=-13.09 +/- 74.76\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=918000, episode_reward=-29.89 +/- 40.70\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=919000, episode_reward=12.94 +/- 83.62\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=920000, episode_reward=7.48 +/- 82.49\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=921000, episode_reward=-25.47 +/- 61.73\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=922000, episode_reward=-1.75 +/- 73.66\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=923000, episode_reward=-9.07 +/- 72.82\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=924000, episode_reward=2.86 +/- 74.88\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=925000, episode_reward=1.04 +/- 83.14\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=926000, episode_reward=-23.86 +/- 66.41\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=927000, episode_reward=-3.60 +/- 82.22\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=928000, episode_reward=7.46 +/- 71.77\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=929000, episode_reward=-7.49 +/- 64.40\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=930000, episode_reward=-8.58 +/- 77.07\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=931000, episode_reward=1.53 +/- 95.79\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=932000, episode_reward=-49.43 +/- 86.56\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=933000, episode_reward=-29.44 +/- 70.79\n",
      "Episode length: 1.65 +/- 0.65\n",
      "Eval num_timesteps=934000, episode_reward=-17.63 +/- 59.43\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=935000, episode_reward=-32.29 +/- 53.32\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=936000, episode_reward=4.11 +/- 73.88\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=937000, episode_reward=-12.38 +/- 74.79\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=938000, episode_reward=-21.87 +/- 43.38\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=939000, episode_reward=21.50 +/- 77.74\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=940000, episode_reward=-15.86 +/- 83.22\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=941000, episode_reward=-14.44 +/- 71.67\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=942000, episode_reward=25.69 +/- 94.05\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=943000, episode_reward=-11.06 +/- 65.44\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=944000, episode_reward=-12.97 +/- 66.70\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=945000, episode_reward=-56.49 +/- 59.17\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=946000, episode_reward=-9.31 +/- 83.60\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=947000, episode_reward=-14.34 +/- 71.56\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=948000, episode_reward=-14.85 +/- 75.92\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=949000, episode_reward=-4.71 +/- 71.86\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=950000, episode_reward=-16.64 +/- 54.50\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=951000, episode_reward=23.68 +/- 86.47\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=952000, episode_reward=-22.88 +/- 51.06\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=953000, episode_reward=25.30 +/- 91.73\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=954000, episode_reward=26.69 +/- 89.54\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=955000, episode_reward=21.78 +/- 85.38\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=956000, episode_reward=-6.53 +/- 59.40\n",
      "Episode length: 1.75 +/- 0.77\n",
      "Eval num_timesteps=957000, episode_reward=-5.90 +/- 77.06\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=958000, episode_reward=-23.87 +/- 50.41\n",
      "Episode length: 1.60 +/- 0.58\n",
      "Eval num_timesteps=959000, episode_reward=14.75 +/- 74.29\n",
      "Episode length: 1.85 +/- 0.79\n",
      "Eval num_timesteps=960000, episode_reward=-7.38 +/- 65.76\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=961000, episode_reward=-61.79 +/- 83.86\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=962000, episode_reward=-20.27 +/- 71.89\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=963000, episode_reward=-8.20 +/- 101.56\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=964000, episode_reward=-432.69 +/- 1764.85\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=965000, episode_reward=-22.34 +/- 91.10\n",
      "Episode length: 1.75 +/- 0.62\n",
      "Eval num_timesteps=966000, episode_reward=6.17 +/- 75.05\n",
      "Episode length: 1.90 +/- 0.70\n",
      "Eval num_timesteps=967000, episode_reward=-9.54 +/- 117.67\n",
      "Episode length: 1.60 +/- 0.58\n",
      "Eval num_timesteps=968000, episode_reward=12.72 +/- 86.22\n",
      "Episode length: 1.75 +/- 0.70\n",
      "Eval num_timesteps=969000, episode_reward=-4.34 +/- 69.39\n",
      "Episode length: 1.70 +/- 0.64\n",
      "Eval num_timesteps=970000, episode_reward=-7.80 +/- 62.64\n",
      "Episode length: 1.65 +/- 0.65\n",
      "Eval num_timesteps=971000, episode_reward=18.47 +/- 65.43\n",
      "Episode length: 1.85 +/- 0.65\n",
      "Eval num_timesteps=972000, episode_reward=-2.34 +/- 84.43\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=973000, episode_reward=-2.02 +/- 77.79\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=974000, episode_reward=-866.79 +/- 2546.80\n",
      "Episode length: 2.25 +/- 1.48\n",
      "Eval num_timesteps=975000, episode_reward=-12.34 +/- 55.10\n",
      "Episode length: 1.65 +/- 0.65\n",
      "Eval num_timesteps=976000, episode_reward=-13.93 +/- 65.62\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=977000, episode_reward=-16.10 +/- 60.61\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=978000, episode_reward=-47.89 +/- 77.99\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=979000, episode_reward=-0.23 +/- 80.57\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=980000, episode_reward=0.45 +/- 81.75\n",
      "Episode length: 1.50 +/- 0.59\n",
      "Eval num_timesteps=981000, episode_reward=-17.92 +/- 65.10\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=982000, episode_reward=-13.35 +/- 61.01\n",
      "Episode length: 1.70 +/- 0.71\n",
      "Eval num_timesteps=983000, episode_reward=5.71 +/- 69.13\n",
      "Episode length: 1.80 +/- 0.68\n",
      "Eval num_timesteps=984000, episode_reward=-35.58 +/- 40.58\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=985000, episode_reward=8.13 +/- 78.14\n",
      "Episode length: 1.90 +/- 0.70\n",
      "Eval num_timesteps=986000, episode_reward=-5.36 +/- 74.07\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=987000, episode_reward=15.11 +/- 75.01\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=988000, episode_reward=-45.34 +/- 65.85\n",
      "Episode length: 1.75 +/- 0.62\n",
      "Eval num_timesteps=989000, episode_reward=-24.41 +/- 41.68\n",
      "Episode length: 1.75 +/- 0.70\n",
      "Eval num_timesteps=990000, episode_reward=-11.14 +/- 70.23\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=991000, episode_reward=7.28 +/- 86.17\n",
      "Episode length: 1.45 +/- 0.59\n",
      "Eval num_timesteps=992000, episode_reward=-10.68 +/- 72.44\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=993000, episode_reward=-1.21 +/- 90.57\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=994000, episode_reward=-4.61 +/- 73.26\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=995000, episode_reward=-0.70 +/- 84.14\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=996000, episode_reward=-8.77 +/- 74.15\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=997000, episode_reward=-32.54 +/- 60.58\n",
      "Episode length: 1.55 +/- 0.59\n",
      "Eval num_timesteps=998000, episode_reward=-1236.35 +/- 2873.46\n",
      "Episode length: 1.75 +/- 0.43\n",
      "Eval num_timesteps=999000, episode_reward=31.19 +/- 85.44\n",
      "Episode length: 1.65 +/- 0.65\n",
      "Eval num_timesteps=1000000, episode_reward=-43.36 +/- 61.88\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Mean reward: -30.228974056243896, Std reward: 84.09290198984222\n"
     ]
    }
   ],
   "source": [
    "# 设置评估频率\n",
    "eval_freq = 1000  # 每训练500步评估一次\n",
    "\n",
    "# 配置EvalCallback\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=20,\n",
    "    best_model_save_path=\"./best_model/DQN\",  # 更改保存路径为DQN\n",
    "    log_path=\"./logs/DQN\",  # 更改日志路径为DQN\n",
    "    eval_freq=eval_freq,\n",
    ")\n",
    "\n",
    "# DQN的超参数设置\n",
    "model = DQN(\n",
    "    'MlpPolicy', \n",
    "    env, \n",
    "    verbose=0, \n",
    "    tensorboard_log=\"./logs/DQN\",\n",
    "    learning_rate=0.0005,  # 默认学习率，通常比A2C更小\n",
    "    buffer_size=100000,  # 经验回放缓冲区大小\n",
    "    learning_starts=1000,  # 开始学习前的步数\n",
    "    batch_size=32,  # 批量大小\n",
    "    tau=1.0,  # 目标网络软更新的系数\n",
    "    gamma=0.99,  # 折扣因子\n",
    "    train_freq=4,  # 训练的频率（每4步训练一次）\n",
    "    target_update_interval=1000,  # 目标网络更新的频率\n",
    "    exploration_fraction=0.1,  # 探索率衰减部分\n",
    "    exploration_final_eps=0.02,  # 最小探索率\n",
    "    max_grad_norm=10,  # 梯度裁剪的最大范数\n",
    ")\n",
    "\n",
    "\n",
    "# 训练模型，并将TensorBoard日志存储到log目录中\n",
    "model.learn(total_timesteps=1000000, tb_log_name=\"DQN\", callback=eval_callback)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"model/dqn_electric_vehicle\")\n",
    "\n",
    "# 加载模型（如果需要）\n",
    "# model = DQN.load(\"dqn_electric_vehicle\")\n",
    "\n",
    "# 评估模型\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# 测试训练好的模型\n",
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-03T06:30:46.636267200Z",
     "start_time": "2024-09-03T06:05:10.208618700Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b231aa4230c4934"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
